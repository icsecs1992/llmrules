[
    {
        "Bug description": "Constructing a tflite model with a paramater  filter_input_channel  of less than 1 gives a FPE.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -347,6 +347,7 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n   // or equals (normal conv).\n   auto input_channel = input->dims->data[3];\n   auto filter_input_channel = filter->dims->data[3];\n+  TF_LITE_ENSURE(context, filter_input_channel > 0);\n   TF_LITE_ENSURE_EQ(context, input_channel % filter_input_channel, 0);\n   data->groups = input_channel / filter_input_channel;\n \n"
        ],
        "Title": "\n          FPE in TFLite in conv kernel\n        "
    },
    {
        "Bug description": "nn_ops.fractional_avg_pool_v2  and  nn_ops.fractional_max_pool_v2  require the first and fourth elements of their parameter  pooling_ratio  to be equal to 1.0, as pooling on batch and channel dimensions is not supported.",
        "Sample Code": "import os\nimport numpy as np\nfrom tensorflow.python.ops import nn_ops\ntry:\n  arg_0_tensor = tf.random.uniform([3, 30, 50, 3], dtype=tf.float64)\n  arg_0 = tf.identity(arg_0_tensor)\n  arg_1_0 = 2\n  arg_1_1 = 3\n  arg_1_2 = 1\n  arg_1_3 = 1\n  arg_1 = [arg_1_0,arg_1_1,arg_1_2,arg_1_3,]\n  arg_2 = True\n  arg_3 = True\n  seed = 341261001\n  out = nn_ops.fractional_avg_pool_v2(arg_0,arg_1,arg_2,arg_3,seed=seed,)\nexcept Exception as e:\n  :\n  print(\"Error:\"+str(e))",
        "Bug fix": [
            "@@ -51,7 +51,7 @@ class FractionalAvgPoolOp : public OpKernel {\n                       pooling_ratio_[i]));\n     }\n     OP_REQUIRES(\n-        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n+        context, pooling_ratio_[0] == 1 && pooling_ratio_[3] == 1,\n         errors::Unimplemented(\"Fractional average pooling is not yet \"\n                               \"supported on the batch nor channel dimension.\"));\n     OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));\n",
            "@@ -53,7 +53,7 @@ class FractionalMaxPoolOp : public OpKernel {\n     }\n \n     OP_REQUIRES(\n-        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n+        context, pooling_ratio_[0] == 1 && pooling_ratio_[3] == 1,\n         errors::Unimplemented(\"Fractional max pooling is not yet \"\n                               \"supported on the batch nor channel dimension.\"));\n \n",
            "@@ -351,7 +351,7 @@ class FractionalAvgTest(test.TestCase):\n             name=None)\n         self.evaluate(result)\n \n-  def testPoolingRatioValueOutOfRange(self):\n+  def testPoolingRatioIllegalSmallValue(self):\n     with self.cached_session() as _:\n       # Whether turn on `TF2_BEHAVIOR` generates different error messages\n       with self.assertRaisesRegex(\n@@ -368,6 +368,16 @@ class FractionalAvgTest(test.TestCase):\n         )\n         self.evaluate(result)\n \n+  def testPoolingIllegalRatioForBatch(self):\n+    with self.cached_session() as _:\n+      with self.assertRaises(errors.UnimplementedError):\n+        result = nn_ops.gen_nn_ops.fractional_avg_pool(\n+            np.zeros([3, 30, 50, 3]),\n+            [2, 3, 1.5, 1],\n+            True,\n+            True)\n+        self.evaluate(result)\n+\n \n class FractionalAvgPoolGradTest(test.TestCase):\n   \"\"\"Tests for FractionalAvgPoolGrad.\n",
            "@@ -338,7 +338,7 @@ class FractionalMaxPoolTest(test.TestCase):\n             name=None)\n         self.evaluate(result)\n \n-  def testPoolingRatioValueOutOfRange(self):\n+  def testPoolingRatioIllegalSmallValue(self):\n     with self.cached_session() as _:\n       # Whether turn on `TF2_BEHAVIOR` generates different error messages\n       with self.assertRaisesRegex(\n@@ -355,6 +355,16 @@ class FractionalMaxPoolTest(test.TestCase):\n         )\n         self.evaluate(result)\n \n+  def testPoolingIllegalRatioForBatch(self):\n+    with self.cached_session() as _:\n+      with self.assertRaises(errors.UnimplementedError):\n+        result = nn_ops.fractional_max_pool(\n+            np.zeros([3, 30, 50, 3]),\n+            [2, 3, 1.5, 1],\n+            True,\n+            True)\n+        self.evaluate(result)\n+\n \n class FractionalMaxPoolGradTest(test.TestCase):\n   \"\"\"Tests for FractionalMaxPoolGrad.\n"
        ],
        "Title": "\n          Double free in Fractional(Max/Avg)Pool\n        "
    },
    {
        "Bug description": "When running with XLA,  tf.raw_ops.ParallelConcat  segfaults with a nullptr dereference when given a parameter  shape  with rank that is not greater than zero.",
        "Sample Code": "func = tf.raw_ops.ParallelConcat\npara = {'shape':  0, 'values': [1]}\n\n@\ndef test():\n   y = func(**para)\n   return y\n\n\n\ntest()",
        "Bug fix": [
            "@@ -78,7 +78,7 @@ class ParallelConcatUpdate : public OpKernel {\n     OP_REQUIRES(\n         ctx, value.dim_size(0) > loc_,\n         errors::InvalidArgument(\"0th dimension of value = \", value.dim_size(0),\n-                                \" is less than loc_=\", loc_));\n+                                \" must be greater than loc_ = \", loc_));\n \n     auto update = ctx->input(1);\n \n",
            "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include <algorithm>\n #include <ostream>\n+#include <vector>\n \n #include \"tensorflow/core/framework/common_shape_fns.h\"\n #include \"tensorflow/core/framework/full_type.pb.h\"\n@@ -309,6 +310,12 @@ REGISTER_OP(\"ParallelConcat\")\n           return errors::InvalidArgument(\n               \"All input shapes must be fully defined.\");\n         }\n+        if (c->Rank(c->input(i)) < 1) {\n+          return errors::InvalidArgument(\n+              \"The rank of all input shapes must be greater than 0, \"\n+              \"but input \",\n+              i, \" had rank \", c->Rank(c->input(i)), \".\");\n+        }\n         DimensionHandle unused;\n         if (!c->WithValue(c->Dim(c->input(i), 0), 1, &unused).ok()) {\n           return errors::InvalidArgument(\"Size of first dimension must be 1.\");\n",
            "@@ -83,8 +83,9 @@ class StackOpTest(test.TestCase):\n       y = gen_array_ops.parallel_concat(values=[[\"tf\"]], shape=0)\n       return y\n \n-    with self.assertRaisesRegex(errors.InvalidArgumentError,\n-                                r\"0th dimension of value .* is less than\"):\n+    with self.assertRaisesRegex(\n+        errors.InvalidArgumentError, r\"0th dimension .* must be greater than\"\n+    ):\n       f()\n \n   def testSimpleParallelGPU(self):\n",
            "@@ -18,6 +18,7 @@ from tensorflow.python.eager import backprop\n from tensorflow.python.eager import def_function\n from tensorflow.python.framework import dtypes\n from tensorflow.python.framework import tensor_spec\n+from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import math_ops\n from tensorflow.python.ops import random_ops\n@@ -91,6 +92,20 @@ class ArrayOpTest(test.TestCase):\n     conc = g.get_concrete_function(tensor_spec.TensorSpec([10, None]))\n     self.assertAllEqual(conc.output_shapes.as_list(), [10])\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def testParallelConcatFailsWithRankZeroShape(self):\n+    op = array_ops.ParallelConcat\n+    para = {\"shape\": 0, \"values\": [1]}\n+\n+    def func():\n+      y = op(**para)\n+      return y\n+\n+    with self.assertRaisesRegex(\n+        Exception, \"(rank|dimension) of .* must be greater than .* 0\"\n+    ):\n+      func()\n+\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          Null dereference on ParallelConcat with XLA\n        "
    },
    {
        "Bug description": "When running with XLA,  tf.raw_ops.Bincount  segfaults when given a parameter  weights  that is neither the same shape as parameter  arr  nor a length-0 tensor.",
        "Sample Code": "func = tf.raw_ops.Bincount\npara={'arr': 6, 'size': 804, 'weights': [52, 351]}\n\n@\ndef fuzz_jit():\n y = func(**para)\n return y\n\n\n\nprint(fuzz_jit())",
        "Bug fix": [
            "@@ -2417,3 +2417,19 @@ tf_xla_py_test(\n         \"//tensorflow/python:training\",\n     ],\n )\n+\n+tf_xla_py_test(\n+    name = \"bincount_op_test\",\n+    size = \"small\",\n+    srcs = [\"bincount_op_test.py\"],\n+    enable_mlir_bridge = False,\n+    python_version = \"PY3\",\n+    shard_count = 10,\n+    tags = [\n+        \"no_pip\",  # TODO(b/149738646): fix pip install so these tests run on kokoro pip\n+    ],\n+    deps = [\n+        \":xla_test\",\n+        \"//tensorflow/python:platform_test\",\n+    ],\n+)\n",
            "@@ -0,0 +1,40 @@\n+# Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for bincount using the XLA JIT.\"\"\"\n+from tensorflow.compiler.tests import xla_test\n+from tensorflow.python.framework import errors\n+from tensorflow.python.ops import gen_math_ops\n+from tensorflow.python.platform import googletest\n+\n+\n+class BincountTest(xla_test.XLATestCase):\n+\n+  def testInputRank0(self):\n+    with self.session():\n+      with self.test_scope():\n+        bincount = gen_math_ops.bincount(arr=6, size=804, weights=[52, 351])\n+\n+      with self.assertRaisesRegex(\n+          errors.InvalidArgumentError,\n+          (\n+              \"`weights` must be the same shape as `arr` or a length-0\"\n+              \" `Tensor`, in which case it acts as all weights equal to 1.\"\n+          ),\n+      ):\n+        self.evaluate(bincount)\n+\n+\n+if __name__ == \"__main__\":\n+  googletest.main()\n",
            "@@ -62,21 +62,15 @@ class DenseBincountOp : public XlaOpKernel {\n     StatusOr<xla::Shape> input_shape_or = ctx->builder()->GetShape(input);\n     OP_REQUIRES_OK(ctx, input_shape_or.status());\n     auto input_shape = input_shape_or.value();\n-    auto size = input_shape.dimensions(0);\n \n-    if (!size) {\n-      output = xla::Broadcast(zero, {output_size});\n-      ctx->SetOutput(0, output);\n-      return;\n-    }\n     auto rank = input_shape.rank();\n \n     OP_REQUIRES(ctx, rank <= 2,\n                 errors::InvalidArgument(\n                     \"Shape must be at most rank 2 but is rank \", rank));\n-\n     xla::XlaOp weights = ctx->Input(2);\n     StatusOr<xla::Shape> weights_shape_or = ctx->builder()->GetShape(weights);\n+\n     OP_REQUIRES_OK(ctx, weights_shape_or.status());\n \n     auto weights_shape = weights_shape_or.value();\n@@ -91,11 +85,20 @@ class DenseBincountOp : public XlaOpKernel {\n                     \"1. Received \",\n                     weights_shape.DebugString()));\n \n+    auto size = input_shape.dimensions(0);\n+\n+    if (!size) {\n+      output = xla::Broadcast(zero, {output_size});\n+      ctx->SetOutput(0, output);\n+      return;\n+    }\n+\n     auto weights_size = weights_shape.dimensions(0);\n     bool has_weights = false;\n     if (weights_size) {\n       has_weights = true;\n     }\n+\n     xla::Shape output_shape = xla::ShapeUtil::MakeShape(dtype, {output_size});\n     xla::ScatterDimensionNumbers scatter_dnums;\n     scatter_dnums.set_index_vector_dim(1);\n"
        ],
        "Title": "\n          Segfault in Bincount with XLA\n        "
    },
    {
        "Bug description": "NPE in RandomShuffle with XLA enable",
        "Sample Code": "func = tf.raw_ops.RandomShuffle\npara = {'value': 1e+20, 'seed': -4294967297, 'seed2': -2147483649}\n\n@\ndef test():\n   y = func(**para)\n   return y\n\n\n\ntest()",
        "Bug fix": [
            "@@ -236,6 +236,17 @@ class ListOpsTest(parameterized.TestCase, xla_test.XLATestCase):\n       self.assertAllEqual(z.shape.as_list(), [None])\n       self.assertAllEqual(z, [0.0, 0.0])\n \n+  def testInvalidSplitLength(self):\n+    with self.session(), self.test_scope():\n+      tensor_list_split = list_ops.tensor_list_split(\n+          tensor=[1], element_shape=[-1], lengths=[0]\n+      )\n+      with self.assertRaisesRegex(\n+          errors.UnimplementedError, \"All lengths must be positive\"\n+      ):\n+        self.evaluate(tensor_list_split)\n+\n+\n if __name__ == \"__main__\":\n   os.environ[\"TF_XLA_FLAGS\"] = (\"--tf_xla_min_cluster_size=2 \" +\n                                 os.environ.get(\"TF_XLA_FLAGS\", \"\"))\n",
            "@@ -553,6 +553,8 @@ class TensorListSplitOp : public XlaOpKernel {\n       OP_REQUIRES(ctx, len == length,\n                   errors::Unimplemented(\"All lengths have to be the same\"));\n     }\n+    OP_REQUIRES(ctx, length,\n+                errors::Unimplemented(\"All lengths must be positive\"));\n     OP_REQUIRES(\n         ctx, element_dims[0] % length == 0,\n         errors::Unimplemented(\"Buffer size has to be a multiple of length\"));\n"
        ],
        "Title": "\n          NPE in RandomShuffle with XLA enable \n        "
    },
    {
        "Bug description": "FPE in TensorListSplit with XLA",
        "Sample Code": "func = tf.raw_ops.TensorListSplit\npara = {'tensor': [1], 'element_shape': -1, 'lengths': [0]}\n\n@\ndef fuzz_jit():\n y = func(**para)\n return y\n\n\n\nprint(fuzz_jit())",
        "Bug fix": [
            "@@ -236,6 +236,17 @@ class ListOpsTest(parameterized.TestCase, xla_test.XLATestCase):\n       self.assertAllEqual(z.shape.as_list(), [None])\n       self.assertAllEqual(z, [0.0, 0.0])\n \n+  def testInvalidSplitLength(self):\n+    with self.session(), self.test_scope():\n+      tensor_list_split = list_ops.tensor_list_split(\n+          tensor=[1], element_shape=[-1], lengths=[0]\n+      )\n+      with self.assertRaisesRegex(\n+          errors.UnimplementedError, \"All lengths must be positive\"\n+      ):\n+        self.evaluate(tensor_list_split)\n+\n+\n if __name__ == \"__main__\":\n   os.environ[\"TF_XLA_FLAGS\"] = (\"--tf_xla_min_cluster_size=2 \" +\n                                 os.environ.get(\"TF_XLA_FLAGS\", \"\"))\n",
            "@@ -553,6 +553,8 @@ class TensorListSplitOp : public XlaOpKernel {\n       OP_REQUIRES(ctx, len == length,\n                   errors::Unimplemented(\"All lengths have to be the same\"));\n     }\n+    OP_REQUIRES(ctx, length,\n+                errors::Unimplemented(\"All lengths must be positive\"));\n     OP_REQUIRES(\n         ctx, element_dims[0] % length == 0,\n         errors::Unimplemented(\"Buffer size has to be a multiple of length\"));\n"
        ],
        "Title": "\n          FPE in TensorListSplit with XLA \n        "
    },
    {
        "Bug description": "The function  tf.raw_ops.LookupTableImportV2  cannot handle scalars in the  values  parameter and gives an NPE.",
        "Sample Code": "v = tf.Variable(1)\n\n@\ndef test():\n   func = tf.raw_ops.LookupTableImportV2\n   para={'table_handle': v.handle,'keys': [62.98910140991211, 94.36528015136719], 'values': -919}\n\n   y = func(**para)\n   return y\n\n\n\nprint(test())",
        "Bug fix": [
            "@@ -309,9 +309,10 @@ REGISTER_OP(\"LookupTableImportV2\")\n \n       ShapeHandle keys;\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));\n+      ShapeHandle values;\n+      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(2), 1, &values));\n       DimensionHandle unused;\n-      TF_RETURN_IF_ERROR(\n-          c->Merge(c->Dim(keys, 0), c->Dim(c->input(2), 0), &unused));\n+      TF_RETURN_IF_ERROR(c->Merge(c->Dim(keys, 0), c->Dim(values, 0), &unused));\n       return OkStatus();\n     });\n \n",
            "@@ -41,6 +41,7 @@ from tensorflow.python.framework import test_ops\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import gen_lookup_ops\n from tensorflow.python.ops import lookup_ops\n from tensorflow.python.ops import map_fn\n from tensorflow.python.ops import variables\n@@ -573,6 +574,20 @@ class StaticHashTableTest(BaseLookupTableTest, parameterized.TestCase):\n     self.evaluate(lookup_ops.tables_initializer())\n     self.assertAllEqual(grad, -10.)\n \n+  def testImportShapeInference(self, is_anonymous):\n+    v = variables.Variable(1)\n+\n+    @def_function.function(jit_compile=True)\n+    def foo():\n+      return gen_lookup_ops.lookup_table_import_v2(\n+          table_handle=v.handle, keys=[1.1, 2.2], values=1\n+      )\n+\n+    with self.assertRaisesRegex(\n+        ValueError, r\"Shape must be at least rank 1 but is rank 0\"\n+    ):\n+      foo()\n+\n   def testExportShapeInference(self, is_anonymous):\n     table = self.getHashTable()(\n         lookup_ops.KeyValueTensorInitializer(\n"
        ],
        "Title": "\n          NPE in LookupTableImportV2\n        "
    },
    {
        "Bug description": "Out-of-bounds access due to mismatched integer type sizes in ValueMap::Manager::GetValueOrCreatePlaceholder. Bug with tfg-translate call to InitMlir. The problem happens with generic functions, as it is already handled for non-generic functions. This is because they, unlike non-generic functions, are using the \"old importer\". A better long-term solution may be to have the \"new importer\" handle generic functions.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Segmentation fault in tfg-translate \n        "
    },
    {
        "Bug description": "NPE in QuantizedMatMulWithBiasAndDequantize with MKL enable",
        "Sample Code": "func = tf.raw_ops.QuantizedMatMulWithBiasAndDequantize\npara={'a': tf.constant(138, dtype=tf.quint8), 'b': tf.constant(4, dtype=tf.qint8), 'bias': [[31.81644630432129, 47.21876525878906], [109.95201110839844, 152.07968139648438]], 'min_a': 141.5337138686371, 'max_a': [73.84139251708984, 173.15280151367188], 'min_b': [], 'max_b': [[16.128345489501953, 193.26820373535156]], 'min_freezed_output': [], 'max_freezed_output': [115.50032806396484, 156.974853515625], 'Toutput': 1.0, 'transpose_a': True, 'transpose_b': False, 'input_quant_mode': 'MIN_FIRST'}\n\n}\n\nfunc(**para)",
        "Bug fix": "",
        "Title": "\n          NPE in QuantizedMatMulWithBiasAndDequantize\n        "
    },
    {
        "Bug description": "If the stride and window size are not positive for  tf.raw_ops.AvgPoolGrad , it can give an FPE.",
        "Sample Code": "import numpy as np\n\n@\ndef test():\n   y = tf.raw_ops.AvgPoolGrad(orig_input_shape=[1,0,0,0], grad=[[[[0.39117979]]]], ksize=[1,0,0,0], strides=[1,0,0,0], padding=\"SAME\", data_format=\"NCHW\")\n   return y\n\n\n\nprint(test())",
        "Bug fix": [
            "@@ -18,7 +18,9 @@ import numpy as np\n \n from tensorflow.compiler.tests import xla_test\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n+from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import gen_nn_ops\n from tensorflow.python.ops import nn_ops\n@@ -560,6 +562,34 @@ class PoolGradTest(xla_test.XLATestCase):\n \n     self._TestPooling(nn_ops.avg_pool, AvgPoolGrad)\n \n+  @test_util.disable_mlir_bridge(\n+      \"TODO(b/266613412): investigate FPE in AvgPoolGrad for TPU\"\n+  )\n+  def testAvgPoolGradSamePaddingZeroStrideZeroSize(self):\n+    output_gradient_vals = np.array([0.39117979], dtype=np.float32)\n+    output_gradient_vals = output_gradient_vals.reshape([1, 1, 1, 1])\n+    with self.session() as sess:\n+      with self.test_scope():\n+        output_gradients = array_ops.placeholder(\n+            dtypes.float32, shape=output_gradient_vals.shape\n+        )\n+        t = gen_nn_ops.avg_pool_grad(\n+            orig_input_shape=[1, 0, 0, 0],\n+            grad=output_gradients,\n+            ksize=[1, 0, 0, 0],\n+            strides=[1, 0, 0, 0],\n+            padding=\"SAME\",\n+            data_format=\"NCHW\",\n+        )\n+      with self.assertRaisesRegex(\n+          errors.InvalidArgumentError,\n+          (\n+              \"Sliding window ksize field for dimension 1 must be positive but\"\n+              \" is 0\"\n+          ),\n+      ):\n+        sess.run(t, {output_gradients: output_gradient_vals})\n+\n   # The CPU implementation of AvgPoolGrad doesn't accept kernels smaller than\n   # the stride size, so we only run the following tests on MaxPoolGrad.\n \n",
            "@@ -33,15 +33,41 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/util.h\"\n #include \"tensorflow/core/framework/bounds_check.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/op_requires.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/util/determinism.h\"\n #include \"tensorflow/core/util/tensor_format.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n namespace tensorflow {\n namespace {\n \n+template <typename T>\n+static Status ValidateKernelSizes(const T& ksizes) {\n+  for (size_t i = 0; i < ksizes.size(); ++i) {\n+    if (ksizes[i] <= 0) {\n+      return errors::InvalidArgument(\n+          \"Sliding window ksize field for dimension \", i,\n+          \" must be positive but is \", ksizes[i]);\n+    }\n+  }\n+  return OkStatus();\n+}\n+\n+template <typename T>\n+static Status ValidateStrides(const T& strides) {\n+  for (size_t i = 0; i < strides.size(); ++i) {\n+    if (strides[i] <= 0) {\n+      return errors::InvalidArgument(\n+          \"Sliding window stride field for dimension \", i,\n+          \" must be positive but is \", strides[i]);\n+    }\n+  }\n+  return OkStatus();\n+}\n+\n // Superclass of pooling ops.\n class PoolingOp : public XlaOpKernel {\n  public:\n@@ -83,50 +109,54 @@ class PoolingOp : public XlaOpKernel {\n \n  protected:\n   StatusOr<std::vector<int64_t>> GetKernelSize(XlaOpKernelContext* ctx) {\n-    if (ctx->num_inputs() == 1) {\n-      return ksize_;\n-    }\n-    const TensorShape ksize_shape = ctx->InputShape(1);\n-    // Validate input sizes.\n-    if (!TensorShapeUtils::IsVector(ksize_shape)) {\n-      return errors::InvalidArgument(\"ksize must be a vector, not shape \",\n-                                     ksize_shape.DebugString());\n-    }\n-    if (ksize_shape.num_elements() != num_dims()) {\n-      return errors::InvalidArgument(\n-          \"Sliding window ksize field must \"\n-          \"specify \",\n-          num_dims(), \" dimensions\");\n-    }\n     std::vector<int64_t> ksize;\n-    auto status = ctx->ConstantInputAsIntVector(1, &ksize);\n-    if (!status.ok()) {\n-      return status;\n+    if (ctx->num_inputs() == 1) {\n+      ksize = ksize_;\n+    } else {\n+      const TensorShape ksize_shape = ctx->InputShape(1);\n+      // Validate input sizes.\n+      if (!TensorShapeUtils::IsVector(ksize_shape)) {\n+        return errors::InvalidArgument(\"ksize must be a vector, not shape \",\n+                                       ksize_shape.DebugString());\n+      }\n+      if (ksize_shape.num_elements() != num_dims()) {\n+        return errors::InvalidArgument(\n+            \"Sliding window ksize field must \"\n+            \"specify \",\n+            num_dims(), \" dimensions\");\n+      }\n+      auto status = ctx->ConstantInputAsIntVector(1, &ksize);\n+      if (!status.ok()) {\n+        return status;\n+      }\n     }\n+    TF_RETURN_IF_ERROR(ValidateKernelSizes(ksize));\n     return ksize;\n   }\n \n   StatusOr<std::vector<int64_t>> GetStride(XlaOpKernelContext* ctx) {\n-    if (ctx->num_inputs() == 1) {\n-      return stride_;\n-    }\n-    const TensorShape stride_shape = ctx->InputShape(2);\n-    // Validate input sizes.\n-    if (!TensorShapeUtils::IsVector(stride_shape)) {\n-      return errors::InvalidArgument(\"stride must be a vector, not shape \",\n-                                     stride_shape.DebugString());\n-    }\n-    if (stride_shape.num_elements() != num_dims()) {\n-      return errors::InvalidArgument(\n-          \"Sliding window stride field must \"\n-          \"specify \",\n-          num_dims(), \" dimensions\");\n-    }\n     std::vector<int64_t> stride;\n-    auto status = ctx->ConstantInputAsIntVector(2, &stride);\n-    if (!status.ok()) {\n-      return status;\n+    if (ctx->num_inputs() == 1) {\n+      stride = stride_;\n+    } else {\n+      const TensorShape stride_shape = ctx->InputShape(2);\n+      // Validate input sizes.\n+      if (!TensorShapeUtils::IsVector(stride_shape)) {\n+        return errors::InvalidArgument(\"stride must be a vector, not shape \",\n+                                       stride_shape.DebugString());\n+      }\n+      if (stride_shape.num_elements() != num_dims()) {\n+        return errors::InvalidArgument(\n+            \"Sliding window stride field must \"\n+            \"specify \",\n+            num_dims(), \" dimensions\");\n+      }\n+      auto status = ctx->ConstantInputAsIntVector(2, &stride);\n+      if (!status.ok()) {\n+        return status;\n+      }\n     }\n+    TF_RETURN_IF_ERROR(ValidateStrides(stride));\n     return stride;\n   }\n \n@@ -355,10 +385,12 @@ class MaxPoolGradOp : public XlaOpKernel {\n                 errors::InvalidArgument(\"Sliding window ksize field must \"\n                                         \"specify \",\n                                         num_dims(), \" dimensions\"));\n+    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));\n     OP_REQUIRES(ctx, stride_.size() == num_dims(),\n                 errors::InvalidArgument(\"Sliding window strides field must \"\n                                         \"specify \",\n                                         num_dims(), \" dimensions\"));\n+    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));\n \n     const TensorShape tensor_in_shape = ctx->InputShape(0);\n     const TensorShape tensor_out_shape = ctx->InputShape(1);\n@@ -446,11 +478,13 @@ class AvgPoolGradOp : public XlaOpKernel {\n                 errors::InvalidArgument(\"Sliding window ksize field must \"\n                                         \"specify \",\n                                         num_dims(), \" dimensions\"));\n+    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"strides\", &stride_));\n     OP_REQUIRES(ctx, stride_.size() == num_dims(),\n                 errors::InvalidArgument(\"Sliding window strides field must \"\n                                         \"specify \",\n                                         num_dims(), \" dimensions\"));\n+    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"padding\", &padding_));\n     OP_REQUIRES(ctx, padding_ != EXPLICIT,\n                 errors::Unimplemented(\n@@ -579,10 +613,12 @@ class MaxPoolGradGradOp : public XlaOpKernel {\n                 errors::InvalidArgument(\"Sliding window ksize field must \"\n                                         \"specify \",\n                                         num_dims(), \" dimensions\"));\n+    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));\n     OP_REQUIRES(ctx, stride_.size() == num_dims(),\n                 errors::InvalidArgument(\"Sliding window strides field must \"\n                                         \"specify \",\n                                         num_dims(), \" dimensions\"));\n+    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));\n \n     const TensorShape tensor_in_shape = ctx->InputShape(0);\n     const TensorShape tensor_out_shape = ctx->InputShape(1);\n",
            "@@ -35,6 +35,16 @@ Status ValidatePaddingValues(absl::Span<const int64_t> input_dimensions,\n         input_dimensions.size(), window_dimensions.size(),\n         window_strides.size());\n   }\n+  for (size_t i = 0; i < input_dimensions.size(); ++i) {\n+    if (window_dimensions[i] <= 0) {\n+      return InvalidArgument(\"Window dimension %u has non-positive size %d\", i,\n+                             window_dimensions[i]);\n+    }\n+    if (window_strides[i] <= 0) {\n+      return InvalidArgument(\"Window dimension %u has non-positive stride %d\",\n+                             i, window_strides[i]);\n+    }\n+  }\n   return OkStatus();\n }\n \n"
        ],
        "Title": "\n          FPE in AvgPoolGrad with XLA\n        "
    }
]
[
    {
        "Bug description": "Attackers using Tensorflow can exploit the vulnerability. They can access heap memory which is not in the control of user, leading to a crash or RCE. \nWhen axis is larger than the dim of input, c->Dim(input,axis) goes out of bound. \nSame problem occurs in the QuantizeAndDequantizeV2/V3/V4/V4Grad operations too.",
        "Sample Code": "@\ndef test():\n    tf.raw_ops.QuantizeAndDequantizeV2(input=[2.5],\n    \t\t\t\t\t\t\t\t   input_min=[1.0],\n    \t\t\t\t\t\t\t\t   input_max=[10.0],\n    \t\t\t\t\t\t\t\t   signed_input=True,\n    \t\t\t\t\t\t\t\t   num_bits=1,\n    \t\t\t\t\t\t\t\t   range_given=True,\n    \t\t\t\t\t\t\t\t   round_mode='HALF_TO_EVEN',\n    \t\t\t\t\t\t\t\t   narrow_range=True,\n    \t\t\t\t\t\t\t\t   axis=0x7fffffff)\n)\ntest()",
        "Bug fix": [
            "@@ -2879,6 +2879,10 @@ REGISTER_OP(\"QuantizeAndDequantizeV2\")\n                                        axis);\n       } else if (axis != -1) {\n         ShapeHandle input;\n+        if (axis >= kint32max) {\n+          return errors::InvalidArgument(\n+              \"Axis cannot be >= kint32max value, got \", axis);\n+        }\n         TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n         DimensionHandle depth;\n         TF_RETURN_IF_ERROR(\n@@ -2914,6 +2918,10 @@ REGISTER_OP(\"QuantizeAndDequantizeV4\")\n                                        axis);\n       } else if (axis != -1) {\n         ShapeHandle input;\n+        if (axis >= kint32max) {\n+          return errors::InvalidArgument(\n+              \"Axis cannot be >= kint32max value, got \", axis);\n+        }\n         TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n         DimensionHandle depth;\n         TF_RETURN_IF_ERROR(\n@@ -2945,6 +2953,10 @@ REGISTER_OP(\"QuantizeAndDequantizeV4Grad\")\n                                        axis);\n       } else if (axis != -1) {\n         ShapeHandle input;\n+        if (axis >= kint32max) {\n+          return errors::InvalidArgument(\n+              \"Axis cannot be >= kint32max value, got \", axis);\n+        }\n         TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n         DimensionHandle depth;\n         TF_RETURN_IF_ERROR(\n@@ -2981,6 +2993,10 @@ REGISTER_OP(\"QuantizeAndDequantizeV3\")\n                                        axis);\n       } else if (axis != -1) {\n         ShapeHandle input;\n+        if (axis >= kint32max) {\n+          return errors::InvalidArgument(\n+              \"Axis cannot be >= kint32max value, got \", axis);\n+        }\n         TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n         DimensionHandle depth;\n         TF_RETURN_IF_ERROR(\n",
            "@@ -1856,6 +1856,72 @@ class QuantizeAndDequantizeTest(test_util.TensorFlowTestCase):\n               max_range=input_max,\n               axis=2**31 - 1))\n \n+  @test_util.run_v2_only\n+  def testInvalidAxis(self):\n+\n+    @def_function.function\n+    def test_quantize_and_dequantize_v2():\n+      gen_array_ops.quantize_and_dequantize_v2(\n+          input=[2.5],\n+          input_min=[1.0],\n+          input_max=[10.0],\n+          signed_input=True,\n+          num_bits=1,\n+          range_given=True,\n+          round_mode=\"HALF_TO_EVEN\",\n+          narrow_range=True,\n+          axis=0x7fffffff)\n+\n+    @def_function.function\n+    def test_quantize_and_dequantize_v3():\n+      gen_array_ops.quantize_and_dequantize_v3(\n+          input=[2.5],\n+          input_min=[1.0],\n+          input_max=[10.0],\n+          num_bits=1,\n+          signed_input=True,\n+          range_given=True,\n+          narrow_range=True,\n+          axis=0x7fffffff)\n+\n+    @def_function.function\n+    def test_quantize_and_dequantize_v4():\n+      gen_array_ops.quantize_and_dequantize_v4(\n+          input=[2.5],\n+          input_min=[1.0],\n+          input_max=[10.0],\n+          signed_input=True,\n+          num_bits=1,\n+          range_given=True,\n+          round_mode=\"HALF_TO_EVEN\",\n+          narrow_range=True,\n+          axis=0x7fffffff)\n+\n+    @def_function.function\n+    def test_quantize_and_dequantize_v4_grad():\n+      gen_array_ops.quantize_and_dequantize_v4_grad(\n+          gradients=[2.5],\n+          input=[2.5],\n+          input_min=[1.0],\n+          input_max=[10.0],\n+          axis=0x7fffffff)\n+\n+    with self.assertRaisesRegex(\n+        ValueError, \"Axis cannot be >= kint32max value, got 2147483647\"):\n+      test_quantize_and_dequantize_v2()\n+\n+    with self.assertRaisesRegex(\n+        ValueError, \"Axis cannot be >= kint32max value, got 2147483647\"):\n+      test_quantize_and_dequantize_v3()\n+\n+    with self.assertRaisesRegex(\n+        ValueError, \"Axis cannot be >= kint32max value, got 2147483647\"):\n+      test_quantize_and_dequantize_v4()\n+\n+    with self.assertRaisesRegex(\n+        ValueError, \"Axis cannot be >= kint32max value, got 2147483647\"):\n+      test_quantize_and_dequantize_v4_grad()\n+\n \n @test_util.run_all_in_graph_and_eager_modes\n class SortedSearchTest(test_util.TensorFlowTestCase):\n"
        ],
        "Title": "\n          A heap out-of-buffer read vulnerability in the QuantizeAndDequantize operation\n        "
    },
    {
        "Bug description": "Integer overflow occurs when 2^31 <= num_frames * height * width * channels < 2^32, for example Full HD screencast of at least 346 frames.",
        "Sample Code": "dat = urllib.request.urlopen('https://raw.githubusercontent.com/tensorflow/tensorflow/1c38ad9b78ffe06076745a1ee00cec42f39ff726/tensorflow/core/lib/gif/testdata/3g_multiframe.gif').read()\nimport tensorflow as tf\n\ntf.io.decode_gif(dat)",
        "Bug fix": [
            "@@ -452,12 +452,12 @@ class DecodeImageV2Op : public OpKernel {\n     // allocation til after dtype conversion is done. `gif`::Decode` supports\n     // uint8 only.\n     Tensor* output = nullptr;\n-    int buffer_size = 0;\n+    ptrdiff_t buffer_size = 0;\n     string error_string;\n     uint8* buffer = gif::Decode(\n         input.data(), input.size(),\n         [&](int num_frames, int width, int height, int channels) -> uint8* {\n-          buffer_size = num_frames * height * width * channels;\n+          buffer_size = ptrdiff_t(num_frames) * height * width * channels;\n \n           Status status;\n           // By the existing API, we support decoding GIF with `decode_jpeg` or\n",
            "@@ -105,7 +105,7 @@ uint8* Decode(const void* srcdata, int datasize,\n   uint8* const dstdata =\n       allocate_output(target_num_frames, width, height, channel);\n   if (!dstdata) return nullptr;\n-  for (int k = 0; k < target_num_frames; k++) {\n+  for (ptrdiff_t k = 0; k < target_num_frames; k++) {\n     uint8* this_dst = dstdata + k * width * channel * height;\n \n     SavedImage* this_image = &gif_file->SavedImages[k];\n@@ -125,10 +125,10 @@ uint8* Decode(const void* srcdata, int datasize,\n \n     if (k > 0) {\n       uint8* last_dst = dstdata + (k - 1) * width * channel * height;\n-      for (int i = 0; i < height; ++i) {\n+      for (ptrdiff_t i = 0; i < height; ++i) {\n         uint8* p_dst = this_dst + i * width * channel;\n         uint8* l_dst = last_dst + i * width * channel;\n-        for (int j = 0; j < width; ++j) {\n+        for (ptrdiff_t j = 0; j < width; ++j) {\n           p_dst[j * channel + 0] = l_dst[j * channel + 0];\n           p_dst[j * channel + 1] = l_dst[j * channel + 1];\n           p_dst[j * channel + 2] = l_dst[j * channel + 2];\n@@ -141,9 +141,9 @@ uint8* Decode(const void* srcdata, int datasize,\n       // If the first frame does not fill the entire canvas then fill the\n       // unoccupied canvas with zeros (black).\n       if (k == 0) {\n-        for (int i = 0; i < height; ++i) {\n+        for (ptrdiff_t i = 0; i < height; ++i) {\n           uint8* p_dst = this_dst + i * width * channel;\n-          for (int j = 0; j < width; ++j) {\n+          for (ptrdiff_t j = 0; j < width; ++j) {\n             p_dst[j * channel + 0] = 0;\n             p_dst[j * channel + 1] = 0;\n             p_dst[j * channel + 2] = 0;\n@@ -165,9 +165,9 @@ uint8* Decode(const void* srcdata, int datasize,\n       return nullptr;\n     }\n \n-    for (int i = imgTop; i < imgBottom; ++i) {\n+    for (ptrdiff_t i = imgTop; i < imgBottom; ++i) {\n       uint8* p_dst = this_dst + i * width * channel;\n-      for (int j = imgLeft; j < imgRight; ++j) {\n+      for (ptrdiff_t j = imgLeft; j < imgRight; ++j) {\n         GifByteType color_index =\n             this_image->RasterBits[(i - img_desc->Top) * (img_desc->Width) +\n                                    (j - img_desc->Left)];\n",
            "@@ -52,7 +52,7 @@ void TestDecodeGif(Env* env, DecodeGifTestCase testcase) {\n         w = width;\n         h = height;\n         c = channels;\n-        return new uint8[frame_cnt * height * width * channels];\n+        return new uint8[ptrdiff_t(frame_cnt) * height * width * channels];\n       },\n       &error_string));\n   ASSERT_NE(imgdata, nullptr);\n@@ -72,7 +72,8 @@ TEST(GifTest, Gif) {\n        {testdata_path + \"optimized.gif\", 12, 20, 40, 3},\n        {testdata_path + \"red_black.gif\", 1, 16, 16, 3},\n        {testdata_path + \"scan.gif\", 12, 20, 40, 3},\n-       {testdata_path + \"squares.gif\", 2, 16, 16, 3}});\n+       {testdata_path + \"squares.gif\", 2, 16, 16, 3},\n+       {testdata_path + \"3g_multiframe.gif\", 519, 1920, 1080, 3}});\n \n   for (const auto& tc : testcases) {\n     TestDecodeGif(env, tc);\n",
            "Binary files /dev/null and b/tensorflow/core/lib/gif/testdata/3g_multiframe.gif differ\n",
            "@@ -16,6 +16,7 @@ filegroup(\n         \"scan.gif\",\n         \"red_black.gif\",\n         \"squares.gif\",\n+        \"3g_multiframe.gif\",\n         \"pendulum_sm.gif\",\n         # Add groundtruth frames for `pendulum_sm.gif`.\n         # PNG format because it's lossless.\n"
        ],
        "Title": "\n          Segfault when opening multiframe gif\n        "
    },
    {
        "Bug description": "When  SparseSparseMaximum  is given invalid sparse tensors as inputs, it can give an NPE.",
        "Sample Code": "tf.raw_ops.SparseSparseMaximum(\n a_indices=[[1]],\n a_values =[ 0.1 ],\n a_shape = [2],\n b_indices=[[]],\n b_values =[2 ],\n b_shape = [2],\n)],\n)",
        "Bug fix": [
            "@@ -41,6 +41,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/cwise_ops.h\"\n #include \"tensorflow/core/kernels/cwise_ops_common.h\"\n+#include \"tensorflow/core/kernels/sparse_utils.h\"\n #include \"tensorflow/core/util/sparse/sparse_tensor.h\"\n \n namespace tensorflow {\n@@ -131,22 +132,12 @@ class SparseSparseBinaryOpShared : public OpKernel {\n     OP_REQUIRES_OK(ctx, ctx->input(\"b_shape\", &b_shape_t));\n \n     // Validations.\n-    OP_REQUIRES(\n-        ctx,\n-        TensorShapeUtils::IsMatrix(a_indices_t->shape()) &&\n-            TensorShapeUtils::IsMatrix(b_indices_t->shape()),\n-        errors::InvalidArgument(\"Inputs a_indices and b_indices should be \"\n-                                \"matrices but received shapes: \",\n-                                a_indices_t->shape().DebugString(), \", \",\n-                                b_indices_t->shape().DebugString()));\n-    OP_REQUIRES(ctx,\n-                TensorShapeUtils::IsVector(a_values_t->shape()) &&\n-                    TensorShapeUtils::IsVector(b_values_t->shape()),\n-                errors::InvalidArgument(\n-                    \"Inputs a_values and b_values should be vectors \"\n-                    \"but received shapes: \",\n-                    a_values_t->shape().DebugString(), \" and \",\n-                    b_values_t->shape().DebugString()));\n+    OP_REQUIRES_OK(ctx, sparse_utils::ValidateSparseTensor<int64_t>(\n+                            *a_indices_t, *a_values_t, *a_shape_t,\n+                            sparse_utils::IndexValidation::kUnordered));\n+    OP_REQUIRES_OK(ctx, sparse_utils::ValidateSparseTensor<int64_t>(\n+                            *b_indices_t, *b_values_t, *b_shape_t,\n+                            sparse_utils::IndexValidation::kUnordered));\n \n     const int64_t a_nnz = a_indices_t->dim_size(0);\n     const int64_t b_nnz = b_indices_t->dim_size(0);\n@@ -154,25 +145,7 @@ class SparseSparseBinaryOpShared : public OpKernel {\n     const auto a_values = a_values_t->vec<T>();\n     const auto b_values = b_values_t->vec<T>();\n \n-    OP_REQUIRES(\n-        ctx, a_values.size() == a_nnz && b_values.size() == b_nnz,\n-        errors::InvalidArgument(\"Expected \", a_nnz, \" and \", b_nnz,\n-                                \" non-empty input values, got \",\n-                                a_values.size(), \" and \", b_values.size()));\n-\n-    OP_REQUIRES(ctx,\n-                TensorShapeUtils::IsVector(a_shape_t->shape()) &&\n-                    TensorShapeUtils::IsVector(b_shape_t->shape()),\n-                errors::InvalidArgument(\n-                    \"Input shapes should be a vector but received shapes \",\n-                    a_shape_t->shape().DebugString(), \" and \",\n-                    b_shape_t->shape().DebugString()));\n     const int num_dims = a_indices_t->dim_size(1);\n-    OP_REQUIRES(\n-        ctx, a_shape_t->NumElements() == num_dims,\n-        errors::InvalidArgument(\"Second dimension of a_indices and length of \"\n-                                \"a_shape must match, got \",\n-                                num_dims, \" and \", a_shape_t->NumElements()));\n     OP_REQUIRES(ctx, num_dims > 0,\n                 errors::InvalidArgument(\"Tensors must not be empty\"));\n     OP_REQUIRES(ctx, a_shape_t->IsSameSize(*b_shape_t),\n@@ -192,7 +165,7 @@ class SparseSparseBinaryOpShared : public OpKernel {\n     const auto a_indices_mat = a_indices_t->matrix<int64_t>();\n     const auto b_indices_mat = b_indices_t->matrix<int64_t>();\n     std::vector<T> a_augmented_values, b_augmented_values;\n-    std::vector<std::pair<bool, int64>> entries_to_copy;  // from_a?, idx\n+    std::vector<std::pair<bool, int64_t>> entries_to_copy;  // from_a?, idx\n     UnionSparseIndicesAndValues(a_indices_mat, a_values, a_nnz, b_indices_mat,\n                                 b_values, b_nnz, num_dims, &a_augmented_values,\n                                 &b_augmented_values, &entries_to_copy);\n",
            "@@ -26,6 +26,7 @@ from tensorflow.python.framework import tensor_shape\n from tensorflow.python.framework import tensor_spec\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import gen_sparse_ops\n from tensorflow.python.ops import gradient_checker\n from tensorflow.python.ops import nn_ops\n from tensorflow.python.ops import sparse_ops\n@@ -1175,6 +1176,18 @@ class SparseMinimumMaximumTest(test_util.TensorFlowTestCase):\n       self._assertSparseTensorValueEqual(expected, max_tf)\n       self._assertSparseTensorValueEqual(expected, min_tf)\n \n+  def testInvalidSparseInputs(self):\n+    with test_util.force_cpu():\n+      with self.assertRaisesRegex(\n+          (ValueError, errors.InvalidArgumentError),\n+          \".*Index rank .* and shape rank .* do not match.*\",\n+      ):\n+        self.evaluate(\n+            gen_sparse_ops.sparse_sparse_maximum(\n+                [[1]], [0], [2], [[]], [1], [2]\n+            )\n+        )\n+\n   @test_util.run_deprecated_v1\n   def testRandom(self):\n     np.random.seed(1618)\n"
        ],
        "Title": "\n          NPE in SparseSparseMaximum\n        "
    },
    {
        "Bug description": "version:2.11.0 //core/ops/audio_ops.cc:70",
        "Sample Code": "para = {'input': tf.constant([[14.], [24.]], dtype=tf.float32), 'window_size': 1, 'stride': 0, 'magnitude_squared': False}\nfunc = tf.raw_ops.AudioSpectrogram\n\n@\ndef fuzz_jit():\n   y = func(**para)\n   return y\n\n\n\nfuzz_jit()",
        "Bug fix": [
            "@@ -5922,6 +5922,7 @@ tf_cuda_cc_test(\n         \"//tensorflow/core:test\",\n         \"//tensorflow/core:test_main\",\n         \"//tensorflow/core:testlib\",\n+        \"//tensorflow/core/platform:status_matchers\",\n     ],\n )\n \n",
            "@@ -19,6 +19,8 @@ limitations under the License.\n #include <memory>\n #include <vector>\n \n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n #include \"tensorflow/cc/client/client_session.h\"\n #include \"tensorflow/cc/ops/audio_ops.h\"\n #include \"tensorflow/cc/ops/const_op.h\"\n@@ -29,6 +31,9 @@ limitations under the License.\n #include \"tensorflow/core/kernels/ops_util.h\"\n #include \"tensorflow/core/lib/core/status_test_util.h\"\n #include \"tensorflow/core/platform/test.h\"\n+#include \"tensorflow/tsl/lib/core/status_test_util.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n+#include \"tensorflow/tsl/platform/status_matchers.h\"\n \n namespace tensorflow {\n namespace ops {\n@@ -140,6 +145,42 @@ TEST(SpectrogramOpTest, MultichannelTest) {\n   }\n }\n \n+TEST(SpectrogramOpTest, InvalidWindowSize) {\n+  Scope root = Scope::NewRootScope();\n+  const int audio_size = 8;\n+  const int channel_size = 2;\n+  Tensor audio_tensor(DT_FLOAT, TensorShape({audio_size, channel_size}));\n+  test::FillValues<float>(\n+      &audio_tensor, {-1.0f, -1.0f, 0.0f, 0.0f, 1.0f, 1.0f, 0.0f, 0.0f, -1.0f,\n+                      -1.0f, 0.0f, 0.0f, 1.0f, 1.0f, 0.0f, 0.0f});\n+  Output audio_const_op = Const(root.WithOpName(\"audio_const_op\"),\n+                                Input::Initializer(audio_tensor));\n+  AudioSpectrogram spectrogram_op =\n+      AudioSpectrogram(root.WithOpName(\"spectrogram_op\"), audio_const_op,\n+                       /*window_size=*/1, /*stride=*/1);\n+  EXPECT_THAT(root.status(),\n+              tsl::testing::StatusIs(tsl::error::Code::INVALID_ARGUMENT,\n+                                     ::testing::ContainsRegex(\"window size\")));\n+}\n+\n+TEST(SpectrogramOpTest, InvalidStride) {\n+  Scope root = Scope::NewRootScope();\n+  const int audio_size = 8;\n+  const int channel_size = 2;\n+  Tensor audio_tensor(DT_FLOAT, TensorShape({audio_size, channel_size}));\n+  test::FillValues<float>(\n+      &audio_tensor, {-1.0f, -1.0f, 0.0f, 0.0f, 1.0f, 1.0f, 0.0f, 0.0f, -1.0f,\n+                      -1.0f, 0.0f, 0.0f, 1.0f, 1.0f, 0.0f, 0.0f});\n+  Output audio_const_op = Const(root.WithOpName(\"audio_const_op\"),\n+                                Input::Initializer(audio_tensor));\n+  AudioSpectrogram spectrogram_op =\n+      AudioSpectrogram(root.WithOpName(\"spectrogram_op\"), audio_const_op,\n+                       /*window_size=*/2, /*stride=*/0);\n+  EXPECT_THAT(root.status(),\n+              tsl::testing::StatusIs(tsl::error::Code::INVALID_ARGUMENT,\n+                                     ::testing::ContainsRegex(\"stride\")));\n+}\n+\n }  // namespace\n }  // namespace ops\n }  // namespace tensorflow\n",
            "@@ -17,6 +17,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/op.h\"\n #include \"tensorflow/core/framework/shape_inference.h\"\n #include \"tensorflow/core/lib/core/bits.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n \n namespace tensorflow {\n \n@@ -72,8 +73,17 @@ Status SpectrogramShapeFn(InferenceContext* c) {\n   TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &input));\n   int32_t window_size;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"window_size\", &window_size));\n+  if (window_size <= 1) {\n+    return errors::InvalidArgument(\"window size must be > 1, got \",\n+                                   window_size);\n+  }\n+\n   int32_t stride;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"stride\", &stride));\n+  if (stride <= 0) {\n+    return errors::InvalidArgument(\"stride must be strictly positive, got \",\n+                                   stride);\n+  }\n \n   DimensionHandle input_length = c->Dim(input, 0);\n   DimensionHandle input_channels = c->Dim(input, 1);\n"
        ],
        "Title": "\n          FPE in AudioSpectrogram \n        "
    },
    {
        "Bug description": "import",
        "Sample Code": "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nimport tensorflow as tf\nprint(tf.__version__)\nwith tf.device(\"CPU\"):\n    ksize = [1, 40, 128, 1]\n    strides = [1, 128, 128, 30]\n    padding = \"SAME\"\n    data_format = \"NHWC\"\n    orig_input_shape = [11, 9, 78, 9]\n    grad = tf.saturate_cast(tf.random.uniform([16, 16, 16, 16], minval=-128, maxval=129, dtype=tf.int64), dtype=tf.float32)\n    res = tf.raw_ops.AvgPoolGrad(\n        ksize=ksize,\n        strides=strides,\n        padding=padding,\n        data_format=data_format,\n        orig_input_shape=orig_input_shape,\n        grad=grad,\n    ),\n    )",
        "Bug fix": [
            "@@ -342,6 +342,19 @@ class AvgPoolingGradOp : public OpKernel {\n     const T* out_backprop_ptr = out_backprop.flat<T>().data();\n     T* input_backprop_ptr = output->flat<T>().data();\n \n+    for (int64_t r = 0; r < out_backprop_rows; ++r) {\n+      int rindex, rsize;\n+      OP_REQUIRES_OK(context,\n+                     GetBroadcastSize(r, in_rows, window_rows, row_stride,\n+                                      pad_rows, &rindex, &rsize));\n+      for (int64_t c = 0; c < out_backprop_cols; ++c) {\n+        int cindex, csize;\n+        OP_REQUIRES_OK(context,\n+                       GetBroadcastSize(c, in_cols, window_cols, col_stride,\n+                                        pad_cols, &cindex, &csize));\n+      }\n+    }\n+\n     auto shard = [context, out_backprop_ptr, input_backprop_ptr,\n                   out_backprop_rows, out_backprop_cols, out_backprop_depth,\n                   in_rows, in_cols, window_rows, window_cols, row_stride,\n",
            "@@ -2510,6 +2510,21 @@ class PoolingTest(test.TestCase, parameterized.TestCase):\n             data_format=\"NHWC\")\n         self.evaluate(t)\n \n+  def testAvgPoolGradInvalidStrideRaiseErrorProperly(self):\n+    with self.assertRaises(errors_impl.InvalidArgumentError):\n+      with self.cached_session():\n+        orig_input_shape = [11, 9, 78, 9]\n+        grad = constant_op.constant(\n+            0.1, shape=[16, 16, 16, 16], dtype=dtypes.float64)\n+        t = gen_nn_ops.AvgPoolGrad(\n+            orig_input_shape=orig_input_shape,\n+            grad=grad,\n+            ksize=[1, 40, 128, 1],\n+            strides=[1, 128, 128, 30],\n+            padding=\"SAME\",\n+            data_format=\"NHWC\")\n+        self.evaluate(t)\n+\n \n def GetMaxPoolFwdTest(input_size, filter_size, strides, padding):\n \n"
        ],
        "Title": "\n          Heap-buffer-overflow in AvgPoolGrad \n        "
    },
    {
        "Bug description": "When ctx->step_containter() is a null ptr, the Lookup function will be executed with a null pointer.",
        "Sample Code": " tensorflow as tf\ntf.raw_ops.TensorArrayConcatV2(handle=['a', 'b'], flow_in = 0.1, dtype=tf.int32, element_shape_except0=1)",
        "Bug fix": [
            "@@ -80,8 +80,9 @@ Status GetTensorArray(OpKernelContext* ctx, TensorArray** tensor_array) {\n     TF_RETURN_IF_ERROR(GetHandle(ctx, &container, &ta_handle));\n     ResourceMgr* rm = ctx->resource_manager();\n     if (rm == nullptr) return errors::Internal(\"No resource manager.\");\n-    TF_RETURN_IF_ERROR(\n-        ctx->step_container()->Lookup(rm, container + ta_handle, tensor_array));\n+    ScopedStepContainer* sc = ctx->step_container();\n+    if (sc == nullptr) return errors::Internal(\"No step container.\");\n+    TF_RETURN_IF_ERROR(sc->Lookup(rm, container + ta_handle, tensor_array));\n     return OkStatus();\n   } else {\n     return LookupResource(ctx, HandleFromInput(ctx, 0), tensor_array);\n",
            "@@ -1846,6 +1846,22 @@ class TensorArrayTest(test.TestCase):\n     ta = ta.write(0, [0])\n     self.assertEqual([42, 1], ta.stack().shape.as_list())\n \n+  def testTensorArrayConcatFailsWhenMissingStepContainer(self):\n+    @def_function.function\n+    def func():\n+      y = data_flow_ops.TensorArrayConcatV2(\n+          handle=[\"a\", \"b\"],\n+          flow_in=0.1,\n+          dtype=dtypes.int32,\n+          element_shape_except0=1,\n+      )\n+      return y\n+\n+    with self.assertRaisesRegex(\n+        errors.NotFoundError, \"Container .* does not exist\"\n+    ):\n+      self.evaluate(func())\n+\n \n class TensorArrayBenchmark(test.Benchmark):\n \n"
        ],
        "Title": "\n          NPE in TensorArrayConcatV2\n        "
    },
    {
        "Bug description": "TFversion 2.11.0 //tensorflow/core/ops/array_ops.cc:1067 const Tensor* hypothesis_shape_t = c->input_tensor(2); std::vector dims(hypothesis_shape_t->NumElements() - 1); for (int i = 0; i < dims.size(); ++i) { dims[i] = c->MakeDim(std::max(h_values(i), t_values(i))); }",
        "Sample Code": "para={\n    'hypothesis_indices': [[]],\n    'hypothesis_values': ['tmp/'],\n    'hypothesis_shape': [],\n    'truth_indices': [[]],\n    'truth_values': [''],\n    'truth_shape': [],\n    'normalize': False\n    }\n\n    }\ntf.raw_ops.EditDistance(**para)",
        "Bug fix": [
            "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/framework/types.pb.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/platform/status.h\"\n #include \"tensorflow/core/platform/types.h\"\n #include \"tensorflow/core/util/mirror_pad_mode.h\"\n #include \"tensorflow/core/util/padding.h\"\n@@ -1072,13 +1073,24 @@ REGISTER_OP(\"EditDistance\")\n         // or else the output shape is unknown.\n         return shape_inference::UnknownShape(c);\n       }\n-\n       if (hypothesis_shape_t->NumElements() != truth_shape_t->NumElements()) {\n         return errors::InvalidArgument(\n             \"Num elements of hypothesis_shape does not match truth_shape: \",\n             hypothesis_shape_t->NumElements(), \" vs. \",\n             truth_shape_t->NumElements());\n       }\n+      if (hypothesis_shape_t->NumElements() < 2) {\n+        return errors::InvalidArgument(\n+            \"Input Hypothesis SparseTensors must have rank at least 2, but \"\n+            \"hypothesis_shape rank is: \",\n+            hypothesis_shape_t->NumElements());\n+      }\n+      if (truth_shape_t->NumElements() < 2) {\n+        return errors::InvalidArgument(\n+            \"Input Truth SparseTensors must have rank at least 2, but \"\n+            \"truth_shape rank is: \",\n+            truth_shape_t->NumElements());\n+      }\n \n       auto h_values = hypothesis_shape_t->flat<int64_t>();\n       auto t_values = truth_shape_t->flat<int64_t>();\n",
            "@@ -15,8 +15,9 @@\n \"\"\"Tests for tensorflow.kernels.edit_distance_op.\"\"\"\n \n import numpy as np\n-\n+from tensorflow.python.eager import def_function\n from tensorflow.python.framework import constant_op\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import sparse_tensor\n from tensorflow.python.ops import array_ops\n@@ -225,6 +226,66 @@ class EditDistanceTest(test.TestCase):\n                          \"to outside of the buffer for the output tensor|\"\n                          r\"Dimension -\\d+ must be >= 0\"))\n \n+  def testEmptyShapeWithEditDistanceRaisesError(self):\n+    para = {\n+        \"hypothesis_indices\": [[]],\n+        \"hypothesis_values\": [\"tmp/\"],\n+        \"hypothesis_shape\": [],\n+        \"truth_indices\": [[]],\n+        \"truth_values\": [\"\"],\n+        \"truth_shape\": [],\n+        \"normalize\": False,\n+    }\n+\n+    # Check edit distance raw op with empty shape in eager mode.\n+    with self.assertRaisesRegex(\n+        (errors.InvalidArgumentError, ValueError),\n+        (\n+            r\"Input Hypothesis SparseTensors must have rank at least 2, but\"\n+            \" hypothesis_shape rank is: 0|Input SparseTensors must have rank \"\n+            \"at least 2, but truth_shape rank is: 0\"\n+        ),\n+    ):\n+      array_ops.gen_array_ops.EditDistance(**para)\n+\n+    # Check raw op with tf.function\n+    @def_function.function\n+    def TestFunction():\n+      \"\"\"Wrapper function for edit distance call.\"\"\"\n+      array_ops.gen_array_ops.EditDistance(**para)\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        (\n+            \"Input Hypothesis SparseTensors must have rank at least 2, but\"\n+            \" hypothesis_shape rank is: 0\"\n+        ),\n+    ):\n+      TestFunction()\n+\n+    # Check with python wrapper API\n+    hypothesis_indices = [[]]\n+    hypothesis_values = [0]\n+    hypothesis_shape = []\n+    truth_indices = [[]]\n+    truth_values = [1]\n+    truth_shape = []\n+    expected_output = []  # dummy ignored\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        (\n+            \"Input Hypothesis SparseTensors must have rank at least 2, but\"\n+            \" hypothesis_shape rank is: 0\"\n+        ),\n+    ):\n+      self._testEditDistance(\n+          hypothesis=(hypothesis_indices, hypothesis_values, hypothesis_shape),\n+          truth=(truth_indices, truth_values, truth_shape),\n+          normalize=False,\n+          expected_output=expected_output,\n+      )\n+\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          Integer overflow in EditDistance\n        "
    },
    {
        "Bug description": "When the parameter  summarize  of  tf.raw_ops.Print  is zero, the new method  SummarizeArray<bool>  will reference to a nullptr, leading to a seg fault.",
        "Sample Code": "tf.raw_ops.Print(input =  tf.constant([1, 1, 1, 1],dtype=tf.int32),\n                            data =  [[False, False, False, False], [False], [False, False, False]],\n                            message =  'tmp/I',\n                            first_n = 100,\n                            ,\n                            summarize = 0)",
        "Bug fix": [
            "@@ -1297,6 +1297,9 @@ template <>\n string SummarizeArray<bool>(int64_t limit, int64_t num_elts,\n                             const TensorShape& tensor_shape, const char* data,\n                             const bool print_v2) {\n+  if (data == nullptr) {\n+    return strings::StrCat(\"\");  // we already print type and shape\n+  }\n   // We first convert all chars to be 0/1 to not get InvalidEnumValue sanitizer\n   // error\n   auto mutable_data = std::unique_ptr<char[]>(new char[num_elts]);\n"
        ],
        "Title": "\n          Seg fault in `tf.raw_ops.Print`\n        "
    },
    {
        "Bug description": "If the parameter  indices  for  DynamicStitch  does not match the shape of the parameter  data , it can trigger an stack OOB read.",
        "Sample Code": "func = tf.raw_ops.DynamicStitch\npara={'indices': [[0xdeadbeef], [405], [519], [758], [1015]], 'data': [[110.27793884277344], [120.29475402832031], [157.2418212890625], [157.2626953125], [188.45382690429688]]}\n]]}\ny = func(**para)",
        "Bug fix": [
            "@@ -146,6 +146,10 @@ class DynamicStitchOp : public XlaOpKernel {\n     for (int input_num = 0; input_num < indices.size(); input_num++) {\n       for (int i = 0; i < indices[input_num].shape().dimensions(0); ++i) {\n         int index = indices[input_num].Get<int>({i});\n+        OP_REQUIRES(\n+            ctx, index >= 0,\n+            errors::InvalidArgument(\"indices[\", index, \"] is out of range\"));\n+\n         src_input_vector[index] = input_num;\n         src_slice_vector[index] = i;\n         if (!src_index_used[index]) {\n",
            "@@ -97,6 +97,17 @@ class DynamicStitchOpImplBase : public OpKernel {\n \n     *first_dim_size = max_index + 1;\n \n+    for (const Tensor& indices : *indices_inputs) {\n+      auto indices_vec = indices.flat<int32>();\n+\n+      for (int i = 0; i < indices_vec.size(); i++) {\n+        int32_t index = internal::SubtleMustCopy(indices_vec(i));\n+        OP_REQUIRES(\n+            c, FastBoundsCheck(index, *first_dim_size),\n+            errors::InvalidArgument(\"indices[\", i, \"] is out of range\"));\n+      }\n+    }\n+\n     // Validate that data[i].shape = indices[i].shape + constant\n     OP_REQUIRES_OK(c, c->input_list(\"data\", data_inputs));\n     const Tensor& data0 = (*data_inputs)[0];\n@@ -265,9 +276,6 @@ class DynamicStitchOpImplCPU : public DynamicStitchOpImplBase<T> {\n           const T* data_base = data_flat.data();\n           for (int i = 0; i < indices_vec.size(); i++) {\n             int32_t index = internal::SubtleMustCopy(indices_vec(i));\n-            OP_REQUIRES(\n-                c, FastBoundsCheck(index, first_dim_size),\n-                errors::InvalidArgument(\"indices[\", i, \"] is out of range\"));\n             memcpy(merged_base + index * slice_size, data_base + i * slice_size,\n                    slice_bytes);\n           }\n@@ -277,9 +285,6 @@ class DynamicStitchOpImplCPU : public DynamicStitchOpImplBase<T> {\n             // Copy slice data[i] to merged[indices[i]]\n             Eigen::DSizes<Eigen::DenseIndex, 2> data_indices(i, 0);\n             int32_t index = internal::SubtleMustCopy(indices_vec(i));\n-            OP_REQUIRES(\n-                c, FastBoundsCheck(index, first_dim_size),\n-                errors::InvalidArgument(\"indices[\", i, \"] is out of range\"));\n             Eigen::DSizes<Eigen::DenseIndex, 2> merged_indices(index, 0);\n             merged_flat.slice(merged_indices, sizes) =\n                 data_flat.slice(data_indices, sizes);\n",
            "@@ -226,6 +226,19 @@ class DynamicStitchTestBase(object):\n     with self.assertRaises(ValueError):\n       self.stitch_op(indices, data)\n \n+  def testOutOfBoundsIndexRaisesInvalidArgument(self):\n+    with self.assertRaisesRegex(errors.InvalidArgumentError, \"out of range\"):\n+      indices = [[-1000], [405], [519], [758], [1015]]\n+      data = [\n+          [110.27793884277344],\n+          [120.29475402832031],\n+          [157.2418212890625],\n+          [157.2626953125],\n+          [188.45382690429688],\n+      ]\n+\n+      self.evaluate(self.stitch_op(indices, data))\n+\n \n class DynamicStitchTest(DynamicStitchTestBase, test.TestCase):\n \n"
        ],
        "Title": "\n          OOB read in DynamicStitch\n        "
    },
    {
        "Bug description": "Out of bounds read in GRUBlockCellGrad",
        "Sample Code": "para = {'x': [[21.1, 156.2], [83.3, 115.4]], 'h_prev': array([[136.5],\n      [136.6]]), 'w_ru': array([[26.7,  0.8],\n      [47.9, 26.1],\n      [26.2, 26.3]]), 'w_c': array([[ 0.4],\n      [31.5],\n      [ 0.6]]), 'b_ru': array([0.1, 0.2 ], dtype=float32), 'b_c': 0x41414141, 'r': array([[0.3],\n      [0.4]], dtype=float32), 'u': array([[5.7],\n      [5.8]]), 'c': array([[52.9],\n      [53.1]]), 'd_h': array([[172.2],\n      [],\n      [188.3 ]])}",
        "Bug fix": [
            "@@ -242,7 +242,6 @@ build:mkl_aarch64 -c opt\n # Config setting to build oneDNN with Compute Library for the Arm Architecture (ACL).\n # with Eigen threadpool support\n build:mkl_aarch64_threadpool --define=build_with_mkl_aarch64=true\n-build:mkl_aarch64_threadpool --define=build_with_acl=true\n build:mkl_aarch64_threadpool -c opt\n \n # This config refers to building CUDA op kernels with nvcc.\n",
            "@@ -411,6 +411,7 @@ cc_library(\n         \":internal\",\n         # We reuse VariableInfo in TFRT's implementation of TpuExecuteOp.\n         \"//learning/brain/tfrt/tf_tpu:__pkg__\",\n+        \"//learning/brain/tfrt/tpu_plugin:__pkg__\",\n         \"//learning/brain/tfrt/tpu_common:__pkg__\",\n         \"//tensorflow/core/common_runtime/next_pluggable_device:__pkg__\",\n     ],\n",
            "@@ -138,7 +138,7 @@ cc_library(\n         \":executor\",\n         \":platform_id\",\n         \"//tensorflow/compiler/xla/stream_executor\",\n-        \"//tensorflow/compiler/xla/stream_executor/lib\",\n+        \"//tensorflow/compiler/xla/stream_executor/platform\",\n         \"//tensorflow/tsl/platform:status\",\n         \"@com_google_absl//absl/strings:str_format\",\n     ],\n",
            "@@ -21,9 +21,9 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"tensorflow/compiler/xla/backends/interpreter/executor.h\"\n #include \"tensorflow/compiler/xla/stream_executor/device_options.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/multi_platform_manager.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/tsl/platform/status.h\"\n \n namespace stream_executor {\n",
            "@@ -222,7 +222,7 @@ Status MakeEvalErrorDueToParamOrInfeed(const HloInstruction& eval_instruction) {\n   absl::little_endian::Store32(\n       const_cast<char*>(error_payload.data()),\n       static_cast<uint32_t>(EvalErrorDetail::kDynamicValueDependence));\n-  error.SetPayload(kEvalErrorDetailUrl, error_payload);\n+  error.SetPayload(kEvalErrorDetailUrl, absl::Cord(error_payload));\n   return error;\n }\n \n",
            "@@ -9,6 +9,7 @@ xla_cc_binary(\n         \"//tensorflow/compiler/xla:debug_options_flags\",\n         \"//tensorflow/compiler/xla/mlir/runtime/ir:rt\",\n         \"//tensorflow/compiler/xla/mlir/tools/mlir_replay/public:compiler_trace_proto_cc\",\n+        \"//tensorflow/compiler/xla/mlir/tools/mlir_replay/public:compiler_trace_proto_cc_impl\",\n         \"//tensorflow/compiler/xla/mlir/tools/mlir_replay/public:execution_trace_proto_cc\",\n         \"//tensorflow/compiler/xla/mlir_hlo:gml_st\",\n         \"//tensorflow/compiler/xla/mlir_hlo:hlo_dialect_registration\",\n",
            "@@ -1251,6 +1251,10 @@ ExtractGemmBackendConfigProps(const gpu::GemmBackendConfig& config,\n   if (config.algorithm_case() == gpu::GemmBackendConfig::kSelectedAlgorithm) {\n     props.emplace_back(\"algorithm\", StrCat(config.selected_algorithm()));\n   }\n+  if (config.epilogue() != gpu::GemmBackendConfig::DEFAULT) {\n+    props.emplace_back(\n+        \"epilogue\", gpu::GemmBackendConfig::Epilogue_Name(config.epilogue()));\n+  }\n   return props;\n }\n \n",
            "@@ -239,6 +239,7 @@ cc_library(\n         \":stream_executor_headers\",\n         \"//tensorflow/compiler/xla/stream_executor/lib\",\n         \"//tensorflow/compiler/xla/stream_executor/platform\",\n+        \"//tensorflow/tsl/platform:errors\",\n         \"//tensorflow/tsl/platform:status\",\n         \"//tensorflow/tsl/platform:statusor\",\n         \"@com_google_absl//absl/strings\",\n@@ -450,6 +451,7 @@ tsl_gpu_library(\n         \"//tensorflow/compiler/xla/stream_executor/lib\",\n         \"//tensorflow/compiler/xla/stream_executor/platform\",\n         \"//tensorflow/tsl/platform:env\",\n+        \"//tensorflow/tsl/platform:errors\",\n         \"//tensorflow/tsl/platform:logging\",\n         \"//tensorflow/tsl/platform:stacktrace\",\n         \"//tensorflow/tsl/platform:status\",\n@@ -661,7 +663,7 @@ cc_library(\n         \":platform\",\n         \":plugin\",\n         \":stream_executor_headers\",\n-        \"//tensorflow/compiler/xla/stream_executor/lib\",\n+        \"//tensorflow/tsl/platform:errors\",\n         \"//tensorflow/tsl/platform:status\",\n         \"//tensorflow/tsl/platform:statusor\",\n         \"@com_google_absl//absl/base:core_headers\",\n",
            "@@ -63,7 +63,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_timer.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_types.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n",
            "@@ -36,8 +36,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_timer.h\"\n #include \"tensorflow/compiler/xla/stream_executor/dnn.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n #include \"tensorflow/compiler/xla/stream_executor/scratch_allocator.h\"\n@@ -85,7 +84,7 @@ static_assert(CUDNN_VERSION >= 7300, \"cuDNN needs to be version 7.3 or higher\");\n       std::ostringstream oss;                                           \\\n       oss << CudnnStatusToString(_status) << \"\\nin \" << __FILE__ << \"(\" \\\n           << __LINE__ << \"): '\" << #expr << \"'\";                        \\\n-      return tsl::Status(port::error::UNKNOWN, oss.str());              \\\n+      return tsl::Status(tsl::error::UNKNOWN, oss.str());               \\\n     }                                                                   \\\n   } while (false)\n \n@@ -96,7 +95,7 @@ static_assert(CUDNN_VERSION >= 7300, \"cuDNN needs to be version 7.3 or higher\");\n       std::ostringstream oss;                                           \\\n       oss << CudnnStatusToString(_status) << \"\\nin \" << __FILE__ << \"(\" \\\n           << __LINE__ << \"): '\" << #expr << \"' \" << (expr).get_error(); \\\n-      return tsl::Status(port::error::UNKNOWN, oss.str());              \\\n+      return tsl::Status(tsl::error::UNKNOWN, oss.str());               \\\n     }                                                                   \\\n   } while (false)\n \n@@ -417,7 +416,7 @@ tsl::Status CudnnSupport::Init() {\n           \"configuration.\");\n       LOG(ERROR) << error;\n       cudnnDestroy(cudnn_handle);\n-      return tsl::Status(port::error::INTERNAL, error);\n+      return tsl::Status(tsl::error::INTERNAL, error);\n     }\n \n     cudnn_.reset(new CudnnAccess(cudnn_handle));\n@@ -441,7 +440,7 @@ tsl::Status CudnnSupport::Init() {\n     }\n   }\n \n-  return tsl::Status(port::error::INTERNAL,\n+  return tsl::Status(tsl::error::INTERNAL,\n                      absl::StrCat(\"cudnn library could not create a handle: \",\n                                   CudnnStatusToString(status)));\n }\n@@ -1299,7 +1298,7 @@ class CudnnRnnDescriptor : public dnn::RnnDescriptor {\n             ? algorithm_config.algorithm()->tensor_ops_enabled()\n             : allow_tensor_ops;\n     if (use_tensor_ops && !allow_tensor_ops) {\n-      return tsl::Status(port::error::INVALID_ARGUMENT,\n+      return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                          \"Algo requests disallowed tensor op evaluation.\");\n     }\n \n@@ -1658,7 +1657,7 @@ class CudnnRnnSequenceTensorDescriptor\n       GpuExecutor* parent, int max_seq_length, int batch_size, int data_size,\n       cudnnDataType_t data_type) {\n     if (max_seq_length <= 0) {\n-      return tsl::Status(port::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n+      return tsl::Status(tsl::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n     }\n     int dims[] = {batch_size, data_size, 1};\n     int strides[] = {dims[1] * dims[2], dims[2], 1};\n@@ -1677,7 +1676,7 @@ class CudnnRnnSequenceTensorDescriptor\n       const absl::Span<const int>& seq_lengths, bool time_major,\n       cudnnDataType_t data_type) {\n     if (max_seq_length <= 0) {\n-      return tsl::Status(port::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n+      return tsl::Status(tsl::error::INVALID_ARGUMENT, \"max_seq_length <= 0\");\n     }\n     int dims[] = {batch_size, data_size, 1};\n     int strides[] = {dims[1] * dims[2], dims[2], 1};\n@@ -1804,30 +1803,30 @@ tsl::StatusOr<RnnModelDims> ExtractAndCheckRnnForward(\n             model_dims.num_layers * model_dims.dir_count &&\n         input_h_desc.batch_size() == model_dims.batch_size &&\n         input_h_desc.data_size() == model_dims.hidden_size)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid input_h shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid input_h shape\");\n   }\n   // The LSTM projection will be used if input_h_desc.data_size() <\n   // input_c_desc.data_size()\n   if (!(input_h_desc.num_layers() == input_c_desc.num_layers() &&\n         input_h_desc.batch_size() == input_c_desc.batch_size() &&\n         input_h_desc.data_size() <= input_c_desc.data_size())) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid input_c shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid input_c shape\");\n   }\n   if (!(output_desc.max_seq_length() == model_dims.max_seq_length &&\n         output_desc.batch_size() == model_dims.batch_size &&\n         output_desc.data_size() ==\n             model_dims.hidden_size * model_dims.dir_count)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid output shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid output shape\");\n   }\n   if (!(input_h_desc.num_layers() == output_h_desc.num_layers() &&\n         input_h_desc.batch_size() == output_h_desc.batch_size() &&\n         input_h_desc.data_size() == output_h_desc.data_size())) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid output_h shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid output_h shape\");\n   }\n   if (!(input_h_desc.num_layers() == output_c_desc.num_layers() &&\n         input_h_desc.batch_size() == output_c_desc.batch_size() &&\n         input_h_desc.data_size() <= output_c_desc.data_size())) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT, \"Invalid output_c shape\");\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT, \"Invalid output_c shape\");\n   }\n \n   return model_dims;\n@@ -1849,7 +1848,7 @@ tsl::Status CheckRNNParameterSize(\n #endif\n   if (static_cast<int64_t>(params_size_in_bytes) !=\n       rnn_desc.ParamsSizeInBytes()) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"Mismatching RNN parameter size\");\n   }\n   return ::tsl::OkStatus();\n@@ -1997,7 +1996,7 @@ tsl::Status CudnnSupport::DoRnnForwardImpl(\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n \n@@ -2020,7 +2019,7 @@ tsl::Status CudnnSupport::DoRnnForwardImpl(\n \n     if (is_profiling) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n       output_profile_result->set_algorithm(algo_desc);\n@@ -2058,7 +2057,7 @@ tsl::Status CudnnSupport::DoRnnForwardImpl(\n     // possible. It is still possible for other threads to issue workload on\n     // to this stream. So it could take multiple profiling measurements.\n     if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-      return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+      return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n     }\n   }\n \n@@ -2130,7 +2129,7 @@ tsl::Status CudnnSupport::DoRnnForwardImpl(\n \n   if (is_profiling) {\n     if (!timer->Stop(AsGpuStream(stream))) {\n-      return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+      return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n     }\n     auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n     output_profile_result->set_algorithm(algo_desc);\n@@ -2204,7 +2203,7 @@ tsl::Status CudnnSupport::DoRnnBackwardImpl(\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n \n@@ -2253,7 +2252,7 @@ tsl::Status CudnnSupport::DoRnnBackwardImpl(\n \n     if (is_profiling) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n       output_profile_result->set_algorithm(algo_desc);\n@@ -2275,7 +2274,7 @@ tsl::Status CudnnSupport::DoRnnBackwardImpl(\n     // possible. It is still possible for other threads to issue workload on\n     // to this stream. So it could take multiple profiling measurements.\n     if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-      return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+      return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n     }\n   }\n \n@@ -2362,7 +2361,7 @@ tsl::Status CudnnSupport::DoRnnBackwardImpl(\n \n   if (is_profiling) {\n     if (!timer->Stop(AsGpuStream(stream))) {\n-      return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+      return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n     }\n     auto algo_desc = *rnn_desc.algorithm_config().algorithm();\n     output_profile_result->set_algorithm(algo_desc);\n@@ -2404,7 +2403,7 @@ tsl::Status CudnnSupport::DoCtcLossImpl(\n       /*workspace=*/scratch_memory.opaque(),\n       /*workSpaceSizeInBytes=*/scratch_memory.size()));\n #else\n-  return tsl::Status(port::error::INVALID_ARGUMENT,\n+  return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                      \"No supported cudnnCTCLoss when \"\n                      \"CUDNN_VERSION < 7.6.3\");\n #endif\n@@ -2786,7 +2785,7 @@ tsl::StatusOr<cudnnConvolutionFwdAlgo_t> GetCudnnConvolutionForwardAlgo(\n       return perf_results[r].algo;\n     }\n   }\n-  return tsl::Status(port::error::INTERNAL,\n+  return tsl::Status(tsl::error::INTERNAL,\n                      \"cudnnGetConvolutionForwardAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n #else\n@@ -2828,7 +2827,7 @@ GetCudnnConvolutionBackwardDataAlgo(const CudnnHandle& cudnn,\n       return perf_results[r].algo;\n     }\n   }\n-  return tsl::Status(port::error::INTERNAL,\n+  return tsl::Status(tsl::error::INTERNAL,\n                      \"cudnnGetConvolutionBackwardDataAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n #else\n@@ -2870,7 +2869,7 @@ GetCudnnConvolutionBackwardFilterAlgo(const CudnnHandle& cudnn,\n       return perf_results[r].algo;\n     }\n   }\n-  return tsl::Status(port::error::INTERNAL,\n+  return tsl::Status(tsl::error::INTERNAL,\n                      \"cudnnGetConvolutionBackwardFilterAlgorithm_v7 returned \"\n                      \"no suitable algorithms. This could be a cudnn bug.\");\n #else\n@@ -2895,7 +2894,7 @@ tsl::StatusOr<DeviceMemory<uint8_t>> AllocateCudnnConvolutionForwardWorkspace(\n     ScratchAllocator* scratch_allocator) {\n   if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"Mismatch between cudnn conv and algorithm descriptors.\");\n   }\n \n@@ -2917,7 +2916,7 @@ tsl::StatusOr<DeviceMemory<uint8_t>> AllocateCudnnConvolutionForwardWorkspace(\n \n   if (ABSL_PREDICT_FALSE(size_in_bytes_int64_t < 0)) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"cudnnGetConvolutionForwardWorkspaceSize() returned \"\n         \"negative sizeInBytes value. This could be a cudnn bug.\");\n   }\n@@ -2927,7 +2926,7 @@ tsl::StatusOr<DeviceMemory<uint8_t>> AllocateCudnnConvolutionForwardWorkspace(\n   }\n \n   if (ABSL_PREDICT_FALSE(!scratch_allocator)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n   }\n \n@@ -2944,7 +2943,7 @@ AllocateCudnnConvolutionBackwardDataWorkspace(\n     ScratchAllocator* scratch_allocator) {\n   if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"Mismatch between cudnn conv and algorithm descriptors.\");\n   }\n \n@@ -2967,7 +2966,7 @@ AllocateCudnnConvolutionBackwardDataWorkspace(\n \n   if (ABSL_PREDICT_FALSE(size_in_bytes_int64_t < 0)) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"cudnnGetConvolutionBackwardDataWorkspaceSize() returned \"\n         \"negative sizeInBytes value. This could be a cudnn bug.\");\n   }\n@@ -2977,7 +2976,7 @@ AllocateCudnnConvolutionBackwardDataWorkspace(\n   }\n \n   if (ABSL_PREDICT_FALSE(!scratch_allocator)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n   }\n \n@@ -2994,7 +2993,7 @@ AllocateCudnnConvolutionBackwardFilterWorkspace(\n     ScratchAllocator* scratch_allocator) {\n   if (IsTensorMathOpSet(conv) != algorithm_desc.tensor_ops_enabled()) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"Mismatch between cudnn conv and algorithm descriptors.\");\n   }\n \n@@ -3017,7 +3016,7 @@ AllocateCudnnConvolutionBackwardFilterWorkspace(\n \n   if (ABSL_PREDICT_FALSE(size_in_bytes_int64_t < 0)) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         \"cudnnGetConvolutionBackwardFilterWorkspaceSize() returned \"\n         \"negative sizeInBytes value. This could be a cudnn bug.\");\n   }\n@@ -3027,7 +3026,7 @@ AllocateCudnnConvolutionBackwardFilterWorkspace(\n   }\n \n   if (ABSL_PREDICT_FALSE(!scratch_allocator)) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"No scratch allocator provided\");\n   }\n \n@@ -3040,7 +3039,7 @@ tsl::StatusOr<bool> UseTensorOps(Stream* stream, dnn::DataType type,\n   if (desc.has_value()) {\n     use_tensor_ops = desc->tensor_ops_enabled();\n     if (use_tensor_ops && !IsTensorMathEnabled(stream, type)) {\n-      return tsl::Status(port::error::INVALID_ARGUMENT,\n+      return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                          \"Algo requests disabled tensor op evaluation.\");\n     }\n   } else {\n@@ -3162,7 +3161,7 @@ tsl::StatusOr<dnn::AlgorithmDesc> GetCudnnConvolutionBackwardDataAlgorithm(\n   // no_scratch algorithm.\n   if (!algo_desc.has_value()) {\n     return tsl::Status(\n-        port::error::INVALID_ARGUMENT,\n+        tsl::error::INVALID_ARGUMENT,\n         \"The primary convolution algorithm failed memory allocation, \"\n         \"while a secondary algorithm is not provided.\");\n   }\n@@ -3224,7 +3223,7 @@ tsl::StatusOr<dnn::AlgorithmDesc> GetCudnnConvolutionBackwardFilterAlgorithm(\n   // no_scratch algorithm.\n   if (!algo_desc.has_value()) {\n     return tsl::Status(\n-        port::error::INVALID_ARGUMENT,\n+        tsl::error::INVALID_ARGUMENT,\n         absl::StrCat(\n             \"The primary convolution algorithm failed memory allocation, \"\n             \"while a secondary algorithm is not provided. Actual error: \",\n@@ -4254,7 +4253,7 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n \n@@ -4264,7 +4263,7 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n           ToCudnnDataType(input_type_) == CUDNN_DATA_INT8 &&\n           ToCudnnDataType(output_type_) == CUDNN_DATA_FLOAT) {\n         return tsl::Status(\n-            port::error::FAILED_PRECONDITION,\n+            tsl::error::FAILED_PRECONDITION,\n             \"This configuration potentially produces incorrect results.\");\n       }\n #else\n@@ -4336,7 +4335,7 @@ class CudnnLegacyConvRunner : public dnn::ConvRunner {\n \n     if (is_profiling) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       profile_result->set_algorithm(algo);\n       profile_result->set_elapsed_time_in_ms(timer->GetElapsedMilliseconds());\n@@ -4631,7 +4630,7 @@ class CudnnExecutionPlanRunner<void(Args...)>\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n \n@@ -4641,7 +4640,7 @@ class CudnnExecutionPlanRunner<void(Args...)>\n \n     if (is_profiling) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       TF_ASSIGN_OR_RETURN(auto desc, ToAlgorithmDesc());\n       profile_result->set_algorithm(desc);\n@@ -4868,7 +4867,7 @@ tsl::Status CudnnSupport::GetConvolveRunners(\n     }\n     if (!got_algos) {\n       return tsl::Status(\n-          port::error::UNKNOWN,\n+          tsl::error::UNKNOWN,\n           absl::StrFormat(\"Listing algorithms failed for kind %d\", kind));\n     }\n \n@@ -5037,7 +5036,7 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n       // possible. It is still possible for other threads to issue workload on\n       // to this stream. So it could take multiple profiling measurements.\n       if (!timer->Init() || !timer->Start(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to start timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to start timer\");\n       }\n     }\n     auto side_input_data_ptr = (side_input_scale_ == 0)\n@@ -5065,7 +5064,7 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n             << \"\\noutput_data.opaque() = \" << output_data.opaque();\n \n     if (IsTensorMathOpSet(conv_) != tensor_ops_enabled_) {\n-      return tsl::Status(port::error::FAILED_PRECONDITION,\n+      return tsl::Status(tsl::error::FAILED_PRECONDITION,\n                          \"Tensor op math type in dnn::AlgorithmDesc does not \"\n                          \"match that of the CudnnConvolutionDescriptor\");\n     }\n@@ -5095,7 +5094,7 @@ class CudnnLegacyFusedConvRunner : public dnn::FusedConvRunner {\n \n     if (profile_result) {\n       if (!timer->Stop(AsGpuStream(stream))) {\n-        return tsl::Status(port::error::INTERNAL, \"Failed to stop timer\");\n+        return tsl::Status(tsl::error::INTERNAL, \"Failed to stop timer\");\n       }\n       profile_result->set_algorithm(algo);\n       profile_result->set_elapsed_time_in_ms(timer->GetElapsedMilliseconds());\n@@ -5308,7 +5307,7 @@ tsl::Status CudnnSupport::GetFusedConvolveRunners(\n       activation_mode != dnn::ActivationMode::kElu &&\n       activation_mode != dnn::ActivationMode::kLeakyRelu &&\n       activation_mode != dnn::ActivationMode::kNone) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"CuDNN fusion only supports activations of \"\n                        \"{Relu, Relu6, Elu, <None>}.\");\n   }\n@@ -5319,7 +5318,7 @@ tsl::Status CudnnSupport::GetFusedConvolveRunners(\n     auto cuda_compute_capability = stream->GetCudaComputeCapability();\n     if (!GetConvolveAlgorithms(cuda_compute_capability, input_type,\n                                &algorithms)) {\n-      return tsl::Status(port::error::UNKNOWN,\n+      return tsl::Status(tsl::error::UNKNOWN,\n                          \"Listing fused convolve algorithms failed.\");\n     }\n \n@@ -5354,7 +5353,7 @@ tsl::Status CudnnSupport::GetFusedConvolveRunners(\n       leakyrelu_alpha, input_descriptor, filter_descriptor, bias_descriptor,\n       output_descriptor, convolution_descriptor, activation_mode, cudnn);\n   if (!op_graph_status.status().ok()) {\n-    return tsl::Status(port::error::INTERNAL,\n+    return tsl::Status(tsl::error::INTERNAL,\n                        absl::StrCat(\"Cudnn graph failed to build: \",\n                                     op_graph_status.status().ToString()));\n   }\n@@ -5391,7 +5390,7 @@ tsl::Status CudnnSupport::GetFusedMatmulRunners(\n       input_type, bias_type, output_type, trans_a, trans_b, m, n, k, lda, ldb,\n       ldc, activation_mode, cudnn);\n   if (!op_graph_status.status().ok()) {\n-    return tsl::Status(port::error::INTERNAL,\n+    return tsl::Status(tsl::error::INTERNAL,\n                        absl::StrCat(\"Cudnn graph failed to build: \",\n                                     op_graph_status.status().ToString()));\n   }\n@@ -5685,7 +5684,7 @@ tsl::Status CudnnSupport::DoBatchNormalizationForwardImpl(\n     if (activation_mode != dnn::ActivationMode::kNone ||\n         !side_input.is_null()) {\n       return tsl::Status(\n-          port::error::INTERNAL,\n+          tsl::error::INTERNAL,\n           absl::StrCat(\n               \"Side input and activation are not supported by cuDNN version: \",\n               CUDNN_VERSION));\n@@ -5968,7 +5967,7 @@ tsl::Status CudnnSupport::DoFusedConvolve(\n \n   if (activation_mode != dnn::ActivationMode::kRelu &&\n       activation_mode != dnn::ActivationMode::kNone) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"cudnnConvolutionBiasActivationForward() only supports \"\n                        \"Relu or None activation.\");\n   }\n@@ -6070,7 +6069,7 @@ tsl::Status CudnnSupport::DoPrepareForCtcLoss(\n   }\n   *ctc_loss_algo_id = algo;\n #else\n-  return tsl::Status(port::error::INVALID_ARGUMENT,\n+  return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                      \"No supported cudnnGetCTCLossWorkspaceSize when \"\n                      \"CUDNN_VERSION < 7.6.3\");\n #endif\n@@ -6100,7 +6099,7 @@ tsl::Status CudnnSupport::DoCtcLoss(\n     int ctc_loss_algo_id) {\n   // Current cuDNN CTC Loss only supports the float datatype\n   if (CUDNN_VERSION < 7603 || element_type != dnn::DataType::kFloat) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"CudnnCtcLossDescriptor is supported only when the \"\n                        \"CUDNN_VERSION >= 7.6.3 and DataType is float\");\n   }\n@@ -6382,7 +6381,7 @@ tsl::StatusOr<std::vector<PoolingSplitsSpec>> GetTensorSplits(\n \n   if (max_batches_per_split == 0) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrCat(\n             \"Tensor has too many elements for int32 indexing: batches=\",\n             num_batches, \" elements_per_batch=\", elements_per_batch_input,\n@@ -6442,7 +6441,7 @@ tsl::Status CudnnSupport::DoPoolForward(\n   auto splits_or =\n       GetTensorSplits(input_dimensions, output_dimensions, element_type);\n   if (!splits_or.ok()) {\n-    return tsl::Status(port::error::INTERNAL, \"Cudnn pooling failed to split\");\n+    return tsl::Status(tsl::error::INTERNAL, \"Cudnn pooling failed to split\");\n   }\n   auto splits = std::move(splits_or.value());\n \n@@ -6511,7 +6510,7 @@ tsl::Status CudnnSupport::DoPoolBackward(\n   auto splits_or =\n       GetTensorSplits(input_dimensions, output_dimensions, element_type);\n   if (!splits_or.ok()) {\n-    return tsl::Status(port::error::INTERNAL, \"Cudnn pooling failed to split\");\n+    return tsl::Status(tsl::error::INTERNAL, \"Cudnn pooling failed to split\");\n   }\n   auto splits = std::move(splits_or.value());\n \n",
            "@@ -35,10 +35,10 @@ limitations under the License.\n #include \"absl/synchronization/notification.h\"\n #include \"third_party/gpus/cuda/include/cuda_runtime_api.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/tsl/platform/env.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/stacktrace.h\"\n #include \"tensorflow/tsl/platform/static_threadlocal.h\"\n #include \"tensorflow/tsl/platform/threadpool.h\"\n@@ -267,7 +267,7 @@ static tsl::Status InternalInit() {\n   }\n \n   Diagnostician::LogDiagnosticInformation();\n-  return tsl::Status(port::error::ABORTED,\n+  return tsl::Status(tsl::error::ABORTED,\n                      absl::StrCat(\"failed call to cuInit: \", ToString(res)));\n }\n \n@@ -400,7 +400,7 @@ bool DeviceOptionsToContextFlags(const DeviceOptions& device_options,\n     }\n   }\n \n-  return tsl::Status(port::error::INTERNAL, message);\n+  return tsl::Status(tsl::error::INTERNAL, message);\n }\n \n /* static */ void GpuDriver::DestroyContext(GpuContext* context) {\n@@ -673,7 +673,7 @@ bool DeviceOptionsToContextFlags(const DeviceOptions& device_options,\n   }\n \n   return tsl::Status(\n-      port::error::INTERNAL,\n+      tsl::error::INTERNAL,\n       absl::StrCat(\"failed to get device for context: \", ToString(result)));\n }\n \n@@ -972,7 +972,7 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n /* static */ tsl::Status GpuDriver::DestroyEvent(GpuContext* context,\n                                                  CUevent* event) {\n   if (*event == nullptr) {\n-    return tsl::Status(port::error::INVALID_ARGUMENT,\n+    return tsl::Status(tsl::error::INVALID_ARGUMENT,\n                        \"input event cannot be null\");\n   }\n \n@@ -997,7 +997,7 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n   CUresult res = cuEventQuery(event);\n   if (res != CUDA_SUCCESS && res != CUDA_ERROR_NOT_READY) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\"failed to query event: %s\", ToString(res)));\n   }\n \n@@ -1263,11 +1263,11 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n   if (res == CUDA_SUCCESS) {\n     return ::tsl::OkStatus();\n   } else if (res == CUDA_ERROR_OUT_OF_MEMORY) {\n-    return tsl::Status(port::error::RESOURCE_EXHAUSTED,\n+    return tsl::Status(tsl::error::RESOURCE_EXHAUSTED,\n                        \"could not create CUDA event: out of device memory\");\n   } else {\n     return tsl::Status(\n-        port::error::FAILED_PRECONDITION,\n+        tsl::error::FAILED_PRECONDITION,\n         absl::StrCat(\"could not create CUDA event: \", ToString(res)));\n   }\n }\n@@ -1299,14 +1299,14 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n     // error then the original one.\n     if (context == nullptr) {\n       return tsl::Status(\n-          port::error::UNAVAILABLE,\n+          tsl::error::UNAVAILABLE,\n           \"Empty context returned while querying context for device pointer\");\n     }\n     return context;\n   }\n \n   return tsl::Status(\n-      port::error::INTERNAL,\n+      tsl::error::INTERNAL,\n       absl::StrCat(\"failed to query context for device pointer: \",\n                    ToString(result)));\n }\n@@ -1324,13 +1324,13 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n         return MemorySpace::kHost;\n       default:\n         return tsl::Status(\n-            port::error::INTERNAL,\n+            tsl::error::INTERNAL,\n             absl::StrCat(\"unknown memory space provided by CUDA API: \", value));\n     }\n   }\n \n   return tsl::Status(\n-      port::error::INTERNAL,\n+      tsl::error::INTERNAL,\n       absl::StrCat(\"failed to query device pointer for memory space: \",\n                    ToString(result)));\n }\n@@ -1346,13 +1346,13 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n     // \"there was an internal error while performing this operation\" (return\n     // below).\n     return tsl::Status(\n-        port::error::NOT_FOUND,\n+        tsl::error::NOT_FOUND,\n         absl::StrFormat(\"not a device pointer %p; %s\",\n                         reinterpret_cast<void*>(dptr), ToString(result)));\n   }\n \n   return tsl::Status(\n-      port::error::INTERNAL,\n+      tsl::error::INTERNAL,\n       absl::StrFormat(\"failed to get pointer into for device pointer %p; %s\",\n                       reinterpret_cast<void*>(dptr), ToString(result)));\n }\n@@ -1377,7 +1377,7 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n       cc_major, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, device);\n   if (res != CUDA_SUCCESS) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\n             \"failed to get compute capability major for device: %s; %d\",\n             ToString(res), device));\n@@ -1387,7 +1387,7 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n       cc_minor, CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, device);\n   if (res != CUDA_SUCCESS) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\n             \"failed to get compute capability minor for device: %s; %d\",\n             ToString(res), device));\n@@ -1399,13 +1399,13 @@ GpuDriver::CreateMemoryHandle(GpuContext* context, uint64_t bytes) {\n /* static */ tsl::Status GpuDriver::GetGpuISAVersion(int* version,\n                                                      CUdevice device) {\n   return tsl::Status{\n-      port::error::INTERNAL,\n+      tsl::error::INTERNAL,\n       \"Feature not supported on CUDA platform (GetGpuISAVersion)\"};\n }\n \n /* static */ tsl::Status GpuDriver::GetGpuGCNArchName(CUdevice, std::string*) {\n   return tsl::Status{\n-      port::error::INTERNAL,\n+      tsl::error::INTERNAL,\n       \"Feature not supported on CUDA platform (GetGpuGCNArchName)\"};\n }\n \n@@ -1519,7 +1519,7 @@ static tsl::StatusOr<T> GetSimpleAttribute(CUdevice device,\n   CUresult res = cuDeviceGetAttribute(&val, attribute, device);\n   if (res != CUDA_SUCCESS) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\"failed to get device attribute %d for device %d: %s\",\n                         attribute, device, ToString(res)));\n   }\n@@ -1628,7 +1628,7 @@ static tsl::StatusOr<T> GetSimpleAttribute(CUdevice device,\n   if (result != CUDA_SUCCESS &&\n       result != CUDA_ERROR_PEER_ACCESS_ALREADY_ENABLED) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\"failed to enable peer access from %p to %p: %s\", from,\n                         to, ToString(result)));\n   }\n",
            "@@ -26,7 +26,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_platform_id.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/device_memory.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n",
            "@@ -42,9 +42,8 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_timer.h\"\n #include \"tensorflow/compiler/xla/stream_executor/kernel_cache_config.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n@@ -53,6 +52,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor_pimpl.h\"\n #include \"tensorflow/compiler/xla/stream_executor/timer.h\"\n #include \"tensorflow/tsl/platform/env.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/numbers.h\"\n #include \"tensorflow/tsl/platform/statusor.h\"\n \n@@ -745,7 +745,7 @@ tsl::Status GpuExecutor::WaitForEvent(Stream* stream, Event* event) {\n     return ::tsl::OkStatus();\n   } else {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\"error recording waiting for CUDA event on stream %p\",\n                         stream));\n   }\n",
            "@@ -23,8 +23,8 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_platform_id.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/status.h\"\n \n namespace stream_executor {\n@@ -117,7 +117,7 @@ tsl::StatusOr<StreamExecutor*> CudaPlatform::FirstExecutorForBus(\n   }\n \n   return tsl::Status(\n-      port::error::NOT_FOUND,\n+      tsl::error::NOT_FOUND,\n       absl::StrFormat(\"Executor for bus %d not found.\", bus_ordinal));\n }\n \n@@ -177,7 +177,7 @@ CudaPlatform::GetUncachedExecutor(const StreamExecutorConfig& config) {\n   auto init_status = executor->Init(config.device_options);\n   if (!init_status.ok()) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\n             \"failed initializing StreamExecutor for CUDA device ordinal %d: %s\",\n             config.ordinal, init_status.ToString()));\n",
            "@@ -23,7 +23,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_platform_id.h\"\n #include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/device_memory.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rng.h\"\n #include \"tensorflow/tsl/platform/status.h\"\n",
            "@@ -46,7 +46,7 @@ cc_library(\n         \"//tensorflow/compiler/xla/stream_executor:stream_executor_headers\",\n         \"//tensorflow/compiler/xla/stream_executor/lib\",\n         \"//tensorflow/compiler/xla/stream_executor/platform\",\n-        \"@com_google_absl//absl/base:core_headers\",\n+        \"//tensorflow/tsl/platform:errors\",\n         \"@com_google_absl//absl/memory\",\n         \"@com_google_absl//absl/strings:str_format\",\n     ],\n@@ -104,9 +104,8 @@ cc_library(\n         \"//tensorflow/compiler/xla/stream_executor:kernel\",\n         \"//tensorflow/compiler/xla/stream_executor:rng\",\n         \"//tensorflow/compiler/xla/stream_executor:stream_executor_internal\",\n-        \"//tensorflow/compiler/xla/stream_executor:stream_executor_pimpl\",\n-        \"//tensorflow/compiler/xla/stream_executor:timer\",\n-        \"//tensorflow/compiler/xla/stream_executor/lib\",\n+        \"//tensorflow/compiler/xla/stream_executor:stream_executor_pimpl\",  # fixdeps: keep\n+        \"//tensorflow/tsl/platform:errors\",\n         \"//tensorflow/tsl/platform:platform_port\",\n         \"//tensorflow/tsl/platform/profile_utils:profile_utils_cpu_utils\",\n         \"@com_google_absl//absl/functional:any_invocable\",\n",
            "@@ -25,10 +25,10 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/blas.h\"\n #include \"tensorflow/compiler/xla/stream_executor/host/host_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/host/host_timer.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rng.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor_internal.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n namespace stream_executor {\n namespace host {\n",
            "@@ -21,8 +21,8 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"tensorflow/compiler/xla/stream_executor/host/host_gpu_executor.h\"\n #include \"tensorflow/compiler/xla/stream_executor/host/host_platform_id.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n namespace stream_executor {\n namespace host {\n@@ -75,7 +75,7 @@ HostPlatform::GetUncachedExecutor(const StreamExecutorConfig& config) {\n   auto init_status = executor->Init(config.device_options);\n   if (!init_status.ok()) {\n     return tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\n             \"failed initializing StreamExecutor for device ordinal %d: %s\",\n             config.ordinal, init_status.ToString().c_str()));\n",
            "@@ -31,7 +31,6 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/types:span\",\n-        \"//tensorflow/compiler/xla/stream_executor/platform\",\n         \"//tensorflow/tsl/platform:env\",\n         \"//tensorflow/tsl/platform:stacktrace\",\n         \"//tensorflow/tsl/platform:status\",\n",
            "@@ -1,30 +0,0 @@\n-/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-\n-#ifndef TENSORFLOW_COMPILER_XLA_STREAM_EXECUTOR_LIB_ERROR_H_\n-#define TENSORFLOW_COMPILER_XLA_STREAM_EXECUTOR_LIB_ERROR_H_\n-\n-#include \"tensorflow/tsl/protobuf/error_codes.pb.h\"  // IWYU pragma: export\n-\n-namespace stream_executor {\n-namespace port {\n-\n-namespace error = tensorflow::error;\n-\n-}  // namespace port\n-}  // namespace stream_executor\n-\n-#endif  // TENSORFLOW_COMPILER_XLA_STREAM_EXECUTOR_LIB_ERROR_H_\n",
            "@@ -1,21 +0,0 @@\n-/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef TENSORFLOW_COMPILER_XLA_STREAM_EXECUTOR_LIB_INITIALIZE_H_\n-#define TENSORFLOW_COMPILER_XLA_STREAM_EXECUTOR_LIB_INITIALIZE_H_\n-\n-#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n-\n-#endif  // TENSORFLOW_COMPILER_XLA_STREAM_EXECUTOR_LIB_INITIALIZE_H_\n",
            "@@ -24,8 +24,7 @@ limitations under the License.\n #include \"absl/strings/str_join.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/tsl/platform/errors.h\"\n \n namespace stream_executor {\n@@ -96,7 +95,7 @@ tsl::Status MultiPlatformManagerImpl::RegisterPlatform(\n   std::string key = absl::AsciiStrToLower(platform->Name());\n   absl::MutexLock lock(&mu_);\n   if (name_map_.find(key) != name_map_.end()) {\n-    return tsl::Status(port::error::INTERNAL,\n+    return tsl::Status(tsl::error::INTERNAL,\n                        \"platform is already registered with name: \\\"\" +\n                            platform->Name() + \"\\\"\");\n   }\n@@ -156,7 +155,7 @@ tsl::StatusOr<Platform*> MultiPlatformManagerImpl::InitializePlatformWithName(\n   TF_ASSIGN_OR_RETURN(Platform * platform, LookupByNameLocked(target));\n   if (platform->Initialized()) {\n     return tsl::Status(\n-        port::error::FAILED_PRECONDITION,\n+        tsl::error::FAILED_PRECONDITION,\n         absl::StrCat(\"platform \\\"\", target, \"\\\" is already initialized\"));\n   }\n \n@@ -172,7 +171,7 @@ tsl::StatusOr<Platform*> MultiPlatformManagerImpl::InitializePlatformWithId(\n   TF_ASSIGN_OR_RETURN(Platform * platform, LookupByIdLocked(id));\n   if (platform->Initialized()) {\n     return tsl::Status(\n-        port::error::FAILED_PRECONDITION,\n+        tsl::error::FAILED_PRECONDITION,\n         absl::StrFormat(\"platform with id %p is already initialized\", id));\n   }\n \n@@ -232,7 +231,7 @@ tsl::StatusOr<Platform*> MultiPlatformManagerImpl::LookupByNameLocked(\n   auto it = name_map_.find(absl::AsciiStrToLower(target));\n   if (it == name_map_.end()) {\n     return tsl::Status(\n-        port::error::NOT_FOUND,\n+        tsl::error::NOT_FOUND,\n         absl::StrCat(\"Could not find registered platform with name: \\\"\", target,\n                      \"\\\". Available platform names are: \",\n                      absl::StrJoin(InitializedPlatformNamesWithFilter(), \" \")));\n@@ -245,7 +244,7 @@ tsl::StatusOr<Platform*> MultiPlatformManagerImpl::LookupByIdLocked(\n   auto it = id_map_.find(id);\n   if (it == id_map_.end()) {\n     return tsl::Status(\n-        port::error::NOT_FOUND,\n+        tsl::error::NOT_FOUND,\n         absl::StrFormat(\"could not find registered platform with id: %p\", id));\n   }\n   return it->second;\n",
            "@@ -70,8 +70,8 @@ limitations under the License.\n #include <vector>\n \n #include \"absl/strings/string_view.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/tsl/platform/status.h\"\n #include \"tensorflow/tsl/platform/statusor.h\"\n",
            "@@ -16,10 +16,10 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/platform.h\"\n \n #include \"absl/strings/str_cat.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor_pimpl.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n namespace stream_executor {\n \n@@ -92,14 +92,14 @@ bool Platform::Initialized() const { return true; }\n tsl::Status Platform::Initialize(\n     const std::map<std::string, std::string> &platform_options) {\n   if (!platform_options.empty()) {\n-    return tsl::Status(port::error::UNIMPLEMENTED,\n+    return tsl::Status(tsl::error::UNIMPLEMENTED,\n                        \"this platform does not support custom initialization\");\n   }\n   return ::tsl::OkStatus();\n }\n \n tsl::Status Platform::ForceExecutorShutdown() {\n-  return tsl::Status(port::error::UNIMPLEMENTED,\n+  return tsl::Status(tsl::error::UNIMPLEMENTED,\n                      \"executor shutdown is not supported on this platform\");\n }\n \n",
            "@@ -19,8 +19,8 @@ limitations under the License.\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"absl/synchronization/mutex.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/multi_platform_manager.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n namespace stream_executor {\n \n@@ -76,7 +76,7 @@ tsl::Status PluginRegistry::RegisterFactoryInternal(\n \n   if (factories->find(plugin_id) != factories->end()) {\n     return tsl::Status(\n-        port::error::ALREADY_EXISTS,\n+        tsl::error::ALREADY_EXISTS,\n         absl::StrFormat(\"Attempting to register factory for plugin %s when \"\n                         \"one has already been registered\",\n                         plugin_name));\n@@ -96,7 +96,7 @@ tsl::StatusOr<FACTORY_TYPE> PluginRegistry::GetFactoryInternal(\n     iter = generic_factories.find(plugin_id);\n     if (iter == generic_factories.end()) {\n       return tsl::Status(\n-          port::error::NOT_FOUND,\n+          tsl::error::NOT_FOUND,\n           absl::StrFormat(\"Plugin ID %p not registered.\", plugin_id));\n     }\n   }\n@@ -217,7 +217,7 @@ bool PluginRegistry::HasFactory(Platform::Id platform_id,\n                                                                               \\\n       if (plugin_id == kNullPlugin) {                                         \\\n         return tsl::Status(                                                   \\\n-            port::error::FAILED_PRECONDITION,                                 \\\n+            tsl::error::FAILED_PRECONDITION,                                  \\\n             \"No suitable \" PLUGIN_STRING                                      \\\n             \" plugin registered. Have you linked in a \" PLUGIN_STRING         \\\n             \"-providing plugin?\");                                            \\\n@@ -236,7 +236,7 @@ bool PluginRegistry::HasFactory(Platform::Id platform_id,\n       PlatformKind platform_kind, PluginId plugin_id) {                       \\\n     auto iter = platform_id_by_kind_.find(platform_kind);                     \\\n     if (iter == platform_id_by_kind_.end()) {                                 \\\n-      return tsl::Status(port::error::FAILED_PRECONDITION,                    \\\n+      return tsl::Status(tsl::error::FAILED_PRECONDITION,                     \\\n                          absl::StrFormat(\"Platform kind %d not registered.\",  \\\n                                          static_cast<int>(platform_kind)));   \\\n     }                                                                         \\\n",
            "@@ -32,8 +32,8 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_helpers.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_timer.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/dso_loader.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n",
            "@@ -35,8 +35,8 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"absl/strings/str_split.h\"\n #include \"absl/strings/strip.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/host_info.h\"\n \n namespace stream_executor {\n",
            "@@ -31,9 +31,8 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_executor.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_timer.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/dso_loader.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rocm/rocm_diagnostics.h\"\n@@ -42,6 +41,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor_pimpl.h\"\n #include \"tensorflow/tsl/platform/env.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/hash.h\"\n #include \"tensorflow/tsl/util/determinism.h\"\n #include \"tensorflow/tsl/util/env_var.h\"\n",
            "@@ -28,11 +28,11 @@ limitations under the License.\n #include \"absl/synchronization/notification.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_diagnostics.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_driver.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rocm/rocm_driver_wrapper.h\"\n #include \"tensorflow/tsl/platform/env.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/numbers.h\"\n #include \"tensorflow/tsl/platform/stacktrace.h\"\n #include \"tensorflow/tsl/platform/static_threadlocal.h\"\n",
            "@@ -22,8 +22,8 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_executor.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_helpers.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_stream.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/dso_loader.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n",
            "@@ -28,10 +28,9 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_timer.h\"\n #include \"tensorflow/compiler/xla/stream_executor/kernel_cache_config.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/dso_loader.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/plugin_registry.h\"\n@@ -42,6 +41,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor_pimpl.h\"\n #include \"tensorflow/compiler/xla/stream_executor/timer.h\"\n #include \"tensorflow/tsl/platform/env.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n #ifdef PLATFORMS_GPUS_ROCM_DYNAMIC_LIBROCM_DYNAMIC_LIBROCM_H_\n #error \\\n",
            "@@ -21,9 +21,9 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_driver.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_executor.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rocm/rocm_platform_id.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n \n namespace stream_executor {\n namespace gpu {\n",
            "@@ -20,8 +20,8 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_helpers.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_rng.h\"\n #include \"tensorflow/compiler/xla/stream_executor/gpu/gpu_stream.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/dso_loader.h\"\n+#include \"tensorflow/compiler/xla/stream_executor/platform/initialize.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/logging.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rng.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rocm/rocm_platform_id.h\"\n",
            "@@ -32,11 +32,11 @@ limitations under the License.\n #include \"absl/synchronization/notification.h\"\n #include \"tensorflow/compiler/xla/stream_executor/blas.h\"\n #include \"tensorflow/compiler/xla/stream_executor/fft.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/platform/port.h\"\n #include \"tensorflow/compiler/xla/stream_executor/rng.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor_internal.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/stacktrace.h\"\n #include \"tensorflow/tsl/platform/statusor.h\"\n #include \"tensorflow/tsl/platform/threadpool.h\"\n@@ -405,7 +405,7 @@ StreamExecutor::createRnnDescriptor(\n     bool use_padded_io) {\n   dnn::DnnSupport* dnn_support = AsDnn();\n   if (!dnn_support) {\n-    return tsl::Status(port::error::UNKNOWN,\n+    return tsl::Status(tsl::error::UNKNOWN,\n                        \"Fail to find the dnn implementation.\");\n   }\n   return dnn_support->createRnnDescriptor(\n@@ -420,7 +420,7 @@ StreamExecutor::createRnnSequenceTensorDescriptor(int max_seq_length,\n                                                   dnn::DataType data_type) {\n   dnn::DnnSupport* dnn_support = AsDnn();\n   if (!dnn_support) {\n-    return tsl::Status(port::error::UNKNOWN,\n+    return tsl::Status(tsl::error::UNKNOWN,\n                        \"Fail to find the dnn implementation.\");\n   }\n   return dnn_support->createRnnSequenceTensorDescriptor(\n@@ -434,7 +434,7 @@ StreamExecutor::createRnnSequenceTensorDescriptor(\n     dnn::DataType data_type) {\n   dnn::DnnSupport* dnn_support = AsDnn();\n   if (!dnn_support) {\n-    return tsl::Status(port::error::UNKNOWN,\n+    return tsl::Status(tsl::error::UNKNOWN,\n                        \"Fail to find the dnn implementation.\");\n   }\n   return dnn_support->createRnnSequenceTensorDescriptor(\n@@ -448,7 +448,7 @@ StreamExecutor::createRnnStateTensorDescriptor(int num_layer, int batch_size,\n                                                dnn::DataType data_type) {\n   dnn::DnnSupport* dnn_support = AsDnn();\n   if (!dnn_support) {\n-    return tsl::Status(port::error::UNKNOWN,\n+    return tsl::Status(tsl::error::UNKNOWN,\n                        \"Fail to find the dnn implementation.\");\n   }\n   return dnn_support->createRnnStateTensorDescriptor(num_layer, batch_size,\n@@ -546,7 +546,7 @@ tsl::StatusOr<DeviceMemoryBase> StreamExecutor::GetUntypedSymbol(\n   }\n \n   return tsl::Status(\n-      port::error::NOT_FOUND,\n+      tsl::error::NOT_FOUND,\n       absl::StrCat(\"Check if module containing symbol \", symbol_name,\n                    \" is loaded (module_handle = \",\n                    reinterpret_cast<uintptr_t>(module_handle.id()), \")\"));\n@@ -691,7 +691,7 @@ tsl::Status StreamExecutor::SynchronousMemcpyD2H(\n   result = implementation_->SynchronousMemcpy(host_dst, device_src, size);\n   if (!result.ok()) {\n     result = tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\"failed to synchronously memcpy device-to-host: device \"\n                         \"%p to host %p size %d: %s\",\n                         device_src.opaque(), host_dst, size,\n@@ -715,7 +715,7 @@ tsl::Status StreamExecutor::SynchronousMemcpyH2D(const void* host_src,\n   result = implementation_->SynchronousMemcpy(device_dst, host_src, size);\n   if (!result.ok()) {\n     result = tsl::Status(\n-        port::error::INTERNAL,\n+        tsl::error::INTERNAL,\n         absl::StrFormat(\"failed to synchronously memcpy host-to-device: host \"\n                         \"%p to device %p size %d: %s\",\n                         host_src, device_dst->opaque(), size,\n",
            "@@ -16,7 +16,6 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/stream_executor/tf_allocator_adapter.h\"\n \n #include \"absl/synchronization/mutex.h\"\n-#include \"tensorflow/compiler/xla/stream_executor/lib/error.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream.h\"\n #include \"tensorflow/compiler/xla/stream_executor/stream_executor.h\"\n #include \"tensorflow/tsl/platform/errors.h\"\n",
            "@@ -535,7 +535,7 @@ class CallOp : public AsyncOpKernel {\n     OP_REQUIRES_ASYNC(ctx, lib != nullptr,\n                       errors::Internal(\"No function library is provided.\"),\n                       done);\n-    FunctionLibraryRuntime::Options opts;\n+    FunctionLibraryRuntime::Options opts(ctx->step_id());\n     opts.rendezvous = ctx->rendezvous();\n     opts.cancellation_manager = ctx->cancellation_manager();\n     opts.step_container = ctx->step_container();\n",
            "@@ -257,6 +257,7 @@ cc_library(\n     # copybara:uncomment copts = [\"-Wthread-safety-analysis\"],\n     visibility = [\"//visibility:public\"],\n     deps = [\n+        \"//tensorflow/core:framework\",\n         \"//tensorflow/core:lib\",\n         \"//tensorflow/core/platform:env\",\n         \"//tensorflow/core/platform:mutex\",\n",
            "@@ -37,7 +37,7 @@ constexpr char kPortPlaceholder[] = \"%port%\";\n }\n \n GrpcDataServerBase::GrpcDataServerBase(\n-    int port, const std::string& protocol, const std::string server_type,\n+    int port, const std::string& protocol, const std::string& server_type,\n     std::vector<std::unique_ptr<::grpc::ServerBuilderOption>> options)\n     : requested_port_(port),\n       protocol_(protocol),\n",
            "@@ -44,7 +44,7 @@ class GrpcDataServerBase {\n   // found by calling `BoundPort()`.\n   GrpcDataServerBase(\n       int requested_port, const std::string& protocol,\n-      const std::string server_type,\n+      const std::string& server_type,\n       std::vector<std::unique_ptr<::grpc::ServerBuilderOption>> options = {});\n   virtual ~GrpcDataServerBase() = default;\n \n",
            "@@ -84,7 +84,9 @@ cc_library(\n     hdrs = [\"path_utils.h\"],\n     compatible_with = get_compatible_with_portable(),\n     deps = [\n+        \"//tensorflow/tsl/platform:errors\",\n         \"//tensorflow/tsl/platform:path\",\n+        \"//tensorflow/tsl/platform:statusor\",\n         \"@com_google_absl//absl/strings\",\n     ],\n )\n@@ -96,6 +98,8 @@ tf_cc_test(\n         \":path_utils\",\n         \"//tensorflow/core:test\",\n         \"//tensorflow/core:test_main\",\n+        \"//tensorflow/tsl/platform:status_matchers\",\n+        \"//tensorflow/tsl/protobuf:protos_all_cc\",\n     ],\n )\n \n",
            "@@ -15,10 +15,15 @@ limitations under the License.\n #include \"tensorflow/core/data/service/snapshot/path_utils.h\"\n \n #include <string>\n+#include <utility>\n+#include <vector>\n \n #include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n+#include \"tensorflow/tsl/platform/errors.h\"\n #include \"tensorflow/tsl/platform/path.h\"\n+#include \"tensorflow/tsl/platform/statusor.h\"\n \n namespace tensorflow {\n namespace data {\n@@ -65,6 +70,28 @@ std::string SplitPath(absl::string_view snapshot_path, int64_t stream_index,\n       absl::StrCat(\"split_\", local_index, \"_\", global_index));\n }\n \n+tsl::StatusOr<std::pair<int64_t, int64_t>> SplitIndex(\n+    absl::string_view split_path) {\n+  std::vector<std::string> tokens = absl::StrSplit(split_path, '_');\n+  int64_t local_split_index = 0, global_split_index = 0;\n+  if (tokens.size() != 3 || tokens[0] != \"split\" ||\n+      !absl::SimpleAtoi(tokens[1], &local_split_index) ||\n+      local_split_index < 0 ||\n+      !absl::SimpleAtoi(tokens[2], &global_split_index) ||\n+      global_split_index < 0) {\n+    return tsl::errors::InvalidArgument(\n+        \"Invalid split file name: \", split_path,\n+        \". Expected split_<local_split_index>_<global_split_index>.\");\n+  }\n+  if (local_split_index > global_split_index) {\n+    return tsl::errors::InvalidArgument(\n+        \"Invalid split file name: \", split_path, \". The local split index \",\n+        local_split_index, \" exceeds the global split index \",\n+        global_split_index, \".\");\n+  }\n+  return std::make_pair(local_split_index, global_split_index);\n+}\n+\n std::string SnapshotMetadataFilePath(absl::string_view snapshot_path_) {\n   return tsl::io::JoinPath(snapshot_path_, kSnapshotMetadataFileName);\n }\n",
            "@@ -16,8 +16,10 @@ limitations under the License.\n #define TENSORFLOW_CORE_DATA_SERVICE_SNAPSHOT_PATH_UTILS_H_\n \n #include <string>\n+#include <utility>\n \n #include \"absl/strings/string_view.h\"\n+#include \"tensorflow/tsl/platform/statusor.h\"\n \n namespace tensorflow {\n namespace data {\n@@ -45,6 +47,12 @@ std::string SplitPath(absl::string_view snapshot_path, int64_t stream_index,\n                       int64_t source_id, int64_t local_index,\n                       int64_t global_index);\n \n+// Returns a pair of {local_split_index, global_split_index} of the split. The\n+// expected format of `split_path` is:\n+// split_<local_split_index>_<global_split_index>\n+tsl::StatusOr<std::pair<int64_t, int64_t>> SplitIndex(\n+    absl::string_view split_path);\n+\n // Returns the path of the DONE file of a snapshot stream.\n std::string StreamDoneFilePath(absl::string_view snapshot_path,\n                                int64_t stream_index);\n",
            "@@ -14,13 +14,19 @@ limitations under the License.\n ==============================================================================*/\n #include \"tensorflow/core/data/service/snapshot/path_utils.h\"\n \n+#include \"tensorflow/tsl/platform/status_matchers.h\"\n #include \"tensorflow/tsl/platform/test.h\"\n+#include \"tensorflow/tsl/protobuf/error_codes.pb.h\"\n \n namespace tensorflow {\n namespace data {\n namespace {\n \n+using ::testing::HasSubstr;\n using ::testing::MatchesRegex;\n+using ::testing::Pair;\n+using tsl::testing::IsOkAndHolds;\n+using tsl::testing::StatusIs;\n \n TEST(PathUtilsTest, StreamsDirectory) {\n   EXPECT_THAT(StreamsDirectory(\"/path/to/snapshot\"),\n@@ -51,6 +57,34 @@ TEST(PathUtilsTest, SplitPath) {\n           \"/path/to/snapshot.streams.stream_0.splits.source_1.split_2_3\"));\n }\n \n+TEST(PathUtilsTest, SplitIndex) {\n+  EXPECT_THAT(SplitIndex(\"split_0_1\"), IsOkAndHolds(Pair(0, 1)));\n+}\n+\n+TEST(PathUtilsTest, InvalidSplitFile) {\n+  EXPECT_THAT(\n+      SplitIndex(\"\"),\n+      StatusIs(error::INVALID_ARGUMENT,\n+               HasSubstr(\n+                   \"Expected split_<local_split_index>_<global_split_index>\")));\n+  EXPECT_THAT(\n+      SplitIndex(\"split_123\"),\n+      StatusIs(error::INVALID_ARGUMENT,\n+               HasSubstr(\n+                   \"Expected split_<local_split_index>_<global_split_index>\")));\n+  EXPECT_THAT(\n+      SplitIndex(\"split_-1_(-1)\"),\n+      StatusIs(error::INVALID_ARGUMENT,\n+               HasSubstr(\n+                   \"Expected split_<local_split_index>_<global_split_index>\")));\n+  EXPECT_THAT(\n+      SplitIndex(\"split_5_0\"),\n+      StatusIs(\n+          error::INVALID_ARGUMENT,\n+          HasSubstr(\n+              \"The local split index 5 exceeds the global split index 0\")));\n+}\n+\n TEST(PathUtilsTest, StreamDoneFilePath) {\n   EXPECT_THAT(StreamDoneFilePath(\"/path/to/snapshot\", /*stream_index=*/0),\n               MatchesRegex(\"/path/to/snapshot.streams.stream_0.DONE\"));\n",
            "@@ -187,21 +187,8 @@ Status SnapshotManager::ReadOnDiskSource(\n \n     // `split_filename` must have this format:\n     // \"split_<local_split_index>_<global_split_index>\".\n-    std::vector<std::string> tokens = absl::StrSplit(split_filename, '_');\n-    int64_t local_split_index;\n-    int64_t global_split_index;\n-    if (tokens.size() != 3 ||\n-        !absl::SimpleAtoi(tokens[1], &local_split_index) ||\n-        local_split_index < 0 ||\n-        !absl::SimpleAtoi(tokens[2], &global_split_index) ||\n-        global_split_index < 0) {\n-      return InvalidArgument(\"can't parse the name of \", split_path);\n-    }\n-    if (local_split_index > global_split_index) {\n-      return InvalidArgument(\n-          \"found conflict between local split index and global split index in \",\n-          \"name of \", split_path);\n-    }\n+    TF_ASSIGN_OR_RETURN(auto split_index, SplitIndex(split_filename));\n+    auto [local_split_index, global_split_index] = split_index;\n     if (local_split_index > split_filenames.size() - 1) {\n       return InvalidArgument(\n           \"found conflict between the number of splits and name of \",\n",
            "@@ -88,8 +88,9 @@ absl::Duration ApproximateLatencyEstimator::GetAverageLatency(Duration duration)\n   return absl::Duration(absl::Microseconds(interval_latency)) / interval_count;\n }\n \n-TfDatazMetricsCollector::TfDatazMetricsCollector(const Env& env)\n-    : latency_estimator_(env) {}\n+TfDatazMetricsCollector::TfDatazMetricsCollector(const Env& env,\n+                                                 IteratorBase* iterator)\n+    : iterator_(iterator), latency_estimator_(env) {}\n \n void TfDatazMetricsCollector::RecordGetNextLatency(\n     int64_t get_next_latency_usec) {\n@@ -113,6 +114,10 @@ absl::Duration TfDatazMetricsCollector::GetAverageLatencyForLastSixtyMinutes() {\n       ApproximateLatencyEstimator::Duration::kSixtyMinutes);\n }\n \n+int64_t TfDatazMetricsCollector::GetIteratorTotalMemoryUsage() {\n+  return iterator_->TotalBufferedBytes();\n+}\n+\n namespace {\n static mutex* get_tfdataz_metrics_registry_lock() {\n   static mutex tfdataz_metrics_registry_lock(LINKER_INITIALIZED);\n",
            "@@ -25,6 +25,7 @@ limitations under the License.\n \n #include \"absl/container/flat_hash_set.h\"\n #include \"absl/time/time.h\"\n+#include \"tensorflow/core/framework/dataset.h\"\n #include \"tensorflow/core/platform/env.h\"\n #include \"tensorflow/core/platform/mutex.h\"\n #include \"tensorflow/core/platform/thread_annotations.h\"\n@@ -95,7 +96,7 @@ class TfDatazMetricsCollector {\n   // We only collect metrics for CPU devices. This is a heuristic to avoid\n   // collecting metrics for device-side iterators created by the multi-device\n   // iterator mechanism.\n-  explicit TfDatazMetricsCollector(const Env& env);\n+  TfDatazMetricsCollector(const Env& env, IteratorBase* iterator);\n \n   // Records `GetNext` call latency.\n   void RecordGetNextLatency(int64_t get_next_latency_usec);\n@@ -109,7 +110,13 @@ class TfDatazMetricsCollector {\n   // Returns the average `GetNext` latency for past 60 minutes.\n   absl::Duration GetAverageLatencyForLastSixtyMinutes();\n \n+  // Returns the total memory (in bytes) used by the iterator.\n+  // Total memory used by the iterator includes the total number of bytes\n+  // buffered in all nodes in the subtree.\n+  int64_t GetIteratorTotalMemoryUsage();\n+\n  private:\n+  IteratorBase* iterator_;  // not owned\n   ApproximateLatencyEstimator latency_estimator_;\n };\n \n",
            "@@ -18,7 +18,7 @@ limitations under the License.\n #include <utility>\n \n #include \"absl/time/time.h\"\n-#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/dataset.h\"\n #include \"tensorflow/core/platform/env.h\"\n #include \"tensorflow/core/platform/test.h\"\n #include \"tensorflow/core/util/fake_clock_env.h\"\n@@ -41,7 +41,8 @@ class TfDatazMetricsTest : public ::testing::Test {\n  protected:\n   void SetUp() override {\n     env_ = std::make_unique<FakeClockEnv>(Env::Default());\n-    tfdataz_metrics_ = std::make_unique<TfDatazMetricsCollector>(*env_);\n+    tfdataz_metrics_ =\n+        std::make_unique<TfDatazMetricsCollector>(*env_, iterator_.get());\n   }\n \n   void TearDown() override {\n@@ -49,6 +50,7 @@ class TfDatazMetricsTest : public ::testing::Test {\n     tfdataz_metrics_.reset();\n   }\n \n+  std::unique_ptr<IteratorBase> iterator_;\n   std::unique_ptr<FakeClockEnv> env_;\n   std::unique_ptr<TfDatazMetricsCollector> tfdataz_metrics_;\n };\n@@ -184,10 +186,11 @@ class ScopedTfDataMetricsRegistration {\n };\n \n TEST(TfDatazMetricsRegistryTest, Register) {\n-  auto collector_one =\n-      std::make_shared<TfDatazMetricsCollector>(*Env::Default());\n-  auto collector_two =\n-      std::make_shared<TfDatazMetricsCollector>(*Env::Default());\n+  std::unique_ptr<IteratorBase> iterator;\n+  auto collector_one = std::make_shared<TfDatazMetricsCollector>(\n+      *Env::Default(), iterator.get());\n+  auto collector_two = std::make_shared<TfDatazMetricsCollector>(\n+      *Env::Default(), iterator.get());\n \n   ScopedTfDataMetricsRegistration scoped_registration_one(collector_one);\n   ScopedTfDataMetricsRegistration scoped_registration_two(collector_two);\n@@ -196,12 +199,13 @@ TEST(TfDatazMetricsRegistryTest, Register) {\n }\n \n TEST(TfDatazMetricsRegistryTest, Deregister) {\n-  auto collector_one =\n-      std::make_shared<TfDatazMetricsCollector>(*Env::Default());\n-  auto collector_two =\n-      std::make_shared<TfDatazMetricsCollector>(*Env::Default());\n-  auto collector_three =\n-      std::make_shared<TfDatazMetricsCollector>(*Env::Default());\n+  std::unique_ptr<IteratorBase> iterator;\n+  auto collector_one = std::make_shared<TfDatazMetricsCollector>(\n+      *Env::Default(), iterator.get());\n+  auto collector_two = std::make_shared<TfDatazMetricsCollector>(\n+      *Env::Default(), iterator.get());\n+  auto collector_three = std::make_shared<TfDatazMetricsCollector>(\n+      *Env::Default(), iterator.get());\n   ScopedTfDataMetricsRegistration scoped_registration_one(collector_one);\n   ScopedTfDataMetricsRegistration scoped_registration_two(collector_two);\n   ScopedTfDataMetricsRegistration scoped_registration_three(collector_three);\n",
            "@@ -33,7 +33,7 @@ Status WithErrorSourcePayload(Status error) {\n   error_source_proto.set_error_source(\n       core::platform::ErrorSourceProto::EAGER_REMOTE_MGR);\n   error.SetPayload(tensorflow::kErrorSource,\n-                   error_source_proto.SerializeAsString());\n+                   absl::Cord(error_source_proto.SerializeAsString()));\n   return error;\n }\n }  // namespace\n",
            "@@ -149,7 +149,8 @@ class TestReportErrorToClusterOp : public OpKernel {\n     }\n     tensorflow::Status s(static_cast<tensorflow::error::Code>(error_code),\n                          error_message);\n-    s.SetPayload(tsl::CoordinationErrorPayloadKey(), \"testing error payload\");\n+    s.SetPayload(tsl::CoordinationErrorPayloadKey(),\n+                 absl::Cord(\"testing error payload\"));\n     OP_REQUIRES_OK(ctx, coord_agent->ReportError(s));\n   }\n };\n",
            "@@ -959,6 +959,13 @@ class IteratorBase : public Checkpointable {\n     return OkStatus();\n   }\n \n+  // Returns the total number of bytes buffered by the iterator across all nodes\n+  // in the subtree for which autotuning is enabled.\n+  int64_t TotalBufferedBytes() const {\n+    if (node_) return node_->TotalBufferedBytes();\n+    return 0;\n+  }\n+\n  protected:\n   // Returns a node that models this iterator.\n   virtual std::shared_ptr<model::Node> CreateNode(\n",
            "@@ -62,7 +62,7 @@ namespace tensorflow {\n     if (!TF_PREDICT_TRUE(STATUS.ok())) {                                       \\\n       CheckNotInComputeAsync((CTX), \"OP_REQUIRES_OK_ASYNC\");                   \\\n       if (!PAYLOAD_VALUE.empty()) {                                            \\\n-        STATUS.SetPayload(PAYLOAD_KEY, PAYLOAD_VALUE);                         \\\n+        STATUS.SetPayload(PAYLOAD_KEY, absl::Cord(PAYLOAD_VALUE));             \\\n       }                                                                        \\\n       (CTX)->CtxFailureWithWarning(__FILE__, __LINE__, STATUS);                \\\n       return;                                                                  \\\n",
            "@@ -93,8 +93,6 @@ IteratorResource::IteratorResource(\n                                               /*iterator=*/nullptr)),\n       output_dtypes_(output_dtypes),\n       output_shapes_(output_shapes) {\n-  tf_dataz_metrics_collector_ = std::make_shared<TfDatazMetricsCollector>(*env);\n-  TfDatazMetricsRegistry::Register(tf_dataz_metrics_collector_);\n   VLOG(2) << \"creating iterator resource\";\n }\n \n@@ -274,6 +272,9 @@ Status IteratorResource::SetIteratorFromDataset(OpKernelContext* ctx,\n   new_state->MergeCheckpoint(iter_ctx.checkpoint());\n   mutex_lock l(mu_);\n   std::swap(iterator_state_, new_state);\n+  tf_dataz_metrics_collector_ =\n+      std::make_shared<TfDatazMetricsCollector>(env_, iterator.get());\n+  TfDatazMetricsRegistry::Register(tf_dataz_metrics_collector_);\n   return OkStatus();\n }\n \n",
            "@@ -238,7 +238,7 @@ class SymbolicGradientOp : public AsyncOpKernel {\n     OP_REQUIRES_OK_ASYNC(\n         ctx, lib->Instantiate(kGradientOp, AttrSlice(def()), &handle), done);\n \n-    FunctionLibraryRuntime::Options opts;\n+    FunctionLibraryRuntime::Options opts(ctx->step_id());\n     opts.rendezvous = ctx->rendezvous();\n     opts.cancellation_manager = ctx->cancellation_manager();\n     opts.collective_executor = ctx->collective_executor();\n",
            "@@ -160,7 +160,8 @@ class IfOp : public AsyncOpKernel {\n           then_handle_(then_handle),\n           else_handle_(else_handle),\n           done_(std::move(done)),\n-          lib_(CHECK_NOTNULL(ctx_->function_library())) {\n+          lib_(CHECK_NOTNULL(ctx_->function_library())),\n+          opts_(ctx->step_id()) {\n       SetRunOptions(ctx_, &opts_, true /* always_collect_stats */);\n       for (int i = 1; i < ctx_->num_inputs(); ++i) {\n         args_.push_back(ctx_->input(i));\n@@ -286,7 +287,8 @@ class CaseOp : public AsyncOpKernel {\n           branch_(branch),\n           branch_handles_(branch_handles),\n           done_(std::move(done)),\n-          lib_(CHECK_NOTNULL(ctx_->function_library())) {\n+          lib_(CHECK_NOTNULL(ctx_->function_library())),\n+          opts_(ctx->step_id()) {\n       SetRunOptions(ctx_, &opts_, true /* always_collect_stats */);\n       for (int i = 1; i < ctx_->num_inputs(); ++i) {\n         args_.push_back(ctx_->input(i));\n@@ -507,7 +509,8 @@ class WhileOp : public AsyncOpKernel {\n           cond_handle_(cond_handle),\n           body_handle_(body_handle),\n           done_(std::move(done)),\n-          lib_(CHECK_NOTNULL(ctx_->function_library())) {\n+          lib_(CHECK_NOTNULL(ctx_->function_library())),\n+          opts_(ctx->step_id()) {\n       SetRunOptions(ctx_, &opts_, false /* always_collect_stats */);\n       GetArgsFromContext(ctx, &args_, &loop_var_types_);\n       body_frame_ =\n@@ -751,6 +754,7 @@ class ForOp : public AsyncOpKernel {\n           ctx_(ctx),\n           done_(std::move(done)),\n           lib_(CHECK_NOTNULL(ctx_->function_library())),\n+          opts_(ctx->step_id()),\n           args_(1 + ctx_->num_inputs() - 3) {\n       args_[0] = Tensor(DT_INT32, {});\n       iter_ = &args_[0].scalar<int32>()();\n",
            "@@ -49,61 +49,68 @@ class GRUCellBlockOp : public OpKernel {\n     const Tensor* b_c_tensor = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->input(\"b_c\", &b_c_tensor));\n \n+    // Sanity checks for input shapes.\n+\n+    // Shape of 'x' must be [batch_size, input_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(x_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of x must be 2\", x_tensor->dims(),\n+                                        \" vs. 2\"));\n     const int64_t batch_size = x_tensor->dim_size(0);\n     const int64_t input_size = x_tensor->dim_size(1);\n-    const int64_t cell_size = h_prev_tensor->dim_size(1);\n-\n-    // Sanity checks for input shapes.\n \n     // Shape of 'h' must be [batch_size, cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(h_prev_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of h_prev must be 2, got \",\n+                                        h_prev_tensor->dims()));\n     OP_REQUIRES(ctx, h_prev_tensor->dim_size(0) == batch_size,\n                 errors::InvalidArgument(\"h_prev.dims(0) != batch_size: \",\n                                         h_prev_tensor->dim_size(0), \" vs. \",\n                                         batch_size));\n-    OP_REQUIRES(ctx, h_prev_tensor->dim_size(1) == cell_size,\n-                errors::InvalidArgument(\n-                    \"h_prev.dims(1) != cell_size: \", h_prev_tensor->dim_size(1),\n-                    \" vs. \", cell_size));\n+    const int64_t cell_size = h_prev_tensor->dim_size(1);\n \n     // Shape of 'w_ru' must be [input_size+cell_size, 2*cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(w_ru_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of w_ru_ must be 2, got \",\n+                                        w_ru_tensor->dims()));\n     OP_REQUIRES(ctx, w_ru_tensor->dim_size(0) == input_size + cell_size,\n                 errors::InvalidArgument(\n                     \"w_ru.dim_size(0) != input_size + cell_size: \",\n                     w_ru_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n-\n     OP_REQUIRES(ctx, w_ru_tensor->dim_size(1) == cell_size * 2,\n                 errors::InvalidArgument(\"w_ru.dim_size(1) != cell_size * 2: \",\n                                         w_ru_tensor->dim_size(1), \" vs. \",\n                                         cell_size * 2));\n \n     // Shape of 'w_c' must be [input_size+cell_size, cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(w_c_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of w_c must be 2, got \",\n+                                        w_c_tensor->dims()));\n     OP_REQUIRES(ctx, w_c_tensor->dim_size(0) == input_size + cell_size,\n                 errors::InvalidArgument(\n                     \"w_c.dim_size(0) != input_size + cell_size: \",\n                     w_c_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n-\n     OP_REQUIRES(ctx, w_c_tensor->dim_size(1) == cell_size,\n                 errors::InvalidArgument(\n                     \"w_c.dim_size(1) != cell_size: \", w_c_tensor->dim_size(1),\n                     \" vs. \", cell_size));\n \n     // Shape of 'b_ru' must be [2*cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(b_ru_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of b_ru must be 1, got \",\n+                                        b_ru_tensor->dims()));\n     OP_REQUIRES(ctx, b_ru_tensor->dim_size(0) == cell_size * 2,\n                 errors::InvalidArgument(\"b_ru.dim_size(0) != cell_size * 2: \",\n                                         b_ru_tensor->dim_size(0), \" vs. \",\n                                         cell_size * 2));\n \n-    OP_REQUIRES(ctx, b_ru_tensor->dims() == 1,\n-                errors::InvalidArgument(\"Rank of b_ru must be 1\",\n-                                        b_ru_tensor->dims(), \" vs. 1\", 1));\n     // Shape of 'b_c' must be [cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(b_c_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of b_c must be 1, got \",\n+                                        b_c_tensor->dims()));\n     OP_REQUIRES(ctx, b_c_tensor->dim_size(0) == cell_size,\n                 errors::InvalidArgument(\n                     \"b_c.dim_size(0) != cell_size: \", b_c_tensor->dim_size(0),\n                     \" vs. \", cell_size));\n-    OP_REQUIRES(ctx, b_c_tensor->dims() == 1,\n-                errors::InvalidArgument(\"Rank of b_c must be 1\",\n-                                        b_c_tensor->dims(), \" vs. 1\"));\n \n     // Create output tensors.\n     Tensor* r_tensor = nullptr;\n@@ -204,65 +211,71 @@ class GRUBlockCellGradOp : public OpKernel {\n     const Tensor* d_h_tensor = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->input(\"d_h\", &d_h_tensor));\n \n+    // Shape of 'x' must be [batch_size, input_size]\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsMatrix(x_tensor->shape()),\n+        errors::InvalidArgument(\"Rank of x must be 2, got \", x_tensor->dims()));\n     const int64_t batch_size = x_tensor->dim_size(0);\n     const int64_t input_size = x_tensor->dim_size(1);\n-    const int64_t cell_size = h_prev_tensor->dim_size(1);\n \n-    // Sanity checks for input shapes.\n-\n-    // Shape of 'h_prev' must be [batch_size, cell_size]\n+    // Shape of 'h' must be [batch_size, cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(h_prev_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of h_prev must be 2, got \",\n+                                        h_prev_tensor->dims()));\n     OP_REQUIRES(ctx, h_prev_tensor->dim_size(0) == batch_size,\n                 errors::InvalidArgument(\"h_prev.dims(0) != batch_size: \",\n                                         h_prev_tensor->dim_size(0), \" vs. \",\n                                         batch_size));\n-    OP_REQUIRES(ctx, h_prev_tensor->dim_size(1) == cell_size,\n-                errors::InvalidArgument(\n-                    \"h_prev.dims(1) != cell_size: \", h_prev_tensor->dim_size(1),\n-                    \" vs. \", cell_size));\n+    const int64_t cell_size = h_prev_tensor->dim_size(1);\n \n     // Shape of 'w_ru' must be [input_size+cell_size, 2*cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(w_ru_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of w_ru_ must be 2, got \",\n+                                        w_ru_tensor->dims()));\n     OP_REQUIRES(ctx, w_ru_tensor->dim_size(0) == input_size + cell_size,\n                 errors::InvalidArgument(\n                     \"w_ru.dim_size(0) != input_size + cell_size: \",\n                     w_ru_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n-\n     OP_REQUIRES(ctx, w_ru_tensor->dim_size(1) == cell_size * 2,\n                 errors::InvalidArgument(\"w_ru.dim_size(1) != cell_size * 2: \",\n                                         w_ru_tensor->dim_size(1), \" vs. \",\n                                         cell_size * 2));\n \n     // Shape of 'w_c' must be [input_size+cell_size, cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(w_c_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of w_c must be 2, got \",\n+                                        w_c_tensor->dims()));\n     OP_REQUIRES(ctx, w_c_tensor->dim_size(0) == input_size + cell_size,\n                 errors::InvalidArgument(\n                     \"w_c.dim_size(0) != input_size + cell_size: \",\n                     w_c_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n-\n     OP_REQUIRES(ctx, w_c_tensor->dim_size(1) == cell_size,\n                 errors::InvalidArgument(\n                     \"w_c.dim_size(1) != cell_size: \", w_c_tensor->dim_size(1),\n                     \" vs. \", cell_size));\n \n     // Shape of 'b_ru' must be [2*cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(b_ru_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of b_ru must be 1, got \",\n+                                        b_ru_tensor->dims()));\n     OP_REQUIRES(ctx, b_ru_tensor->dim_size(0) == cell_size * 2,\n                 errors::InvalidArgument(\"b_ru.dim_size(0) != cell_size * 2: \",\n                                         b_ru_tensor->dim_size(0), \" vs. \",\n                                         cell_size * 2));\n \n-    OP_REQUIRES(ctx, b_ru_tensor->dims() == 1,\n-                errors::InvalidArgument(\"Rank of b_ru must be 1\",\n-                                        b_ru_tensor->dims(), \" vs. 1\"));\n-\n     // Shape of 'b_c' must be [cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(b_c_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of b_c must be 1, got \",\n+                                        b_c_tensor->dims()));\n     OP_REQUIRES(ctx, b_c_tensor->dim_size(0) == cell_size,\n                 errors::InvalidArgument(\n                     \"b_c.dim_size(0) != cell_size: \", b_c_tensor->dim_size(0),\n                     \" vs. \", cell_size));\n \n-    OP_REQUIRES(ctx, b_c_tensor->dims() == 1,\n-                errors::InvalidArgument(\"Rank of b_c must be 1 \",\n-                                        b_c_tensor->dims(), \" vs. 1\"));\n-\n     // Shape of 'r' must be [batch_size, cell_size]\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsMatrix(r_tensor->shape()),\n+        errors::InvalidArgument(\"Rank of r must be 2, got \", r_tensor->dims()));\n     OP_REQUIRES(ctx, r_tensor->dim_size(0) == batch_size,\n                 errors::InvalidArgument(\n                     \"r.dims(0) != batch_size: \", r_tensor->dim_size(0), \" vs. \",\n@@ -273,6 +286,9 @@ class GRUBlockCellGradOp : public OpKernel {\n                     cell_size));\n \n     // Shape of 'u' must be [batch_size, cell_size]\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsMatrix(u_tensor->shape()),\n+        errors::InvalidArgument(\"Rank of u must be 2, got \", u_tensor->dims()));\n     OP_REQUIRES(ctx, u_tensor->dim_size(0) == batch_size,\n                 errors::InvalidArgument(\n                     \"u.dims(0) != batch_size: \", u_tensor->dim_size(0), \" vs. \",\n@@ -283,6 +299,9 @@ class GRUBlockCellGradOp : public OpKernel {\n                     cell_size));\n \n     // Shape of 'c' must be [batch_size, cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(c_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of w_c must be 2, got \",\n+                                        c_tensor->dims()));\n     OP_REQUIRES(ctx, c_tensor->dim_size(0) == batch_size,\n                 errors::InvalidArgument(\n                     \"c.dims(0) != batch_size: \", c_tensor->dim_size(0), \" vs. \",\n@@ -293,6 +312,9 @@ class GRUBlockCellGradOp : public OpKernel {\n                     cell_size));\n \n     // Shape of 'd_h' must be [batch_size, cell_size]\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(d_h_tensor->shape()),\n+                errors::InvalidArgument(\"Rank of d_h must be 2, got \",\n+                                        d_h_tensor->dims()));\n     OP_REQUIRES(ctx, d_h_tensor->dim_size(0) == batch_size,\n                 errors::InvalidArgument(\n                     \"d_h.dims(0) != batch_size: \", d_h_tensor->dim_size(0),\n",
            "@@ -177,7 +177,7 @@ TEST(StatusGroup, AggregateWithMultipleErrorStatus) {\n \n TEST(Status, InvalidPayloadGetsIgnored) {\n   Status s = Status();\n-  s.SetPayload(\"Invalid\", \"Invalid Val\");\n+  s.SetPayload(\"Invalid\", absl::Cord(\"Invalid Val\"));\n   ASSERT_FALSE(s.GetPayload(\"Invalid\").has_value());\n   bool is_err_erased = s.ErasePayload(\"Invalid\");\n   ASSERT_EQ(is_err_erased, false);\n@@ -185,15 +185,15 @@ TEST(Status, InvalidPayloadGetsIgnored) {\n \n TEST(Status, SetPayloadSetsOrUpdatesIt) {\n   Status s(error::INTERNAL, \"Error message\");\n-  s.SetPayload(\"Error key\", \"Original\");\n+  s.SetPayload(\"Error key\", absl::Cord(\"Original\"));\n   ASSERT_EQ(s.GetPayload(\"Error key\"), absl::Cord(\"Original\"));\n-  s.SetPayload(\"Error key\", \"Updated\");\n+  s.SetPayload(\"Error key\", absl::Cord(\"Updated\"));\n   ASSERT_EQ(s.GetPayload(\"Error key\"), absl::Cord(\"Updated\"));\n }\n \n TEST(Status, ErasePayloadRemovesIt) {\n   Status s(error::INTERNAL, \"Error message\");\n-  s.SetPayload(\"Error key\", \"Original\");\n+  s.SetPayload(\"Error key\", absl::Cord(\"Original\"));\n \n   bool is_err_erased = s.ErasePayload(\"Error key\");\n   ASSERT_EQ(is_err_erased, true);\n",
            "@@ -27,7 +27,7 @@ void OkOrSetErrorCounterPayload(\n     ErrorSourceProto error_source_proto;\n     error_source_proto.set_error_source(error_source);\n     status.SetPayload(tensorflow::kErrorSource,\n-                      error_source_proto.SerializeAsString());\n+                      absl::Cord(error_source_proto.SerializeAsString()));\n   }\n }\n \n",
            "@@ -408,7 +408,7 @@ Status TpuCompileOpKernelCommon::ComputeInternal(OpKernelContext* ctx) {\n     SerializeToTString(proto, &output.scalar<tstring>()());\n     ctx->set_output(0, output);\n     status.SetPayload(TpuCompileInterface::kTpuCompileErrorPayloadKey,\n-                      output.scalar<tstring>()());\n+                      absl::Cord(output.scalar<tstring>()()));\n   }\n \n   if (status.ok()) {\n",
            "@@ -1410,7 +1410,7 @@ Status TPUPartitionedCallOp::InitializeVarOnTPU(\n   TF_RETURN_IF_ERROR(\n       InstantiatePartition(*init_graph, fname, device, &fhandle, nullptr));\n \n-  FunctionLibraryRuntime::Options opts;\n+  FunctionLibraryRuntime::Options opts(ctx->step_id());\n   opts.step_container = ctx->step_container();\n   opts.cancellation_manager = ctx->cancellation_manager();\n   opts.stats_collector = ctx->stats_collector();\n@@ -1569,7 +1569,7 @@ Status TPUPartitionedCallOp::InitializeShardedVarOnTPU(\n     functions.push_back(DeviceAndFHandle{.device = target, .handle = handle});\n   }\n \n-  FunctionLibraryRuntime::Options opts;\n+  FunctionLibraryRuntime::Options opts(ctx->step_id());\n \n   // Blocking on threads in the same thread pool is disallowed because\n   // concurrent warm-up requests can exhaust the default thread pool.\n@@ -2702,7 +2702,7 @@ void TPUPartitionedCallOp::ExecuteFunctions(\n     const std::vector<DeviceAndFHandle>& functions, OpKernelContext* ctx,\n     int device_ordinal, int64_t ordinal_selector_req_id, DoneCallback done) {\n   profiler::TraceMe trace_me(\"TPUPartitionedCallOp-ExecuteFunctions\");\n-  FunctionLibraryRuntime::Options opts;\n+  FunctionLibraryRuntime::Options opts(ctx->step_id());\n   opts.step_container = ctx->step_container();\n   opts.stats_collector = ctx->stats_collector();\n   // TODO(akshayka): Consider selecting a runner on a per-device basis,\n",
            "@@ -29,7 +29,8 @@ Status AppendTpuEmbeddingErrorPayload(Status obj) {\n         absl::StrCat(kTpuEmbeddingErrorMessage, \". \", obj.error_message());\n     Status status(obj.code(), error_message);\n     TPUEmbeddingError error_payload;\n-    status.SetPayload(kTpuEmbeddingErrorUrl, error_payload.SerializeAsString());\n+    status.SetPayload(kTpuEmbeddingErrorUrl,\n+                      absl::Cord(error_payload.SerializeAsString()));\n     return status;\n   }\n }\n",
            "@@ -50,7 +50,8 @@ StatusOr<T> AppendTpuEmbeddingErrorPayload(StatusOr<T> obj) {\n         kTpuEmbeddingErrorMessage, \". \", obj.status().error_message());\n     Status status(obj.status().code(), error_message);\n     TPUEmbeddingError error_payload;\n-    status.SetPayload(kTpuEmbeddingErrorUrl, error_payload.SerializeAsString());\n+    status.SetPayload(kTpuEmbeddingErrorUrl,\n+                      absl::Cord(error_payload.SerializeAsString()));\n     return status;\n   }\n }\n",
            "@@ -23,7 +23,7 @@ limitations under the License.\n \n namespace tensorflow {\n \n-int64_t GetMempool() {\n+inline int64_t GetMempool() {\n   static absl::once_flag once;\n   static int64_t mempool = 1;\n   absl::call_once(once, [&] {\n@@ -34,7 +34,7 @@ int64_t GetMempool() {\n   return mempool;\n }\n \n-bool IsBlockedFormatEnabled() {\n+inline bool IsBlockedFormatEnabled() {\n   static absl::once_flag once;\n   static bool blocked_format = false;\n   absl::call_once(once, [&] {\n",
            "@@ -454,23 +454,34 @@ Below is the list of currently supported floating-point operators:\n * Output size, filter and bias (if present) must be static (use\n   `kTfLiteMmapRo` allocation type).\n \n-### Floating-Point (IEEE FP16) Operators (experimental)\n+### Floating-Point (IEEE FP16) Operators\n \n XNNPACK supports half-precision (using IEEE FP16 format) inference for a subset\n of floating-point operators. XNNPACK automatically enables half-precision\n inference when the following conditions are met:\n \n * XNNPACK runs on hardware that natively supports computations in IEEE FP16\n-format. Currently, this hardware is limited to ARM64 devices with ARMv8.2 FP16\n-arithmetics extension, and includes Android phones starting with Pixel 3,\n-Galaxy S9 (Snapdragon SoC), Galaxy S10 (Exynos SoC), iOS devices with A11 or\n-newer SoCs, and all Apple Silicon Macs.\n+format. Currently, this hardware is limited to ARM & ARM64 devices with\n+ARMv8.2 FP16 arithmetics extension, and includes Android phones starting with\n+Pixel 3, Galaxy S9 (Snapdragon SoC), Galaxy S10 (Exynos SoC), iOS devices with\n+A11 or newer SoCs, all Apple Silicon Macs, and Windows ARM64 laptops based with\n+Snapdragon 850 SoC or newer.\n \n * IEEE FP16 inference is supported for every floating-point operator in the\n model.\n \n * The model's \"reduced_precision_support\" metadata indicates that the model\n-is compatible with FP16 inference.\n+is compatible with FP16 inference. The metadata can be added during model\n+conversion using the `_experimental_supported_accumulation_type` attribute\n+of the [tf.lite.TargetSpec](https://www.tensorflow.org/api_docs/python/tf/lite/TargetSpec)\n+object:\n+\n+```python\n+converter.optimizations = [tf.lite.Optimize.DEFAULT]\n+...\n+converter.target_spec.supported_types = [tf.float16]\n+converter.target_spec._experimental_supported_accumulation_type = tf.dtypes.float16\n+```\n \n When the above conditions are met, XNNPACK replace FP32 operators with their\n FP16 equivalents, and insert additional operators to convert model inputs\n@@ -486,7 +497,7 @@ is used. Forcing FP16 inference has several effects:\n * Besides ARM64 devices with ARMv8.2 FP16 arithmetics extension, forced FP16\n inference is supported on x86/x86-64 devices with AVX2 extension in emulation\n mode: all elementary floating-point operations are computed in FP32, then\n-converted to FP16 and back to FP32. Note that such simulation is not exactly\n+converted to FP16 and back to FP32. Note that such simulation is not bit-exact\n equivalent to native FP16 inference, but simulates the effects of restricted\n mantissa precision and exponent range in the native FP16 arithmetics.\n \n@@ -512,171 +523,10 @@ TfLiteDelegate* xnnpack_delegate =\n     TfLiteXNNPackDelegateCreate(&xnnpack_options);\n ```\n \n-Below is the list of operators supported in IEEE FP16 inference:\n-\n-#### `ABS`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `ADD`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `AVERAGE_POOL_2D`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `CEIL`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `CONV_2D`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `CONCATENATION`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `DEPTH_TO_SPACE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `DEPTHWISE_CONV_2D`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `DIV`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `FLOOR`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `FULLY_CONNECTED`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `HARD_SWISH`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `LEAKY_RELU`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `LOGISTIC`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `MAX_POOL_2D`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `MAXIMUM`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `MEAN`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `MINIMUM`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `MUL`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `NEG`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `PAD`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `PRELU`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RELU`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RELU6`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RELU_N1_TO_1`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RESHAPE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `RESIZE_BILINEAR`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `ROUND`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SLICE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SOFTMAX`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SPACE_TO_DEPTH`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SPLIT`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SQRT`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SQUARE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SQUARED_DIFFERENCE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `STRIDED_SLICE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `SUB`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-* Neither of the inputs can be static (use `kTfLiteMmapRo` allocation type).\n-\n-#### `TRANSPOSE`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n-\n-#### `TRANSPOSE_CONV`\n-\n-* Must satisfy constraints on the floating-point (FP32) operator.\n+XNNPACK has full feature parity between FP32 and FP16 operators: all operators\n+that are supported for FP32 inference are also supported for FP16 inference,\n+and vice versa. In particular, sparse inference operators are supported for FP16\n+inference on ARM processors.\n \n ### Quantized Operators\n \n@@ -855,7 +705,8 @@ Below is the list of currently supported quantized operators:\n \n XNNPACK backend supports sparse inference for CNN models described in the\n [Fast Sparse ConvNets](https://arxiv.org/abs/1911.09723) paper. Sparse\n-inference is restricted to subgraphs with the following operators:\n+inference is restricted to subgraphs with the following floating-point\n+operators:\n \n * Sparse subgraph must store its weights in sparse representation (using\n   `DENSIFY` operators in the TensorFlow Lite schema).\n",
            "@@ -131,14 +131,16 @@ class SnapshotFtTest(data_service_test_base.TestBase, parameterized.TestCase):\n   def testSnapshotRecoveryFailsWithBadSplitNames(self, bad_split_filename):\n     cluster, _ = self.setup()\n     write_file(os.path.join(self.source_dir(), bad_split_filename))\n-    with self.assertRaisesRegex(ValueError, \"can't parse\"):\n+    with self.assertRaisesRegex(\n+        ValueError, \"Expected split_<local_split_index>_<global_split_index>\"):\n       cluster.restart_dispatcher()\n \n   @combinations.generate(test_base.eager_only_combinations())\n   def testSnapshotRecoveryFailsWithOutOfOrderSplitName(self):\n     cluster, _ = self.setup()\n     write_file(os.path.join(self.source_dir(), \"split_1_0\"))\n-    with self.assertRaisesRegex(ValueError, \"found conflict\"):\n+    with self.assertRaisesRegex(\n+        ValueError, \"The local split index 1 exceeds the global split index 0\"):\n       cluster.restart_dispatcher()\n \n   @combinations.generate(test_base.eager_only_combinations())\n",
            "@@ -1094,6 +1094,8 @@ py_library(\n     name = \"tensor_conversion_registry\",\n     srcs = [\"tensor_conversion_registry.py\"],\n     srcs_version = \"PY3\",\n+    # TODO(b/266747022): remove extra visibility\n+    visibility = visibility + [\"//learning/brain/experimental:__subpackages__\"],\n     deps = [\n         \"//tensorflow/python/eager:context\",\n     ],\n",
            "@@ -21,8 +21,8 @@ PYBIND11_MODULE(_errors_test_helper, m) {\n   m.def(\"TestRaiseFromStatus\", [](int code) {\n     tensorflow::Status status(static_cast<tensorflow::error::Code>(code),\n                               \"test message\");\n-    status.SetPayload(\"key1\", \"value1\");\n-    status.SetPayload(\"key2\", \"value2\");\n+    status.SetPayload(\"key1\", absl::Cord(\"value1\"));\n+    status.SetPayload(\"key2\", absl::Cord(\"value2\"));\n     MaybeRaiseRegisteredFromStatus(status);\n     return 0;\n   });\n",
            "@@ -22,6 +22,8 @@ from tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver import T\n from tensorflow.python.eager import context\n from tensorflow.python.eager import monitoring\n from tensorflow.python.eager.def_function import function\n+from tensorflow.python.eager.def_function import functions_run_eagerly\n+from tensorflow.python.eager.def_function import run_functions_eagerly\n from tensorflow.python.framework import device\n from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n@@ -111,6 +113,15 @@ def initialize_tpu_system(cluster_resolver=None):\n     # The TPU_SYSTEM device must match the device used in tpu.initialize_system\n     # exactly, otherwise you can get errors if there are multiple TPU_SYSTEM\n     # devices available.\n+    run_eagerly = functions_run_eagerly()\n+    if run_eagerly:\n+      logging.warning(\n+          \"It looks like tf.function behavior was disabled, perhaps using\"\n+          \" tf.config.run_functions_eagerly.\"\n+          \" tf.tpu.experimental.initialize_tpu_system requires tf.function to\"\n+          \" work. This primitive will override the disable.\"\n+      )\n+    run_functions_eagerly(False)\n     try:\n       with ops.device(tpu._tpu_system_device_name(job)):  # pylint: disable=protected-access\n         output = _tpu_init_fn()\n@@ -120,7 +131,9 @@ def initialize_tpu_system(cluster_resolver=None):\n           None, None,\n           \"TPUs not found in the cluster. Failed in initialization: \"\n           + str(e))\n-\n+    finally:\n+      if run_eagerly is not None:\n+        run_functions_eagerly(run_eagerly)\n     # Clear out the eager context caches since the memory is invalid now.\n     context.context()._initialize_logical_devices()  # pylint: disable=protected-access\n \n@@ -221,8 +234,21 @@ def shutdown_tpu_system(cluster_resolver=None):\n     # The TPU_SYSTEM device must match the device used in tpu.shutdown_system\n     # exactly, otherwise you can get errors if there are multiple TPU_SYSTEM\n     # devices available.\n-    with ops.device(tpu._tpu_system_device_name(job)):  # pylint: disable=protected-access\n-      _tpu_shutdown_fn()\n+    run_eagerly = functions_run_eagerly()\n+    if run_eagerly:\n+      logging.warning(\n+          \"It looks like tf.function behavior was disabled, perhaps using\"\n+          \" tf.config.run_functions_eagerly.\"\n+          \" tf.tpu.experimental.shutdown_tpu_system requires tf.function to\"\n+          \" work. This primitive will override the disable.\"\n+      )\n+    run_functions_eagerly(False)\n+    try:\n+      with ops.device(tpu._tpu_system_device_name(job)):  # pylint: disable=protected-access\n+        _tpu_shutdown_fn()\n+    finally:\n+      if run_eagerly is not None:\n+        run_functions_eagerly(run_eagerly)\n \n     # Clear out the eager context caches since the memory is invalid now.\n     logging.info(\"Clearing out eager caches\")\n",
            "@@ -8,14 +8,14 @@ google_pasta ~= 0.2\n h5py ~= 3.8.0  # Earliest version for Python 3.11\n # TODO(b/262592253): Support older versions of NumPy for Python 3.10 and lower\n # to support TFX. Remove when Apache Beam upgrades to newer NumPy.\n-numpy ~= 1.21.4; python_version < '3.11'\n+numpy ~= 1.22.0; python_version < '3.11'\n numpy ~= 1.23.2; python_version >= '3.11' # Earliest version for Python 3.11\n opt_einsum ~= 3.3.0\n protobuf ~= 3.19.3  # NOTE: Earliest version for Python 3.10\n six ~= 1.16.0\n termcolor ~= 2.1.1\n typing_extensions ~= 3.10.0.0\n-wheel ~= 0.36.2\n+wheel ~= 0.38.1\n wrapt ~= 1.14.1\n \n # We need to pin the gast dependency exactly\n@@ -37,4 +37,4 @@ scipy ~= 1.9.2; python_version >= '3.11' # Earliest version for Python 3.11\n \n # This is usually vendored in setuptools but ensure it gets installed in CI anyway\n # No bound here, we prefer the one in setuptools\n-packaging\n\\ No newline at end of file\n+packaging\n",
            "@@ -1,7 +1,7 @@\n -r requirements_common.txt\n \n # Dependencies only required for Mac\n-certifi ~= 2020.12.5\n+certifi ~= 2022.12.07\n \n # Install build related dependencies\n twine ~= 3.6.0\n",
            "@@ -11,7 +11,7 @@ google_pasta ~= 0.2\n h5py ~= 3.8.0 # Earliest version for Python 3.11\n # TODO(b/262592253): Support older versions of NumPy for Python 3.10 and lower\n # to support TFX. Remove when Apache Beam upgrades to newer NumPy.\n-numpy ~= 1.21.4; python_version < '3.11'\n+numpy ~= 1.22.0; python_version < '3.11'\n numpy ~= 1.23.2; python_version >= '3.11' # Earliest version for Python 3.11\n opt_einsum ~= 3.3.0\n packaging ~= 21.3\n@@ -19,7 +19,7 @@ protobuf ~= 3.20.1\n six ~= 1.16.0\n termcolor ~= 2.1.1\n typing_extensions ~= 3.10.0.0\n-wheel ~= 0.36.2\n+wheel ~= 0.38.1\n wrapt ~= 1.14.1\n # We need to pin the gast dependency exactly\n gast == 0.4.0\n@@ -48,4 +48,4 @@ twine ~= 3.6.0\n # For user tool scripts\n junitparser ~= 2.2.0\n lxml ~= 4.9.1\n-pylint ~= 2.13.9\n\\ No newline at end of file\n+pylint ~= 2.13.9\n",
            "@@ -36,7 +36,7 @@ void TSL_SetStatus(TSL_Status* s, TSL_Code code, const char* msg) {\n }\n \n void TSL_SetPayload(TSL_Status* s, const char* key, const char* value) {\n-  s->status.SetPayload(key, value);\n+  s->status.SetPayload(key, absl::Cord(absl::string_view(value)));\n }\n \n void TSL_SetStatusFromIOError(TSL_Status* s, int error_code,\n",
            "@@ -24,8 +24,8 @@ namespace {\n TEST(StatusHelper, TestStatusHelper) {\n   TSL_Status* s = TSL_NewStatus();\n   Status cc_status(errors::InvalidArgument(\"some error\"));\n-  cc_status.SetPayload(\"key1\", \"value1\");\n-  cc_status.SetPayload(\"key2\", \"value2\");\n+  cc_status.SetPayload(\"key1\", absl::Cord(\"value1\"));\n+  cc_status.SetPayload(\"key2\", absl::Cord(\"value2\"));\n   Set_TSL_Status_from_Status(s, cc_status);\n   ASSERT_EQ(TSL_INVALID_ARGUMENT, TSL_GetCode(s));\n   ASSERT_EQ(std::string(\"some error\"), TSL_Message(s));\n",
            "@@ -29,7 +29,7 @@ constexpr absl::string_view CoordinationErrorPayloadKey() {\n // Mark error as a coordination service error (as opposed to RPC\n // errors).\n inline Status MakeCoordinationError(Status s) {\n-  s.SetPayload(CoordinationErrorPayloadKey(), \"\");\n+  s.SetPayload(CoordinationErrorPayloadKey(), absl::Cord(\"\"));\n   return s;\n }\n \n@@ -43,14 +43,16 @@ inline Status MakeCoordinationError(Status s,\n   tensorflow::CoordinationServiceError error;\n   *error.mutable_source_task() = origin;\n   error.set_is_reported_error(is_reported_error);\n-  s.SetPayload(CoordinationErrorPayloadKey(), error.SerializeAsString());\n+  s.SetPayload(CoordinationErrorPayloadKey(),\n+               absl::Cord(error.SerializeAsString()));\n   return s;\n }\n \n // Mark error as a coordination service error with payload.\n inline Status MakeCoordinationError(\n     Status s, const tensorflow::CoordinationServiceError& payload) {\n-  s.SetPayload(CoordinationErrorPayloadKey(), payload.SerializeAsString());\n+  s.SetPayload(CoordinationErrorPayloadKey(),\n+               absl::Cord(payload.SerializeAsString()));\n   return s;\n }\n }  // namespace tsl\n",
            "@@ -71,12 +71,12 @@ inline void InsertSerializedPayloads(Status& s, std::string payloads) {\n   tensorflow::distributed_runtime::GrpcPayloadContainer container;\n   if (container.ParseFromString(payloads)) {\n     for (const auto& key_val : container.payloads()) {\n-      s.SetPayload(key_val.first, key_val.second);\n+      s.SetPayload(key_val.first, absl::Cord(key_val.second));\n     }\n   } else {\n     s.SetPayload(kGrpcPayloadsLost,\n-                 tensorflow::distributed_runtime::GrpcPayloadsLost()\n-                     .SerializeAsString());\n+                 absl::Cord(tensorflow::distributed_runtime::GrpcPayloadsLost()\n+                                .SerializeAsString()));\n   }\n }\n \n",
            "@@ -71,7 +71,7 @@ TestRequest MakeProto(int size) {\n \n TEST(PayloadSerialization, PayloadsAreTransmitted) {\n   Status status = errors::InvalidArgument(\"invalid arg message\");\n-  status.SetPayload(\"a\", \"\\\\xFF\\\\x02\\\\x03\");\n+  status.SetPayload(\"a\", absl::Cord(\"\\\\xFF\\\\x02\\\\x03\"));\n   Status status_recovered = FromGrpcStatus(ToGrpcStatus(status));\n \n   ASSERT_TRUE(status_recovered.GetPayload(\"a\").has_value());\n",
            "@@ -21,6 +21,7 @@ limitations under the License.\n #include <utility>\n \n #include \"absl/base/attributes.h\"\n+#include \"absl/strings/cord.h\"\n #include \"absl/strings/str_join.h\"\n #include \"tensorflow/tsl/platform/logging.h\"\n #include \"tensorflow/tsl/platform/macros.h\"\n@@ -102,7 +103,7 @@ inline void InsertPayloads(\n     ::tsl::Status& status,\n     const std::unordered_map<std::string, std::string>& payloads) {\n   for (const auto& payload : payloads) {\n-    status.SetPayload(payload.first, payload.second);\n+    status.SetPayload(payload.first, absl::Cord(payload.second));\n   }\n }\n \n@@ -110,7 +111,7 @@ inline void InsertPayloads(\n // payloads in the destination if they exist with the same key.\n inline void CopyPayloads(const ::tsl::Status& from, ::tsl::Status& to) {\n   from.ForEachPayload([&to](tsl::StringPiece key, tsl::StringPiece value) {\n-    to.SetPayload(key, value);\n+    to.SetPayload(key, absl::Cord(value));\n   });\n }\n \n",
            "@@ -85,6 +85,13 @@ int RandomSeed();\n // NOTE: This function is not thread-safe.\n int PickUnusedPortOrDie();\n \n+// Constant which is false internally and true in open source.\n+#ifdef PLATFORM_GOOGLE\n+inline constexpr bool kIsOpenSource = false;\n+#else\n+inline constexpr bool kIsOpenSource = true;\n+#endif  // PLATFORM_GOOGLE\n+\n }  // namespace testing\n }  // namespace tsl\n \n",
            "@@ -213,6 +213,7 @@ cc_library(\n     deps = [\n         \"//tensorflow/tsl/platform:logging\",\n         \"//tensorflow/tsl/platform:macros\",\n+        \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/strings\",\n     ],\n )\n@@ -226,6 +227,7 @@ tsl_cc_test(\n         \"//tensorflow/tsl/platform:test\",\n         \"//tensorflow/tsl/platform:test_main\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n     ],\n )\n \n",
            "@@ -20,6 +20,7 @@ limitations under the License.\n #include <initializer_list>\n #include <string>\n \n+#include \"absl/base/attributes.h\"\n #include \"absl/strings/match.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/string_view.h\"\n@@ -31,14 +32,21 @@ namespace profiler {\n \n // An argument passed to TraceMeEncode.\n struct TraceMeArg {\n-  // This constructor is required because absl::AlphaNum is non-copyable.\n-  template <typename Value>\n-  TraceMeArg(absl::string_view k, Value v) : key(k), value(v) {}\n+  // String conversions of value types are supported via AlphaNum. We keep a\n+  // reference to the AlphaNum's internal buffer here, so it must remain valid\n+  // for the lifetime of this object. We cannot store it by value because it is\n+  // not safe to construct an AlphaNum as a member of a class, particularly when\n+  // AbslStringify is being used (it may reference default arguments that are on\n+  // the caller's stack, if we constructed it here those default arguments would\n+  // be destroyed before they are used).\n+  TraceMeArg(absl::string_view k,\n+             const absl::AlphaNum& v ABSL_ATTRIBUTE_LIFETIME_BOUND)\n+      : key(k), value(v.Piece()) {}\n \n   TF_DISALLOW_COPY_AND_ASSIGN(TraceMeArg);\n \n   absl::string_view key;\n-  absl::AlphaNum value;\n+  absl::string_view value;\n };\n \n namespace traceme_internal {\n@@ -74,7 +82,7 @@ TF_ATTRIBUTE_ALWAYS_INLINE inline std::string AppendArgs(\n     for (const auto& arg : args) {\n       out = Append(out, arg.key);\n       *out++ = '=';\n-      out = Append(out, arg.value.Piece());\n+      out = Append(out, arg.value);\n       *out++ = ',';\n     }\n     *(out - 1) = '#';\n",
            "@@ -17,6 +17,7 @@ limitations under the License.\n #include <string>\n \n #include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n #include \"tensorflow/tsl/platform/platform.h\"\n #include \"tensorflow/tsl/platform/test.h\"\n \n@@ -53,6 +54,27 @@ TEST(TraceMeEncodeTest, TemporaryStringTest) {\n }\n #endif\n \n+// This can be removed when the absl version has been updated to include\n+// AbslStringify for open source builds.\n+#if defined(PLATFORM_GOOGLE)\n+\n+struct Point {\n+  template <typename Sink>\n+  friend void AbslStringify(Sink& sink, const Point& p) {\n+    absl::Format(&sink, \"(%d, %d)\", p.x, p.y);\n+  }\n+\n+  int x;\n+  int y;\n+};\n+\n+TEST(TraceMeEncodeTest, AbslStringifyTest) {\n+  EXPECT_EQ(TraceMeEncode(\"Plot\", {{\"point\", Point{10, 20}}}),\n+            \"Plot#point=(10, 20)#\");\n+}\n+\n+#endif\n+\n TEST(TraceMeEncodeTest, NoNameTest) {\n   EXPECT_EQ(TraceMeEncode({{\"context\", \"World\"}, {\"request_id\", 42}}),\n             \"#context=World,request_id=42#\");\n",
            "@@ -6,8 +6,8 @@ def repo():\n     \"\"\"Imports TFRT.\"\"\"\n \n     # Attention: tools parse and update these lines.\n-    TFRT_COMMIT = \"c1248a015d23949afa2471bb21f6f52850aead7d\"\n-    TFRT_SHA256 = \"8cdd8ea905478ac4ffd36ffb39cebe288d3b840d71a02d418bc6a8a760f92af8\"\n+    TFRT_COMMIT = \"c653281a1a23c0c3d41536a983c7d10fcc5b1fbf\"\n+    TFRT_SHA256 = \"3d1edd27c4e36d9cfc9493aef7088489babb370d2a7955bab3545acfbb024ccf\"\n \n     tf_http_archive(\n         name = \"tf_runtime\",\n"
        ],
        "Title": "\n          OOB Read in GRUBlockCellGrad\n        "
    }
]
[
    {
        "Bug description": "A malicious invalid input crashes a tensorflow model (Check Failed) and can be used to trigger a denial of service attack. \nTo minimize the bug, we built a simple single-layer TensorFlow model containing a Convolution3DTranspose layer, which works well with expected inputs and can be deployed in real-world systems. However, if we call the model with a malicious input which has a zero dimension, it gives Check Failed failure and crashes.",
        "Sample Code": "class MyModel(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.conv = tf.keras.layers.Convolution3DTranspose(2, [3,3,3], padding=\"same\")\n        \n    def call(self, input):\n        return self.conv(input)\nmodel = MyModel() # Defines a valid model.\n\nx = tf.random.uniform([1, 32, 32, 32, 3], minval=0, maxval=0, dtype=tf.float32) # This is a valid input.\noutput = model.predict(x)\nprint(output.shape) # (1, 32, 32, 32, 2)\n\nx = tf.random.uniform([1, 32, 32, 0, 3], dtype=tf.float32) # This is an invalid input.\n\noutput = model(x) # crash",
        "Bug fix": "",
        "Title": "\n          Denial of Service in TensorFlow\n        "
    },
    {
        "Bug description": "Another instance of  CVE-2022-35991 , where  TensorListScatter  and  TensorListScatterV2  crash via non scalar inputs in element_shape , was found in eager mode and fixed.",
        "Sample Code": "arg_0=tf.random.uniform(shape=(2, 2, 2), dtype=tf.float16, maxval=None)\narg_1=tf.random.uniform(shape=(2, 2, 2), dtype=tf.int32, maxval=65536)\narg_2=tf.random.uniform(shape=(2, 2, 2), dtype=tf.int32, maxval=65536)\narg_3=''\ntf.raw_ops.TensorListScatter(tensor=arg_0, indices=arg_1, \n, \nelement_shape=arg_2, name=arg_3)",
        "Bug fix": [
            "@@ -909,7 +909,12 @@ class TensorListScatter : public OpKernel {\n     OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(2), &element_shape));\n     // TensorListScatterV2 passes the num_elements input, TensorListScatter does\n     // not.\n-    int num_elements = c->num_inputs() >= 4 ? c->input(3).scalar<int>()() : -1;\n+    int num_elements = -1;\n+    if (c->num_inputs() >= 4) {\n+      OP_REQUIRES(c, TensorShapeUtils::IsScalar(c->input(3).shape()),\n+                  errors::InvalidArgument(\"num_elements must be a scalar\"));\n+      num_elements = c->input(3).scalar<int>()();\n+    }\n     OP_REQUIRES(c, num_elements >= -1,\n                 errors::InvalidArgument(\n                     \"TensorListScatter expects num_elements >= -1, found: \",\n",
            "@@ -549,6 +549,17 @@ class ListOpsTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n       l = list_ops.tensor_list_scatter(c0, [-1, -2], element_shape=[])\n       self.evaluate(l)\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def testScatterWithNonScalarFails(self):\n+    c = constant_op.constant(value=[2])\n+    num_elements = np.array([[], [], []], dtype=np.float32)\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                r\"Shape must be rank 0 but is rank \\d+|\"\n+                                r\"\\w+ must be a scalar\"):\n+      self.evaluate(\n+          gen_list_ops.TensorListScatterV2(\n+              tensor=c, indices=c, element_shape=c, num_elements=num_elements))\n+\n   def testScatterIntoExistingList(self):\n     l = list_ops.tensor_list_reserve(\n         element_dtype=dtypes.float32, element_shape=[], num_elements=3)\n"
        ],
        "Title": "\n          `CHECK` fail in `TensorListScatter` and `TensorListScatterV2` in eager mode\n        "
    },
    {
        "Bug description": "Another instance of  CVE-2022-35935 , where  SobolSample  is vulnerable to a denial of service via assumed scalar inputs, was found and fixed.",
        "Sample Code": " tensorflow as tf\ntf.raw_ops.SobolSample(dim=tf.constant([1,0]), num_results=tf.constant([1]), skip=tf.constant([1]))",
        "Bug fix": "",
        "Title": "\n          `CHECK` failure in `SobolSample` via missing validation\n        "
    },
    {
        "Bug description": "The function  MakeGrapplerFunctionItem  takes arguments that determine the sizes of inputs and outputs. If the inputs given are greater than or equal to the sizes of the outputs, an out-of-bounds memory read or a crash is triggered.",
        "Sample Code": "@\ndef test():\n    tf.raw_ops.QuantizeAndDequantizeV2(input=[2.5],\n    \t\t\t\t\t\t\t\t   input_min=[1.0],\n    \t\t\t\t\t\t\t\t   input_max=[10.0],\n    \t\t\t\t\t\t\t\t   signed_input=True,\n    \t\t\t\t\t\t\t\t   num_bits=1,\n    \t\t\t\t\t\t\t\t   range_given=True,\n    \t\t\t\t\t\t\t\t   round_mode='HALF_TO_EVEN',\n    \t\t\t\t\t\t\t\t   narrow_range=True,\n    \t\t\t\t\t\t\t\t   axis=0x7fffffff)\n)\ntest()",
        "Bug fix": [
            "@@ -2879,6 +2879,10 @@ REGISTER_OP(\"QuantizeAndDequantizeV2\")\n                                        axis);\n       } else if (axis != -1) {\n         ShapeHandle input;\n+        if (axis >= kint32max) {\n+          return errors::InvalidArgument(\n+              \"Axis cannot be >= kint32max value, got \", axis);\n+        }\n         TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n         DimensionHandle depth;\n         TF_RETURN_IF_ERROR(\n@@ -2914,6 +2918,10 @@ REGISTER_OP(\"QuantizeAndDequantizeV4\")\n                                        axis);\n       } else if (axis != -1) {\n         ShapeHandle input;\n+        if (axis >= kint32max) {\n+          return errors::InvalidArgument(\n+              \"Axis cannot be >= kint32max value, got \", axis);\n+        }\n         TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n         DimensionHandle depth;\n         TF_RETURN_IF_ERROR(\n@@ -2945,6 +2953,10 @@ REGISTER_OP(\"QuantizeAndDequantizeV4Grad\")\n                                        axis);\n       } else if (axis != -1) {\n         ShapeHandle input;\n+        if (axis >= kint32max) {\n+          return errors::InvalidArgument(\n+              \"Axis cannot be >= kint32max value, got \", axis);\n+        }\n         TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n         DimensionHandle depth;\n         TF_RETURN_IF_ERROR(\n@@ -2981,6 +2993,10 @@ REGISTER_OP(\"QuantizeAndDequantizeV3\")\n                                        axis);\n       } else if (axis != -1) {\n         ShapeHandle input;\n+        if (axis >= kint32max) {\n+          return errors::InvalidArgument(\n+              \"Axis cannot be >= kint32max value, got \", axis);\n+        }\n         TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n         DimensionHandle depth;\n         TF_RETURN_IF_ERROR(\n",
            "@@ -1856,6 +1856,72 @@ class QuantizeAndDequantizeTest(test_util.TensorFlowTestCase):\n               max_range=input_max,\n               axis=2**31 - 1))\n \n+  @test_util.run_v2_only\n+  def testInvalidAxis(self):\n+\n+    @def_function.function\n+    def test_quantize_and_dequantize_v2():\n+      gen_array_ops.quantize_and_dequantize_v2(\n+          input=[2.5],\n+          input_min=[1.0],\n+          input_max=[10.0],\n+          signed_input=True,\n+          num_bits=1,\n+          range_given=True,\n+          round_mode=\"HALF_TO_EVEN\",\n+          narrow_range=True,\n+          axis=0x7fffffff)\n+\n+    @def_function.function\n+    def test_quantize_and_dequantize_v3():\n+      gen_array_ops.quantize_and_dequantize_v3(\n+          input=[2.5],\n+          input_min=[1.0],\n+          input_max=[10.0],\n+          num_bits=1,\n+          signed_input=True,\n+          range_given=True,\n+          narrow_range=True,\n+          axis=0x7fffffff)\n+\n+    @def_function.function\n+    def test_quantize_and_dequantize_v4():\n+      gen_array_ops.quantize_and_dequantize_v4(\n+          input=[2.5],\n+          input_min=[1.0],\n+          input_max=[10.0],\n+          signed_input=True,\n+          num_bits=1,\n+          range_given=True,\n+          round_mode=\"HALF_TO_EVEN\",\n+          narrow_range=True,\n+          axis=0x7fffffff)\n+\n+    @def_function.function\n+    def test_quantize_and_dequantize_v4_grad():\n+      gen_array_ops.quantize_and_dequantize_v4_grad(\n+          gradients=[2.5],\n+          input=[2.5],\n+          input_min=[1.0],\n+          input_max=[10.0],\n+          axis=0x7fffffff)\n+\n+    with self.assertRaisesRegex(\n+        ValueError, \"Axis cannot be >= kint32max value, got 2147483647\"):\n+      test_quantize_and_dequantize_v2()\n+\n+    with self.assertRaisesRegex(\n+        ValueError, \"Axis cannot be >= kint32max value, got 2147483647\"):\n+      test_quantize_and_dequantize_v3()\n+\n+    with self.assertRaisesRegex(\n+        ValueError, \"Axis cannot be >= kint32max value, got 2147483647\"):\n+      test_quantize_and_dequantize_v4()\n+\n+    with self.assertRaisesRegex(\n+        ValueError, \"Axis cannot be >= kint32max value, got 2147483647\"):\n+      test_quantize_and_dequantize_v4_grad()\n+\n \n @test_util.run_all_in_graph_and_eager_modes\n class SortedSearchTest(test_util.TensorFlowTestCase):\n"
        ],
        "Title": "\n          Heap overflow in `QuantizeAndDequantizeV2`\n        "
    },
    {
        "Bug description": "When  printing a tensor , we get it's data as a  const char*  array (since that's the underlying storage) and then we typecast it to the element type. However, conversions from  char  to  bool  are undefined if the  char  is not  0  or  1 , so sanitizers/fuzzers will crash.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -29,6 +29,7 @@ limitations under the License.\n \n #include \"tensorflow/core/framework/tensor.h\"\n \n+#include <memory>\n #include <utility>\n \n #include \"absl/strings/escaping.h\"\n@@ -1183,12 +1184,10 @@ void PrintOneDimV2(int dim_index, const gtl::InlinedVector<int64, 4>& shape,\n }\n \n template <typename T>\n-string SummarizeArray(int64_t limit, int64_t num_elts,\n-                      const TensorShape& tensor_shape, const char* data,\n-                      const bool print_v2) {\n+string SummarizeArrayInternal(int64_t limit, int64_t num_elts,\n+                              const TensorShape& tensor_shape, const T* array,\n+                              const bool print_v2) {\n   string ret;\n-  const T* array = reinterpret_cast<const T*>(data);\n-\n   const gtl::InlinedVector<int64_t, 4> shape = tensor_shape.dim_sizes();\n   if (shape.empty()) {\n     for (int64_t i = 0; i < limit; ++i) {\n@@ -1211,6 +1210,29 @@ string SummarizeArray(int64_t limit, int64_t num_elts,\n \n   return ret;\n }\n+\n+template <typename T>\n+string SummarizeArray(int64_t limit, int64_t num_elts,\n+                      const TensorShape& tensor_shape, const char* data,\n+                      const bool print_v2) {\n+  const T* array = reinterpret_cast<const T*>(data);\n+  return SummarizeArrayInternal<T>(limit, num_elts, tensor_shape, array,\n+                                   print_v2);\n+}\n+\n+template <>\n+string SummarizeArray<bool>(int64_t limit, int64_t num_elts,\n+                            const TensorShape& tensor_shape, const char* data,\n+                            const bool print_v2) {\n+  // We first convert all chars to be 0/1 to not get InvalidEnumValue sanitizer\n+  // error\n+  auto mutable_data = std::unique_ptr<char[]>(new char[num_elts]);\n+  for (int64_t i = 0; i < num_elts; ++i)\n+    mutable_data.get()[i] = data[i] ? 1 : 0;\n+  bool* array = reinterpret_cast<bool*>(mutable_data.get());\n+  return SummarizeArrayInternal<bool>(limit, num_elts, tensor_shape, array,\n+                                      print_v2);\n+}\n }  // namespace\n \n string Tensor::SummarizeValue(int64_t max_entries, bool print_v2) const {\n"
        ],
        "Title": "\n          Invalid char to bool conversion when printing a tensor\n        "
    },
    {
        "Bug description": "An input  encoded  that is not a valid  CompositeTensorVariant  tensor will trigger a segfault in  tf.raw_ops.CompositeTensorVariantToComponents .",
        "Sample Code": "encode = tf.raw_ops.EmptyTensorList(element_dtype=tf.int32, element_shape=[10, 15], max_num_elements=2)\nmeta= \"\"\ncomponent=[tf.int32]\n\n]\n\nprint(tf.raw_ops.CompositeTensorVariantToComponents(encoded=encode,metadata=meta,Tcomponents=component))",
        "Bug fix": [
            "@@ -73,6 +73,10 @@ class CompositeTensorVariantToComponents : public OpKernel {\n                                 \"tensor, but got \",\n                                 encoded_t.DebugString()));\n     auto* encoded = encoded_t.flat<Variant>()(0).get<CompositeTensorVariant>();\n+    OP_REQUIRES(context, encoded != nullptr,\n+                errors::InvalidArgument(\"The input `encoded` is not a valid \"\n+                                        \"CompositeTensorVariant tensor, got \",\n+                                        encoded_t.DebugString()));\n \n     // Check that the encoded TypeSpec is compatible with the expected TypeSpec.\n     // For now, we just check that the class matches.\n",
            "@@ -25,6 +25,7 @@ from tensorflow.python.framework import sparse_tensor\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import composite_tensor_ops\n from tensorflow.python.ops import gen_composite_tensor_ops\n+from tensorflow.python.ops import gen_list_ops\n from tensorflow.python.ops import gradients_impl\n from tensorflow.python.ops import math_ops\n from tensorflow.python.ops import parsing_ops\n@@ -97,6 +98,18 @@ class ExtensionTypeTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n           metadata='',\n           Tcomponents=[dtypes.int32])\n \n+  def testDecodingInvalidEncodedInputError(self):\n+    with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                'not a valid CompositeTensorVariant tensor'):\n+      self.evaluate(\n+          gen_composite_tensor_ops.CompositeTensorVariantToComponents(\n+              encoded=gen_list_ops.EmptyTensorList(\n+                  element_dtype=dtypes.int32,\n+                  element_shape=[1, 2],\n+                  max_num_elements=2),\n+              metadata='',\n+              Tcomponents=[dtypes.int32]))\n+\n   def testRoundTripThroughTensorProto(self):\n     value = ragged_factory_ops.constant([[1, 2], [3], [4, 5, 6]])\n     encoded = composite_tensor_ops.composite_tensor_to_variants(value)\n"
        ],
        "Title": "\n          Segfault in `CompositeTensorVariantToComponents`\n        "
    },
    {
        "Bug description": "An input  token  that is not a UTF-8 bytestring will trigger a  CHECK  fail in  tf.raw_ops.PyFunc .",
        "Sample Code": "value = tf.constant(value=[1,2])\ntoken = b'\ndataType = [tf.int32]\n\n]\n\ntf.raw_ops.PyFunc(input=value,token=token,Tout=dataType)",
        "Bug fix": [
            "@@ -83,8 +83,8 @@ bool IsCPUDevice(const Device* d) {\n   return d == nullptr || d->tensorflow_accelerator_device_info() == nullptr;\n }\n \n-// Givens the 'call', prepares the token and inputs as a python tuple\n-// that is appropriate for calling the trampoline.\n+// Given the 'call', prepares the token and inputs as a python tuple that is\n+// appropriate for calling the trampoline.\n Status MakeArgTuple(const PyCall* call, TFE_Context* ctx, PyObject** tuple) {\n   int64_t n = call->ins.size();\n   PyObject* lst = PyList_New(n);\n@@ -119,7 +119,11 @@ Status MakeArgTuple(const PyCall* call, TFE_Context* ctx, PyObject** tuple) {\n     PyList_SetItem(lst, i, arg);\n   }\n   *tuple = Py_BuildValue(\"(ssN)\", call->token.c_str(), device_name, lst);\n-  CHECK(*tuple);\n+  if (*tuple == nullptr) {\n+    return errors::Internal(\n+        \"Failed to create python tuple. Please make sure `token` is a \"\n+        \"well-formed UTF-8 string.\");\n+  }\n   return OkStatus();\n }\n \n",
            "@@ -17,7 +17,9 @@\n from tensorflow.python.eager import def_function\n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import test_util\n+from tensorflow.python.ops import gen_script_ops\n from tensorflow.python.ops import resource_variable_ops\n from tensorflow.python.ops import script_ops\n from tensorflow.python.ops.script_ops import numpy_function\n@@ -103,6 +105,15 @@ class PyFunctionTest(test.TestCase):\n     expect_result = constant_op.constant(3, dtypes.int32)\n     self.assertAllEqual(actual_result, expect_result)\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def test_fail_on_non_utf8_token(self):\n+    value = constant_op.constant(value=[1, 2])\n+    token = b\"\\xb0\"\n+    data_type = [dtypes.int32]\n+    with self.assertRaises((errors.InternalError, UnicodeDecodeError)):\n+      self.evaluate(\n+          gen_script_ops.py_func(input=[value], token=token, Tout=data_type))\n+\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          `CHECK` fail via inputs in `PyFunc`\n        "
    },
    {
        "Bug description": "When  tf.raw_ops.ResizeNearestNeighborGrad  is given a large  size  input, it overflows.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -257,11 +257,11 @@ class ResizeNearestNeighborOpGrad : public OpKernel {\n     const int64_t out_width = sizes(1);\n \n     Tensor* output = nullptr;\n-    OP_REQUIRES_OK(\n-        context,\n-        context->allocate_output(\n-            0, TensorShape({batch_size, out_height, out_width, channels}),\n-            &output));\n+    TensorShape shape;\n+    OP_REQUIRES_OK(context,\n+                   TensorShape::BuildTensorShape(\n+                       {batch_size, out_height, out_width, channels}, &shape));\n+    OP_REQUIRES_OK(context, context->allocate_output(0, shape, &output));\n \n     // Return if the output is empty.\n     if (output->NumElements() == 0) return;\n",
            "@@ -4175,6 +4175,25 @@ class ResizeImageWithPadV2Test(test_util.TensorFlowTestCase):\n     self._assertReturns(x, x_shape, y, y_shape)\n \n \n+class ResizeNearestNeighborGrad(test_util.TensorFlowTestCase):\n+\n+  def testSizeTooLarge(self):\n+    align_corners = True\n+    half_pixel_centers = False\n+    grads = constant_op.constant(1, shape=[1, 8, 16, 3], dtype=dtypes.float16)\n+    size = constant_op.constant([1879048192, 1879048192],\n+                                shape=[2],\n+                                dtype=dtypes.int32)\n+    with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                r\"Encountered overflow when multiplying\"):\n+      self.evaluate(\n+          gen_image_ops.ResizeNearestNeighborGrad(\n+              grads=grads,\n+              size=size,\n+              align_corners=align_corners,\n+              half_pixel_centers=half_pixel_centers))\n+\n+\n class ResizeImageWithCropOrPadTest(test_util.TensorFlowTestCase):\n \n   def _ResizeImageWithCropOrPad(self, x, target_height, target_width,\n"
        ],
        "Title": "\n          Overflow in `ResizeNearestNeighborGrad`\n        "
    },
    {
        "Bug description": "The function  MakeGrapplerFunctionItem  takes arguments that determine the sizes of inputs and outputs. If the inputs given are greater than or equal to the sizes of the outputs, an out-of-bounds memory read or a crash is triggered.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -291,6 +291,11 @@ Status MakeGrapplerFunctionItem(const FunctionDef& func,\n \n   std::vector<const FunctionDef::ArgAttrs*> arg_attr(inputs.size(), nullptr);\n   for (const auto& attr : func.arg_attr()) {\n+    if (attr.first >= inputs.size()) {\n+      return errors::InvalidArgument(\"Invalid attribute index, got \",\n+                                     attr.first, \" but expected less than \",\n+                                     inputs.size());\n+    }\n     arg_attr.at(attr.first) = &attr.second;\n   }\n \n"
        ],
        "Title": "\n          OOB write in grappler\n        "
    },
    {
        "Bug description": "An input  sparse_matrix  that is not a matrix with a shape with rank 0 will trigger a  CHECK  fail in  tf.raw_ops.SparseMatrixNNZ .",
        "Sample Code": " tensorflow as tf\ntf.raw_ops.SparseMatrixNNZ(sparse_matrix=[])",
        "Bug fix": [
            "@@ -25,10 +25,12 @@ limitations under the License.\n #include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/tensor_types.h\"\n #include \"tensorflow/core/framework/variant.h\"\n #include \"tensorflow/core/framework/variant_encode_decode.h\"\n #include \"tensorflow/core/framework/variant_op_registry.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n \n namespace tensorflow {\n \n@@ -633,6 +635,11 @@ template <typename T>\n Status ExtractVariantFromInput(OpKernelContext* ctx, int index,\n                                const T** value) {\n   const Tensor& input_t = ctx->input(index);\n+  if (!TensorShapeUtils::IsScalar(input_t.shape())) {\n+    return errors::InvalidArgument(\n+        \"Invalid input matrix: Shape must be rank 0 but is rank \",\n+        input_t.dims());\n+  }\n   const Variant& input_variant = input_t.scalar<Variant>()();\n   *value = input_variant.get<T>();\n   if (*value == nullptr) {\n",
            "@@ -1313,6 +1313,16 @@ class CSRSparseMatrixOpsTest(test.TestCase):\n       self.assertLess(cholesky_with_amd_nnz_value,\n                       cholesky_without_ordering_nnz_value)\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def testNoMatrixNoCrash(self):\n+    # Round-about way of creating an empty variant tensor that works in both\n+    # graph and eager modes.\n+    no_matrix = array_ops.reshape(dense_to_csr_sparse_matrix([[0.0]]), [1])[0:0]\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        \"(Invalid input matrix)|(Shape must be rank 0)\"):\n+      sparse_csr_matrix_ops.sparse_matrix_nnz(no_matrix)\n+\n \n class CSRSparseMatrixOpsBenchmark(test.Benchmark):\n \n"
        ],
        "Title": "\n          `CHECK_EQ` fail via input in `SparseMatrixNNZ`\n        "
    }
]
[
    {
        "Bug description": "An input  pooling_ratio  that is smaller than 1 will trigger a heap OOB in  tf.raw_ops.FractionalMaxPool  and  tf.raw_ops.FractionalAvgPool .",
        "Sample Code": "",
        "Bug fix": [
            "@@ -44,6 +44,12 @@ class FractionalAvgPoolOp : public OpKernel {\n     OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                 errors::InvalidArgument(\n                     \"pooling_ratio field must specify 4 dimensions\"));\n+    for (std::size_t i = 0; i < pooling_ratio_.size(); ++i) {\n+      OP_REQUIRES(context, pooling_ratio_[i] >= 1,\n+                  errors::InvalidArgument(\n+                      \"pooling_ratio cannot be smaller than 1, got: \",\n+                      pooling_ratio_[i]));\n+    }\n     OP_REQUIRES(\n         context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n         errors::Unimplemented(\"Fractional average pooling is not yet \"\n@@ -82,9 +88,11 @@ class FractionalAvgPoolOp : public OpKernel {\n     for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n       input_size[i] = tensor_in.dim_size(i);\n       OP_REQUIRES(\n-          context, pooling_ratio_[i] <= input_size[i],\n-          errors::InvalidArgument(\n-              \"Pooling ratio cannot be bigger than input tensor dim size.\"));\n+          context, input_size[i] >= pooling_ratio_[i],\n+          errors::InvalidArgument(\"Pooling ratio is higher than input \"\n+                                  \"dimension size for dimension \",\n+                                  i, \". Input dim size: \", input_size[i],\n+                                  \" pooling ratio: \", pooling_ratio_[i]));\n     }\n     // Output size.\n     for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n",
            "@@ -45,6 +45,12 @@ class FractionalMaxPoolOp : public OpKernel {\n     OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                 errors::InvalidArgument(\"pooling_ratio field must \"\n                                         \"specify 4 dimensions\"));\n+    for (std::size_t i = 0; i < pooling_ratio_.size(); ++i) {\n+      OP_REQUIRES(context, pooling_ratio_[i] >= 1,\n+                  errors::InvalidArgument(\n+                      \"pooling_ratio cannot be smaller than 1, got: \",\n+                      pooling_ratio_[i]));\n+    }\n \n     OP_REQUIRES(\n         context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n",
            "@@ -63,6 +63,13 @@ Status FractionalPoolShapeFn(InferenceContext* c) {\n     }\n   }\n \n+  for (std::size_t i = 0; i < pooling_ratio.size(); ++i) {\n+    if (pooling_ratio[i] < 1) {\n+      return errors::InvalidArgument(\n+          \"pooling_ratio cannot be smaller than 1, got: \", pooling_ratio[i]);\n+    }\n+  }\n+\n   c->set_output(0, c->MakeShape(output_dims));\n   c->set_output(1, c->Vector(output_dims[1]));\n   c->set_output(2, c->Vector(output_dims[2]));\n",
            "@@ -523,7 +523,8 @@ TEST(NNOpsTest, FractionalPool_ShapeFn) {\n                        .Finalize(&op.node_def));\n     };\n \n-    set_op(std::vector<float>{2.0f, 1, 1 / 1.5f, 1 / 2.0f});\n+    // pooling_ratio must >= 1.0\n+    set_op(std::vector<float>{2.0f, 1, 1.5f, 4.0f});\n \n     // Rank check.\n     INFER_ERROR(\"must be rank 4\", op, \"[?,?,?]\");\n@@ -532,11 +533,11 @@ TEST(NNOpsTest, FractionalPool_ShapeFn) {\n     INFER_OK(op, \"?\", \"[?,?,?,?];[?];[?]\");\n     INFER_OK(op, \"[?,?,?,?]\", \"[?,?,?,?];[?];[?]\");\n \n-    INFER_OK(op, \"[10,20,30,40]\", \"[5,20,45,80];[20];[45]\");\n-    INFER_OK(op, \"[?,20,30,40]\", \"[?,20,45,80];[20];[45]\");\n-    INFER_OK(op, \"[10,?,30,40]\", \"[5,?,45,80];[?];[45]\");\n-    INFER_OK(op, \"[10,20,?,40]\", \"[5,20,?,80];[20];[?]\");\n-    INFER_OK(op, \"[10,20,30,?]\", \"[5,20,45,?];[20];[45]\");\n+    INFER_OK(op, \"[10,20,30,40]\", \"[5,20,20,10];[20];[20]\");\n+    INFER_OK(op, \"[?,20,30,40]\", \"[?,20,20,10];[20];[20]\");\n+    INFER_OK(op, \"[10,?,30,40]\", \"[5,?,20,10];[?];[20]\");\n+    INFER_OK(op, \"[10,20,?,40]\", \"[5,20,?,10];[20];[?]\");\n+    INFER_OK(op, \"[10,20,30,?]\", \"[5,20,20,?];[20];[20]\");\n \n     // Wrong number of values for pooling_ratio.\n     set_op(std::vector<float>{.5, 1.0, 1.5});\n",
            "@@ -333,6 +333,41 @@ class FractionalAvgTest(test.TestCase):\n \n         self.evaluate(z)\n \n+  def testPoolingRatioHasMoreDimThanInput(self):\n+    with self.cached_session() as _:\n+      with self.assertRaisesRegex(\n+          errors.InvalidArgumentError,\n+          r\"Pooling ratio is higher than input dimension size for dimension 1.*\"\n+      ):\n+        result = nn_ops.gen_nn_ops.fractional_avg_pool(\n+            value=constant_op.constant(\n+                value=[[[[1, 4, 2, 3]]]], dtype=dtypes.int64),\n+            pooling_ratio=[1.0, 1.44, 1.73, 1.0],\n+            pseudo_random=False,\n+            overlapping=False,\n+            deterministic=False,\n+            seed=0,\n+            seed2=0,\n+            name=None)\n+        self.evaluate(result)\n+\n+  def testPoolingRatioValueOutOfRange(self):\n+    with self.cached_session() as _:\n+      # Whether turn on `TF2_BEHAVIOR` generates different error messages\n+      with self.assertRaisesRegex(\n+          (errors.InvalidArgumentError, ValueError),\n+          r\"(pooling_ratio cannot be smaller than 1, got: .*)|(is negative)\"):\n+        result = nn_ops.gen_nn_ops.fractional_avg_pool(\n+            value=np.zeros([3, 30, 30, 3]),\n+            pooling_ratio=[1, -1, 3, 1],\n+            pseudo_random=False,\n+            overlapping=False,\n+            deterministic=False,\n+            seed=0,\n+            seed2=0,\n+        )\n+        self.evaluate(result)\n+\n \n class FractionalAvgPoolGradTest(test.TestCase):\n   \"\"\"Tests for FractionalAvgPoolGrad.\n",
            "@@ -320,7 +320,7 @@ class FractionalMaxPoolTest(test.TestCase):\n       nn_ops.fractional_max_pool(\n           rand_mat, [1, 1.5, 1.5, 1], seed=1, seed2=1, deterministic=True)\n \n-  def testPoolingRatio(self):\n+  def testPoolingRatioHasMoreDimThanInput(self):\n     with self.cached_session() as _:\n       with self.assertRaisesRegex(\n           errors.InvalidArgumentError,\n@@ -338,6 +338,23 @@ class FractionalMaxPoolTest(test.TestCase):\n             name=None)\n         self.evaluate(result)\n \n+  def testPoolingRatioValueOutOfRange(self):\n+    with self.cached_session() as _:\n+      # Whether turn on `TF2_BEHAVIOR` generates different error messages\n+      with self.assertRaisesRegex(\n+          (errors.InvalidArgumentError, ValueError),\n+          r\"(pooling_ratio cannot be smaller than 1, got: .*)|(is negative)\"):\n+        result = nn_ops.gen_nn_ops.fractional_max_pool(\n+            value=np.zeros([3, 30, 30, 3]),\n+            pooling_ratio=[1, -1, 3, 1],\n+            pseudo_random=False,\n+            overlapping=False,\n+            deterministic=False,\n+            seed=0,\n+            seed2=0,\n+        )\n+        self.evaluate(result)\n+\n \n class FractionalMaxPoolGradTest(test.TestCase):\n   \"\"\"Tests for FractionalMaxPoolGrad.\n"
        ],
        "Title": "\n          FractionalMaxPool and FractionalAvgPool heap out-of-buffer\n        "
    },
    {
        "Bug description": "Inputs  dense_features  or  example_state_data  not of rank 2 will trigger a  CHECK  fail in  SdcaOptimizer .",
        "Sample Code": "tf.raw_ops.SdcaOptimizer(\n    sparse_example_indices=4 * [tf.random.uniform([5,5,5,3], dtype=tf.dtypes.int64, maxval=100)],\n    sparse_feature_indices=4 * [tf.random.uniform([5,5,5,3], dtype=tf.dtypes.int64, maxval=100)],\n    sparse_feature_values=8 * [tf.random.uniform([5,5,5,3], dtype=tf.dtypes.float32, maxval=100)],\n    dense_features=4 * [tf.random.uniform([5,5,5,3], dtype=tf.dtypes.float32, maxval=100)],\n    example_weights=tf.random.uniform([5,5,5,3], dtype=tf.dtypes.float32, maxval=100),\n    example_labels=tf.random.uniform([5,5,5,3], dtype=tf.dtypes.float32, maxval=100),\n    sparse_indices=4 * [tf.random.uniform([5,5,5,3], dtype=tf.dtypes.int64, maxval=100)],\n    sparse_weights=4 * [tf.random.uniform([5,5,5,3], dtype=tf.dtypes.float32, maxval=100)],\n    dense_weights=4 * [tf.random.uniform([5,5,5,3], dtype=tf.dtypes.float32, maxval=100)],\n    example_state_data=tf.random.uniform([5,5,5,3], dtype=tf.dtypes.float32, maxval=100),\n    loss_type=\"squared_loss\",\n    l1=0.0,\n    l2=0.0,\n    num_loss_partitions=1,\n    num_inner_iterations=1,\n    ,\n    adaptative=False,)",
        "Bug fix": [
            "@@ -389,6 +389,13 @@ Status Examples::Initialize(OpKernelContext* const context,\n   OpInputList dense_features_inputs;\n   TF_RETURN_IF_ERROR(\n       context->input_list(\"dense_features\", &dense_features_inputs));\n+  for (int i = 0; i < dense_features_inputs.size(); ++i) {\n+    if (!TensorShapeUtils::IsMatrix(dense_features_inputs[i].shape())) {\n+      return errors::InvalidArgument(\"Dense features at index \", i,\n+                                     \" must be rank 2 but is rank \",\n+                                     dense_features_inputs[i].dims());\n+    }\n+  }\n \n   examples_.clear();\n   examples_.resize(num_examples);\n",
            "@@ -49,6 +49,7 @@ limitations under the License.\n #include \"tensorflow/core/lib/core/status.h\"\n #include \"tensorflow/core/lib/core/stringpiece.h\"\n #include \"tensorflow/core/lib/gtl/inlined_vector.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/platform/fingerprint.h\"\n #include \"tensorflow/core/platform/macros.h\"\n #include \"tensorflow/core/platform/mutex.h\"\n@@ -142,6 +143,10 @@ void DoCompute(const ComputeOptions& options, OpKernelContext* const context) {\n   const Tensor* example_state_data_t;\n   OP_REQUIRES_OK(context,\n                  context->input(\"example_state_data\", &example_state_data_t));\n+  OP_REQUIRES(\n+      context, TensorShapeUtils::IsMatrix(example_state_data_t->shape()),\n+      errors::InvalidArgument(\"example_state_data must be rank 2 but is rank \",\n+                              example_state_data_t->dims()));\n   TensorShape expected_example_state_shape({examples.num_examples(), 4});\n   OP_REQUIRES(context,\n               example_state_data_t->shape() == expected_example_state_shape,\n"
        ],
        "Title": "\n          `CHECK` fail via inputs in `SdcaOptimizer`\n        "
    },
    {
        "Bug description": "If  SparseFillEmptyRowsGrad  is given empty inputs, TensorFlow will crash.",
        "Sample Code": "tf.raw_ops.SparseFillEmptyRowsGrad(\n    reverse_index_map=[], grad_values=[], name=None\n)\n)",
        "Bug fix": [
            "@@ -297,9 +297,12 @@ struct SparseFillEmptyRows<GPUDevice, T, Tindex> {\n       empty_row_indicator = empty_row_indicator_t.vec<bool>().data();\n     }\n \n-    TF_RETURN_IF_ERROR(wrap_kernel_call(ComputeEmptyRowIndicatorKernel<Tindex>,\n-                                        /*device=*/device, /*size=*/dense_rows,\n-                                        elements_per_row, empty_row_indicator));\n+    if (dense_rows > 0) {\n+      TF_RETURN_IF_ERROR(\n+          wrap_kernel_call(ComputeEmptyRowIndicatorKernel<Tindex>,\n+                           /*device=*/device, /*size=*/dense_rows,\n+                           elements_per_row, empty_row_indicator));\n+    }\n \n     // For each row, the number of empty rows up to and including that row.\n     Tensor num_empty_rows_through_t;\n@@ -405,14 +408,16 @@ struct SparseFillEmptyRows<GPUDevice, T, Tindex> {\n             done);\n       }\n \n-      OP_REQUIRES_OK_ASYNC(\n-          context,\n-          wrap_kernel_call(ScatterNewElementsKernel<T, Tindex>,\n-                           /*device=*/device, /*size=*/dense_rows, rank,\n-                           default_value, num_empty_rows_through,\n-                           input_row_ends, empty_row_indicator, output_indices,\n-                           output_values),\n-          done);\n+      if (dense_rows > 0) {\n+        OP_REQUIRES_OK_ASYNC(\n+            context,\n+            wrap_kernel_call(ScatterNewElementsKernel<T, Tindex>,\n+                             /*device=*/device, /*size=*/dense_rows, rank,\n+                             default_value, num_empty_rows_through,\n+                             input_row_ends, empty_row_indicator,\n+                             output_indices, output_values),\n+            done);\n+      }\n \n       done();\n     };\n@@ -461,9 +466,11 @@ struct SparseFillEmptyRows<GPUDevice, T, Tindex> {\n     TF_RETURN_IF_ERROR(\n         context->allocate_temp(index_type, TensorShape({N}), &row_indices_t));\n     auto row_indices = row_indices_t.flat<Tindex>();\n-    TF_RETURN_IF_ERROR(wrap_kernel_call(CopyRowIndicesKernel<Tindex>,\n-                                        /*device=*/device, /*size=*/N, rank,\n-                                        indices, row_indices));\n+    if (N > 0) {\n+      TF_RETURN_IF_ERROR(wrap_kernel_call(CopyRowIndicesKernel<Tindex>,\n+                                          /*device=*/device, /*size=*/N, rank,\n+                                          indices, row_indices));\n+    }\n     // Allocate input_index_map.\n     TF_RETURN_IF_ERROR(context->allocate_temp(index_type, TensorShape({N}),\n                                               input_index_map_t));\n@@ -528,9 +535,11 @@ struct SparseFillEmptyRowsGrad<GPUDevice, T, Tindex> {\n     auto visited = visited_t.vec<bool>();\n     visited.device(device) = visited.constant(false);\n \n-    TF_RETURN_IF_ERROR(wrap_kernel_call(\n-        GatherOriginalGradValuesKernel<T, Tindex>, /*device=*/device,\n-        /*size=*/N, reverse_index_map, grad_values, d_values, visited));\n+    if (N > 0) {\n+      TF_RETURN_IF_ERROR(wrap_kernel_call(\n+          GatherOriginalGradValuesKernel<T, Tindex>, /*device=*/device,\n+          /*size=*/N, reverse_index_map, grad_values, d_values, visited));\n+    }\n \n     // Now we mask out the visited values and sum the remaining ones (which\n     // correspond to the empty rows in the forward input) to compute\n",
            "@@ -514,6 +514,13 @@ class SparseFillEmptyRowsTest(test_util.TensorFlowTestCase):\n         self.assertAllEqual(empty_row_indicator_out,\n                             np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n \n+  def testSparseFillEmptyRowsGradEmpty(self):\n+    with test_util.use_gpu():\n+      grad, _ = self.evaluate(\n+          sparse_ops.sparse_fill_empty_rows_grad(\n+              reverse_index_map=[], grad_values=[]))\n+      self.assertAllEqual(grad, [])\n+\n   @test_util.run_deprecated_v1\n   def testFillFloat(self):\n     with self.session():\n"
        ],
        "Title": "\n          `CHECK` fail via inputs in `SparseFillEmptyRowsGrad`\n        "
    },
    {
        "Bug description": "If  FractionMaxPoolGrad  is given outsize inputs  row_pooling_sequence  and  col_pooling_sequence , TensorFlow will crash.",
        "Sample Code": "tf.raw_ops.FractionMaxPoolGrad(\n\torig_input = [[[[1, 1, 1, 1, 1]]]],\n    orig_output = [[[[1, 1, 1]]]],\n    out_backprop = [[[[3], [3], [6]]]],\n    row_pooling_sequence = [-0x4000000, 1, 1], \n    col_pooling_sequence = [-0x4000000, 1, 1], \n    overlapping = False\n )\n )",
        "Bug fix": [
            "@@ -258,6 +258,18 @@ class FractionalMaxPoolGradOp : public OpKernel {\n     OP_REQUIRES(context, tensor_out.NumElements() > 0,\n                 errors::InvalidArgument(\"orig_output must not be empty, got \",\n                                         tensor_out.DebugString()));\n+    OP_REQUIRES(\n+        context,\n+        height_seq_tensor.NumElements() * width_seq_tensor.NumElements() <=\n+            tensor_in.NumElements(),\n+        errors::InvalidArgument(\n+            \"Pooling region has more elements than the input tensor. \"\n+            \"row_pooling_sequence: \",\n+            height_seq_tensor.DebugString(),\n+            \"col_pooling_sequence: \", width_seq_tensor.DebugString(),\n+            \"orig_input: \", tensor_in.DebugString()));\n+\n+    //\n     std::vector<int64_t> input_size(tensor_in_and_out_dims);\n     std::vector<int64_t> output_size(tensor_in_and_out_dims);\n     for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n",
            "@@ -632,7 +632,7 @@ class FractionalMaxPoolGradTest(test.TestCase):\n \n   def testInvalidSeqRaiseErrorForFractionalMaxPoolGrad(self):\n     with self.assertRaises(errors.InvalidArgumentError):\n-      with self.cached_session() as _:\n+      with self.cached_session():\n         overlapping = True\n         orig_input = constant_op.constant(\n             .453409232, shape=[1, 7, 13, 1], dtype=dtypes.float32)\n@@ -653,6 +653,24 @@ class FractionalMaxPoolGradTest(test.TestCase):\n             overlapping=overlapping)\n         self.evaluate(t)\n \n+  def testOverLargeSeqRaiseErrorForFractionalMaxPoolGrad(self):\n+    with self.assertRaises(errors.InvalidArgumentError):\n+      with self.cached_session():\n+        overlapping = False\n+        orig_input = [[[[1, 1, 1, 1, 1]]]]\n+        orig_output = [[[[1, 1, 1]]]]\n+        out_backprop = [[[[3], [3], [6]]]]\n+        row_pooling_sequence = [-0x4000000, 1, 1]\n+        col_pooling_sequence = [-0x4000000, 1, 1]\n+        t = gen_nn_ops.FractionalMaxPoolGrad(\n+            orig_input=orig_input,\n+            orig_output=orig_output,\n+            out_backprop=out_backprop,\n+            row_pooling_sequence=row_pooling_sequence,\n+            col_pooling_sequence=col_pooling_sequence,\n+            overlapping=overlapping)\n+        self.evaluate(t)\n+\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          `FractionalMaxPoolGrad` Heap OOB\n        "
    },
    {
        "Bug description": "If  ThreadUnsafeUnigramCandidateSampler  is given input  filterbank_channel_count  greater than the allowed max size, TensorFlow will crash.",
        "Sample Code": "tf.raw_ops.Mfcc(\n    spectrogram = [[[1.38, 6.32, 5.75, 9.51]]],\n    sample_rate = 2,\n    upper_frequency_limit = 5.0,\n    lower_frequency_limit = 1.0,\n    filterbank_channel_count = 2**31 - 1,\n    dct_coefficient_count = 1\n)\n)",
        "Bug fix": [
            "@@ -38,8 +38,10 @@ bool Mfcc::Initialize(int input_length, double input_sample_rate) {\n   bool initialized = mel_filterbank_.Initialize(\n       input_length, input_sample_rate, filterbank_channel_count_,\n       lower_frequency_limit_, upper_frequency_limit_);\n-  initialized &=\n-      dct_.Initialize(filterbank_channel_count_, dct_coefficient_count_);\n+  if (initialized) {\n+    initialized =\n+        dct_.Initialize(filterbank_channel_count_, dct_coefficient_count_);\n+  }\n   initialized_ = initialized;\n   return initialized;\n }\n",
            "@@ -32,6 +32,8 @@ limitations under the License.\n \n #include <math.h>\n \n+#include <limits>\n+\n #include \"tensorflow/core/platform/logging.h\"\n \n namespace tensorflow {\n@@ -74,7 +76,17 @@ bool MfccMelFilterbank::Initialize(int input_length, double input_sample_rate,\n \n   // An extra center frequency is computed at the top to get the upper\n   // limit on the high side of the final triangular filter.\n-  center_frequencies_.resize(num_channels_ + 1);\n+  std::size_t center_frequencies_size = std::size_t(num_channels_) + 1;\n+  if (center_frequencies_size >= std::numeric_limits<int>::max() ||\n+      center_frequencies_size > center_frequencies_.max_size()) {\n+    LOG(ERROR) << \"Number of filterbank channels must be less than \"\n+               << std::numeric_limits<int>::max()\n+               << \" and less than or equal to \"\n+               << center_frequencies_.max_size();\n+    return false;\n+  }\n+  center_frequencies_.resize(center_frequencies_size);\n+\n   const double mel_low = FreqToMel(lower_frequency_limit);\n   const double mel_hi = FreqToMel(upper_frequency_limit);\n   const double mel_span = mel_hi - mel_low;\n",
            "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"tensorflow/core/kernels/mfcc_mel_filterbank.h\"\n \n+#include <limits>\n #include <vector>\n \n #include \"tensorflow/core/platform/test.h\"\n@@ -85,4 +86,37 @@ TEST(MfccMelFilterbankTest, IgnoresExistingContentOfOutputVector) {\n   }\n }\n \n+TEST(MfccMelFilterbankTest, FailsWhenChannelsGreaterThanMaxIntValue) {\n+  // Test for bug where vector throws a length_error when it suspects the size\n+  // to be more than it's max_size. For now, we fail initialization when the\n+  // number of requested channels is >= the maximum value int can take (since\n+  // num_channels_ is an int).\n+  MfccMelFilterbank filterbank;\n+\n+  const int kSampleCount = 513;\n+  std::size_t num_channels = std::numeric_limits<int>::max();\n+  bool initialized = filterbank.Initialize(\n+      kSampleCount, 2 /* sample rate */, num_channels /* channels */,\n+      1.0 /*  lower frequency limit */, 5.0 /* upper frequency limit */);\n+\n+  EXPECT_FALSE(initialized);\n+}\n+\n+TEST(MfccMelFilterbankTest, FailsWhenChannelsGreaterThanMaxSize) {\n+  // Test for bug where vector throws a length_error when it suspects the size\n+  // to be more than it's max_size. For now, we fail initialization when the\n+  // number of requested channels is > than std::vector<double>::max_size().\n+  MfccMelFilterbank filterbank;\n+\n+  const int kSampleCount = 513;\n+  // Set num_channels to exceed the max_size a double vector can\n+  // theoretically take.\n+  std::size_t num_channels = std::vector<double>().max_size() + 1;\n+  bool initialized = filterbank.Initialize(\n+      kSampleCount, 2 /* sample rate */, num_channels /* channels */,\n+      1.0 /*  lower frequency limit */, 5.0 /* upper frequency limit */);\n+\n+  EXPECT_FALSE(initialized);\n+}\n+\n }  // namespace tensorflow\n",
            "@@ -25,7 +25,7 @@ limitations under the License.\n \n namespace tensorflow {\n \n-// Create a speech fingerpring from spectrogram data.\n+// Create a speech fingerprint from spectrogram data.\n class MfccOp : public OpKernel {\n  public:\n   explicit MfccOp(OpKernelConstruction* context) : OpKernel(context) {\n@@ -60,10 +60,12 @@ class MfccOp : public OpKernel {\n     mfcc.set_lower_frequency_limit(lower_frequency_limit_);\n     mfcc.set_filterbank_channel_count(filterbank_channel_count_);\n     mfcc.set_dct_coefficient_count(dct_coefficient_count_);\n-    OP_REQUIRES(context, mfcc.Initialize(spectrogram_channels, sample_rate),\n-                errors::InvalidArgument(\n-                    \"Mfcc initialization failed for channel count \",\n-                    spectrogram_channels, \" and sample rate \", sample_rate));\n+    OP_REQUIRES(\n+        context, mfcc.Initialize(spectrogram_channels, sample_rate),\n+        errors::InvalidArgument(\"Mfcc initialization failed for channel count \",\n+                                spectrogram_channels, \", sample rate \",\n+                                sample_rate, \" and filterbank_channel_count \",\n+                                filterbank_channel_count_));\n \n     Tensor* output_tensor = nullptr;\n     OP_REQUIRES_OK(context,\n"
        ],
        "Title": "\n          `tf.raw_ops.Mfcc` crashes\n        "
    },
    {
        "Bug description": "If  MirrorPadGrad  is given outsize input  paddings , TensorFlow will give a heap OOB error.",
        "Sample Code": "tf.raw_ops.MirrorPadGrad(input=[1],\n             paddings=[[0x77f00000,0xa000000]],\n             ]],\n             mode = 'REFLECT')",
        "Bug fix": [
            "@@ -297,13 +297,21 @@ class MirrorPadGradOp : public OpKernel {\n     TensorShape output_shape;\n     typename TTypes<Tpaddings>::ConstMatrix paddings = in1.matrix<Tpaddings>();\n     for (int d = 0; d < dims; ++d) {\n-      const Tpaddings before = paddings(d, 0);  // Pad before existing elements.\n-      const Tpaddings after = paddings(d, 1);   // Pad after existing elements.\n+      const int64_t before = paddings(d, 0);  // Pad before existing elements.\n+      const int64_t after = paddings(d, 1);   // Pad after existing elements.\n       OP_REQUIRES(context, before >= 0 && after >= 0,\n                   errors::InvalidArgument(\n                       \"Paddings must be non-negative: \", before, \", \", after));\n \n-      const int64_t out_size = in0.dim_size(d) - (before + after);\n+      const int64_t in_size = in0.dim_size(d);\n+      const int64_t total_padding = before + after;\n+      OP_REQUIRES(\n+          context, total_padding < in_size && total_padding >= 0,\n+          errors::InvalidArgument(\n+              \"Total paddings must be less than the input dimension size: \",\n+              total_padding, \" was not less than \", in_size));\n+\n+      const int64_t out_size = in_size - total_padding;\n       if (offset_ == 0) {  // SYMMETRIC mode.\n         OP_REQUIRES(context, before <= out_size && after <= out_size,\n                     errors::InvalidArgument(\"paddings must be no greater \"\n",
            "@@ -1617,6 +1617,21 @@ class PadTest(test_util.TensorFlowTestCase):\n                           [[0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 2, 3, 0, 0],\n                            [0, 0, 4, 5, 6, 0, 0], [0, 0, 0, 0, 0, 0, 0]])\n \n+  # b/246325518: Bad shape size. Explicitly testing different execution paths.\n+  def testInvalidMirrorPadGradEagerMode(self):\n+    with context.eager_mode():\n+      with self.assertRaises(Exception):\n+        gen_array_ops.MirrorPadGrad(\n+            input=[1], paddings=[[0x77f00000, 0xa000000]], mode=\"REFLECT\")\n+\n+  # b/246325518: Bad shape size. Explicitly testing different execution paths.\n+  def testInvalidMirrorPadGradGraphMode(self):\n+    with context.graph_mode():\n+      with self.assertRaises(Exception):\n+        result = gen_array_ops.MirrorPadGrad(\n+            input=[1], paddings=[[0x77f00000, 0xa000000]], mode=\"REFLECT\")\n+        self.evaluate(result)\n+\n   def testSymmetricMirrorPadGrad(self):\n     t = np.broadcast_to(np.arange(0, 7), (3, 2, 1, 7))\n     paddings = constant_op.constant([\n"
        ],
        "Title": "\n          `MirrorPadGrad` heap oob\n        "
    },
    {
        "Bug description": "The reference kernel of the  CONV_3D_TRANSPOSE  TensorFlow Lite operator wrongly increments the data_ptr when adding the bias to the result.",
        "Sample Code": "model = tf.keras.Sequential(\n    [\n        tf.keras.layers.InputLayer(input_shape=(2, 2, 2, 1024), batch_size=1),\n        tf.keras.layers.Conv3DTranspose(\n            filters=8,\n            kernel_size=(2, 2, 2),\n            padding=\"same\",\n            data_format=\"channels_last\",\n        ),\n    ]\n)\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\ninterpreter = tf.lite.Interpreter(\n    model_content=tflite_model,\n    experimental_op_resolver_type=tf.lite.experimental.OpResolverType.BUILTIN_REF,\n)\n\ninterpreter.allocate_tensors()\ninterpreter.set_tensor(\n    interpreter.get_input_details()[0][\"index\"], tf.zeros(shape=[1, 2, 2, 2, 1024])\n)\n])\n)\ninterpreter.invoke()",
        "Bug fix": [
            "@@ -111,14 +111,13 @@ inline void Conv3DTranspose(\n   if (bias_data) {\n     const int outer_size =\n         batches * output_depth * output_height * output_width;\n-    const int num_channels = input_shape.Dims(4);\n     for (int n = 0; n < outer_size; ++n) {\n       for (int c = 0; c < output_num_channels; ++c) {\n         data_ptr[c] = ActivationFunctionWithMinMax(data_ptr[c] + bias_data[c],\n                                                    float_activation_min,\n                                                    float_activation_max);\n       }\n-      data_ptr += num_channels;\n+      data_ptr += output_num_channels;\n     }\n   } else {\n     const int flat_size = output_shape.FlatSize();\n"
        ],
        "Title": "\n          Buffer overflow in `CONV_3D_TRANSPOSE` on TFLite\n        "
    },
    {
        "Bug description": "If  tf.raw_ops.TensorListResize  is given a nonscalar value for input  size , it results  CHECK  fail which can be used to trigger a denial of service attack.",
        "Sample Code": "import tensorflow as tf\n\na = data_structures.tf_tensor_list_new(elements = tf.constant(value=[3, 4, 5]))\nb = np.zeros([0, 2, 3, 3])\n\n])\n\ntf.raw_ops.TensorListResize(input_handle=a, size=b)",
        "Bug fix": [
            "@@ -375,6 +375,8 @@ class TensorListResize : public OpKernel {\n   void Compute(OpKernelContext* c) override {\n     const TensorList* input_list = nullptr;\n     OP_REQUIRES_OK(c, GetInputList(c, 0, &input_list));\n+    OP_REQUIRES(c, TensorShapeUtils::IsScalar(c->input(1).shape()),\n+                errors::InvalidArgument(\"size must be a scalar\"));\n     int32_t size = c->input(1).scalar<int32>()();\n     OP_REQUIRES(\n         c, size >= 0,\n",
            "@@ -1658,6 +1658,15 @@ class ListOpsTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n       l = list_ops.tensor_list_resize(l, -1)\n       self.evaluate(l)\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def testResizeWithNonScalarFails(self):\n+    l = list_ops.tensor_list_from_tensor([3, 4, 5], element_shape=[])\n+    size = np.zeros([0, 2, 3, 3])\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                r\"Shape must be rank 0 but is rank \\d+|\"\n+                                r\"\\w+ must be a scalar\"):\n+      self.evaluate(gen_list_ops.TensorListResize(input_handle=l, size=size))\n+\n   @test_util.run_deprecated_v1\n   @test_util.enable_control_flow_v2\n   def testSkipEagerResizeGrad(self):\n"
        ],
        "Title": "\n          `CHECK_EQ` fail in `tf.raw_ops.TensorListResize`\n        "
    },
    {
        "Bug description": "If  tf.raw_ops.TensorListConcat  is given  element_shape=[] , it results segmentation fault which can be used to trigger a denial of service attack.",
        "Sample Code": "tf.raw_ops.TensorListConcat(\n    input_handle=tf.data.experimental.to_variant(tf.data.Dataset.from_tensor_slices([1, 2, 3])),\n    element_dtype=tf.dtypes.float32,\n    element_shape=[]\n)[]\n)",
        "Bug fix": [
            "@@ -395,8 +395,11 @@ class TensorListConcat : public OpKernel {\n   void Compute(OpKernelContext* c) override {\n     PartialTensorShape element_shape_except_first_dim;\n     if (!element_shape_.unknown_rank()) {\n-      element_shape_except_first_dim = PartialTensorShape(\n-          gtl::ArraySlice<int64_t>(element_shape_.dim_sizes()).subspan(1));\n+      auto dim_sizes = element_shape_.dim_sizes();\n+      OP_REQUIRES(c, !dim_sizes.empty(),\n+                  errors::InvalidArgument(\"element_shape must not be empty\"));\n+      element_shape_except_first_dim =\n+          PartialTensorShape(gtl::ArraySlice<int64_t>(dim_sizes).subspan(1));\n     }\n     // Check that the input Variant tensor is indeed a TensorList and has the\n     // correct element type.\n",
            "@@ -1514,6 +1514,15 @@ class ListOpsTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n       t = list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n       self.evaluate(t)\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def testConcatWithInvalidElementShape(self):\n+    l = list_ops.tensor_list_reserve(\n+        element_dtype=dtypes.float32, element_shape=[], num_elements=0)\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                r\"element_shape must not be empty\"):\n+      self.evaluate(gen_list_ops.tensor_list_concat(\n+          input_handle=l, element_dtype=dtypes.float32, element_shape=[]))\n+\n   def testEmptyTensorListInvalidShape(self):\n     with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                 r\"Shape must be at most rank 1 but is rank 2\"):\n"
        ],
        "Title": "\n          Segfault in `tf.raw_ops.TensorListConcat`\n        "
    },
    {
        "Bug description": "If  BCast::ToShape  is given input larger than an  int32 , it will crash, despite being supposed to handle up to an  int64 . An example can be seen in  tf.experimental.numpy.outer  by passing in large input to the input  b .",
        "Sample Code": "value = tf.constant(shape=[2, 1024, 1024, 1024], value=False)\n)\ntf.experimental.numpy.outer(a=6,b=value)",
        "Bug fix": [
            "@@ -134,7 +134,7 @@ BCastList<N>::BCastList(const BCastList::Vec (&x)[N],\n   typedef BCastList::Vec Vec;\n \n   // Safely multiplies dimensions taking into account symbolic shapes.\n-  auto mul_dims = [](int64_t dim1, int64_t dim2) -> int64 {\n+  auto mul_dims = [](int64_t dim1, int64_t dim2) -> int64_t {\n     return dim1 != 0 && dim2 != 0 && (dim1 < 0 || dim2 < 0) ? -1 : dim1 * dim2;\n   };\n \n@@ -199,7 +199,7 @@ BCastList<N>::BCastList(const BCastList::Vec (&x)[N],\n   }\n   Vec output;\n   bool output_dim_set = false;\n-  int output_dim = -1;\n+  int64_t output_dim = -1;\n   bool none_is_one = true;\n   bool set_one = false;\n   for (int j = 0; j < largest_rank; ++j) {\n",
            "@@ -375,6 +375,13 @@ TEST(BCastTest, Basic_Tensor_Scalar) {\n             \"[11,7,5,3,2]\"\n             \"[11,7,5,3,2]\"\n             \"[0,1,2,3,4][]\");\n+\n+  // int32 edge-case:\n+  EXPECT_EQ(BCast({1, 2147483648}, {1}),\n+            \"[2147483648][1][1][2147483648]\"\n+            \"[2147483648]\"\n+            \"[1,2147483648]\"\n+            \"[0][0,1]\");\n }\n \n TEST(BCastTest, Basic_Tensor_With_DimSize_1_Scalar) {\n"
        ],
        "Title": "\n          `CHECK` fail in `BCast` overflow\n        "
    }
]
[
    {
        "Bug description": "If a list of quantized tensors is assigned to an attribute, the pywrap code fails to parse the tensor and returns a  nullptr , which is not caught. An example can be seen in  tf.compat.v1.extract_volume_patches  by passing in quantized tensors as input  ksizes .",
        "Sample Code": "import tensorflow as tf\n\na_input = np.array([1, -1], dtype= np.int32)\na_ksizes =  a_strides = tf.constant(dtype=tf.dtypes.qint16, value=[[1, 4], [5, 2]])\n\n\n]])\n\n\ntf.compat.v1.extract_volume_patches(input=a_input,ksizes=a_ksizes,strides=a_strides,padding='VALID')",
        "Bug fix": [
            "@@ -397,11 +397,20 @@ bool SetOpAttrList(TFE_Context* ctx, TFE_Op* op, const char* key,\n   const int num_values = PySequence_Size(py_list);\n   if (attr_list_sizes != nullptr) (*attr_list_sizes)[key] = num_values;\n \n-#define PARSE_LIST(c_type, parse_fn)                                      \\\n-  std::unique_ptr<c_type[]> values(new c_type[num_values]);               \\\n-  for (int i = 0; i < num_values; ++i) {                                  \\\n-    tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));   \\\n-    if (!parse_fn(key, py_value.get(), status, &values[i])) return false; \\\n+#define PARSE_LIST(c_type, parse_fn)                                       \\\n+  std::unique_ptr<c_type[]> values(new c_type[num_values]);                \\\n+  for (int i = 0; i < num_values; ++i) {                                   \\\n+    tensorflow::Safe_PyObjectPtr py_value(PySequence_ITEM(py_list, i));    \\\n+    if (py_value == nullptr) {                                             \\\n+      TF_SetStatus(status, TF_INVALID_ARGUMENT,                            \\\n+                   tensorflow::strings::StrCat(                            \\\n+                       \"Expecting sequence of \" #c_type \" for attr \", key, \\\n+                       \", got \", py_list->ob_type->tp_name)                \\\n+                       .c_str());                                          \\\n+      return false;                                                        \\\n+    } else if (!parse_fn(key, py_value.get(), status, &values[i])) {       \\\n+      return false;                                                        \\\n+    }                                                                      \\\n   }\n \n   if (type == TF_ATTR_STRING) {\n",
            "@@ -17,7 +17,9 @@\n import numpy as np\n \n from tensorflow.python.framework import constant_op\n+from tensorflow.python.framework import dtypes\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import math_ops\n from tensorflow.python.platform import test\n \n \n@@ -139,6 +141,17 @@ class ExtractImagePatches(test.TestCase):\n             padding=padding,\n             patches=patches)\n \n+  def testInvalidAttributes(self):\n+    \"\"\"Test for passing weird things into ksizes.\"\"\"\n+    with self.assertRaisesRegex(TypeError, \"Expected list\"):\n+      image = constant_op.constant([0.0])\n+      ksizes = math_ops.cast(\n+          constant_op.constant(dtype=dtypes.int16, value=[[1, 4], [5, 2]]),\n+          dtype=dtypes.qint16)\n+      strides = [1, 1, 1, 1]\n+      self.evaluate(\n+          array_ops.extract_image_patches(\n+              image, ksizes=ksizes, strides=strides, padding=\"SAME\"))\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          Segfault via invalid attributes in `pywrap_tfe_src.cc`\n        "
    },
    {
        "Bug description": "When running on GPU,  tf.image.generate_bounding_box_proposals  receives a  scores  input that must be of rank 4 but is not checked.",
        "Sample Code": "a = tf.constant(value=[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0]])\nb = tf.constant(value=[1])\n\n])\n\ntf.image.generate_bounding_box_proposals(scores=a,bbox_deltas=a,image_info=a,anchors=a,pre_nms_topn=b)",
        "Bug fix": [
            "@@ -312,6 +312,22 @@ class GenerateBoundingBoxProposals : public tensorflow::OpKernel {\n     const auto bbox_deltas = context->input(1);\n     const auto image_info = context->input(2);\n     const auto anchors = context->input(3);\n+\n+    OP_REQUIRES(context, scores.dims() == 4,\n+                errors::InvalidArgument(\"`scores` must be rank 4 but is rank \",\n+                                        scores.dims()));\n+    OP_REQUIRES(\n+        context, bbox_deltas.dims() == 4,\n+        errors::InvalidArgument(\"`bbox_deltas` must be rank 4 but is rank \",\n+                                bbox_deltas.dims()));\n+    OP_REQUIRES(\n+        context, image_info.dims() == 2,\n+        errors::InvalidArgument(\"`image_info` must be rank 2 but is rank \",\n+                                image_info.dims()));\n+    OP_REQUIRES(context, anchors.dims() == 3,\n+                errors::InvalidArgument(\"`anchors` must be rank 3 but is rank \",\n+                                        anchors.dims()));\n+\n     const auto num_images = scores.dim_size(0);\n     const auto num_anchors = scores.dim_size(3);\n     const auto height = scores.dim_size(1);\n",
            "@@ -96,7 +96,7 @@ tf_py_test(\n     ],\n )\n \n-tf_py_test(\n+cuda_py_test(\n     name = \"draw_bounding_box_op_test\",\n     size = \"small\",\n     srcs = [\"draw_bounding_box_op_test.py\"],\n",
            "@@ -16,8 +16,11 @@\n \n import numpy as np\n \n+from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n+from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import image_ops\n from tensorflow.python.ops import image_ops_impl\n@@ -131,6 +134,22 @@ class DrawBoundingBoxOpTest(test.TestCase):\n     self._testDrawBoundingBoxColorCycling(\n         image, dtype=dtypes.half, colors=colors)\n \n+  # generate_bound_box_proposals is only available on GPU.\n+  @test_util.run_gpu_only()\n+  def testGenerateBoundingBoxProposals(self):\n+    # Op only exists on GPU.\n+    with self.cached_session(use_gpu=True):\n+      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                  \"must be rank 4\"):\n+        scores = constant_op.constant(\n+            value=[[[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]]])\n+        self.evaluate(\n+            image_ops.generate_bounding_box_proposals(\n+                scores=scores,\n+                bbox_deltas=[],\n+                image_info=[],\n+                anchors=[],\n+                pre_nms_topn=1))\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          FPE in `tf.image.generate_bounding_box_proposals`\n        "
    },
    {
        "Bug description": "tf.keras.losses.poisson  receives a  y_pred  and  y_true  that are passed through  functor::mul  in  BinaryOp . If the resulting dimensions overflow an  int32 , TensorFlow will crash due to a size mismatch during broadcast assignment.",
        "Sample Code": "import tensorflow as tf\n\ntrue_value = tf.reshape(shape=[1, 2500000000], tensor = tf.zeros(dtype=tf.bool, shape=[50000, 50000]))\npred_value = np.array([[[-2]], [[8]]], dtype = np.float64)\n\n)\n\ntf.keras.losses.poisson(y_true=true_value,y_pred=pred_value)",
        "Bug fix": [
            "@@ -450,13 +450,15 @@ struct BinaryFunctor<CPUDevice, Functor, 2, false> {\n     Assign(d, out, in.unaryExpr(Unary(scalar.data())));\n   }\n \n-  inline Eigen::IndexList<int, Eigen::type2index<1>> NByOne(int n) {\n-    Eigen::IndexList<int, Eigen::type2index<1>> ret;\n+  inline Eigen::IndexList<Eigen::DenseIndex, Eigen::type2index<1>> NByOne(\n+      Eigen::DenseIndex n) {\n+    Eigen::IndexList<Eigen::DenseIndex, Eigen::type2index<1>> ret;\n     ret.set(0, n);\n     return ret;\n   }\n-  inline Eigen::IndexList<Eigen::type2index<1>, int> OneByM(int m) {\n-    Eigen::IndexList<Eigen::type2index<1>, int> ret;\n+  inline Eigen::IndexList<Eigen::type2index<1>, Eigen::DenseIndex> OneByM(\n+      Eigen::DenseIndex m) {\n+    Eigen::IndexList<Eigen::type2index<1>, Eigen::DenseIndex> ret;\n     ret.set(1, m);\n     return ret;\n   }\n@@ -487,10 +489,10 @@ struct BinaryFunctor<CPUDevice, Functor, 2, false> {\n       // use_broadcast_optimization<T> are compile-time constant, gcc\n       // does a decent job avoiding generating code when conditions\n       // are not met.\n-      const int a = in0.dimension(0);  // in0 is shape [a, b]\n-      const int b = in0.dimension(1);\n-      const int c = in1.dimension(0);  // in1 is shape [c, d]\n-      const int d = in1.dimension(1);\n+      const Eigen::DenseIndex a = in0.dimension(0);  // in0 is shape [a, b]\n+      const Eigen::DenseIndex b = in0.dimension(1);\n+      const Eigen::DenseIndex c = in1.dimension(0);  // in1 is shape [c, d]\n+      const Eigen::DenseIndex d = in1.dimension(1);\n       if ((a == 1) && (d == 1)) {\n         auto lhs = in0.reshape(OneByM(b)).broadcast(NByOne(c));\n         auto rhs = in1.reshape(NByOne(c)).broadcast(OneByM(b));\n"
        ],
        "Title": "\n          Overflow in `tf.keras.losses.poisson`\n        "
    },
    {
        "Bug description": "When  tf.raw_ops.ImageProjectiveTransformV2  is given a large output shape, it overflows.",
        "Sample Code": "interpolation = \"BILINEAR\"\nfill_mode = \"REFLECT\"\nimages = tf.constant(0.184634328, shape=[2,5,8,3], dtype=tf.float32)\ntransforms = tf.constant(0.378575385, shape=[2,8], dtype=tf.float32)\noutput_shape = tf.constant([1879048192,1879048192], shape=[2], dtype=tf.int32)\n)\ntf.raw_ops.ImageProjectiveTransformV2(images=images, transforms=transforms, output_shape=output_shape, interpolation=interpolation, fill_mode=fill_mode)",
        "Bug fix": [
            "@@ -96,11 +96,12 @@ void DoImageProjectiveTransformOp(OpKernelContext* ctx,\n   }\n \n   Tensor* output_t;\n+  TensorShape output_shape;\n   OP_REQUIRES_OK(\n-      ctx, ctx->allocate_output(0,\n-                                TensorShape({images_t.dim_size(0), out_height,\n-                                             out_width, images_t.dim_size(3)}),\n-                                &output_t));\n+      ctx, TensorShape::BuildTensorShape({images_t.dim_size(0), out_height,\n+                                          out_width, images_t.dim_size(3)},\n+                                         &output_shape));\n+  OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &output_t));\n   auto output = output_t->tensor<T, 4>();\n   auto images = images_t.tensor<T, 4>();\n   auto transform = transform_t.matrix<float>();\n",
            "@@ -2335,6 +2335,29 @@ class PadToBoundingBoxTest(test_util.TensorFlowTestCase,\n         self.evaluate(v)\n \n \n+class ImageProjectiveTransformV2(test_util.TensorFlowTestCase):\n+\n+  def testShapeTooLarge(self):\n+    interpolation = \"BILINEAR\"\n+    fill_mode = \"REFLECT\"\n+    images = constant_op.constant(\n+        0.184634328, shape=[2, 5, 8, 3], dtype=dtypes.float32)\n+    transforms = constant_op.constant(\n+        0.378575385, shape=[2, 8], dtype=dtypes.float32)\n+    output_shape = constant_op.constant([1879048192, 1879048192],\n+                                        shape=[2],\n+                                        dtype=dtypes.int32)\n+    with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                r\"Encountered overflow when multiplying\"):\n+      self.evaluate(\n+          gen_image_ops.ImageProjectiveTransformV2(\n+              images=images,\n+              transforms=transforms,\n+              output_shape=output_shape,\n+              interpolation=interpolation,\n+              fill_mode=fill_mode))\n+\n+\n class InternalPadToBoundingBoxTest(test_util.TensorFlowTestCase,\n                                    parameterized.TestCase):\n \n"
        ],
        "Title": "\n          Overflow in `ImageProjectiveTransformV2`\n        "
    },
    {
        "Bug description": "When  tf.raw_ops.FusedResizeAndPadConv2D  is given a large tensor shape, it overflows.",
        "Sample Code": "mode = \"REFLECT\"\nstrides = [1, 1, 1, 1]\npadding = \"SAME\"\nresize_align_corners = False\ninput = tf.constant(147, shape=[3,3,1,1], dtype=tf.float16)\nsize = tf.constant([1879048192,1879048192], shape=[2], dtype=tf.int32)\npaddings = tf.constant([3,4], shape=[2], dtype=tf.int32)\nfilter = tf.constant(123, shape=[1,3,4,1], dtype=tf.float16)\n)\ntf.raw_ops.FusedResizeAndPadConv2D(input=input, size=size, paddings=paddings, filter=filter, mode=mode, strides=strides, padding=padding, resize_align_corners=resize_align_corners)",
        "Bug fix": [
            "@@ -667,8 +667,11 @@ class FusedResizeConv2DUsingGemmOp : public OpKernel {\n       st.height_scale = 1.0f;\n       st.width_scale = 1.0f;\n     }\n-    TensorShape resized_shape(\n-        {input.dim_size(0), st.out_height, st.out_width, input.dim_size(3)});\n+    TensorShape resized_shape;\n+    OP_REQUIRES_OK(context, TensorShape::BuildTensorShape(\n+                                {input.dim_size(0), st.out_height, st.out_width,\n+                                 input.dim_size(3)},\n+                                &resized_shape));\n     int paddings_index;\n     int filter_index;\n     if (DoResize) {\n",
            "@@ -581,7 +581,7 @@ REGISTER_OP(\"FusedResizeAndPadConv2D\")\n     .Attr(\"strides: list(int)\")\n     .Attr(GetPaddingAttrString())\n     .SetShapeFn([](InferenceContext* c) {\n-      return CommonFusedConvCalculations(c, true /* has_resize */);\n+      return CommonFusedConvCalculations(c, /*has_resize=*/true);\n     });\n \n REGISTER_OP(\"FusedPadConv2D\")\n@@ -594,7 +594,7 @@ REGISTER_OP(\"FusedPadConv2D\")\n     .Attr(\"strides: list(int)\")\n     .Attr(GetPaddingAttrString())\n     .SetShapeFn([](InferenceContext* c) {\n-      return CommonFusedConvCalculations(c, false /* has_resize */);\n+      return CommonFusedConvCalculations(c, /*has_resize=*/false);\n     });\n \n // --------------------------------------------------------------------------\n",
            "@@ -3429,6 +3429,33 @@ class FusedConv2DTest(test.TestCase):\n         np.rint(expected_output),\n         self.evaluate(add).reshape(-1))\n \n+  # Fused resize and pad conv.\n+  @test_util.run_in_graph_and_eager_modes()\n+  def testResizeAndPadLargeResize(self):\n+    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n+                                \"Encountered overflow\"):\n+      mode = \"REFLECT\"\n+      strides = [1, 1, 1, 1]\n+      padding = \"SAME\"\n+      resize_align_corners = False\n+      tensor = constant_op.constant(\n+          147, shape=[3, 3, 1, 4], dtype=dtypes.float32)\n+      size = constant_op.constant([1879048192, 1879048192], dtype=dtypes.int32)\n+      paddings = constant_op.constant([[0, 0], [0, 0], [0, 0], [0, 0]],\n+                                      dtype=dtypes.int32)\n+      kernel = constant_op.constant(\n+          123, shape=[1, 3, 4, 1], dtype=dtypes.float32)\n+      self.evaluate(\n+          gen_nn_ops.fused_resize_and_pad_conv2d(\n+              input=tensor,\n+              size=size,\n+              paddings=paddings,\n+              filter=kernel,\n+              mode=mode,\n+              strides=strides,\n+              padding=padding,\n+              resize_align_corners=resize_align_corners))\n+\n \n if __name__ == \"__main__\":\n   for index, (input_size_, filter_size_, output_size_, stride_,\n"
        ],
        "Title": "\n          Overflow in `FusedResizeAndPadConv2D`\n        "
    },
    {
        "Bug description": "If a numpy array is created with a shape such that one element is zero and the others sum to a large number, an error will be raised. E.g. the following raises an error:",
        "Sample Code": "import tensorflow as tf\n\ninput_val = tf.constant([1])\nshape_val = np.array([i for i in range(21)])\n\n)])\n\ntf.broadcast_to(input=input_val,shape=shape_val)",
        "Bug fix": [
            "@@ -515,6 +515,16 @@ class TFETensorTest(test_util.TensorFlowTestCase):\n         \"can have at most 32 dimensions\"):\n       t.numpy()\n \n+  def testNumpyDimsTooBig(self):\n+    # Creating a Numpy array fails in some cases if the product of non-zero\n+    # dimensions is very big, even if the shape also has a zero in it.\n+    t = array_ops.ones((0, 2**31, 2**31))\n+    with self.assertRaisesRegex(\n+        errors.InvalidArgumentError,\n+        r\"Failed to create numpy array from tensor of shape \"\n+        r\"\\[0, 2147483648, 2147483648\\]. Numpy error.*array is too big\"):\n+      t.numpy()\n+\n \n class TFETensorUtilTest(test_util.TensorFlowTestCase):\n \n",
            "@@ -88,6 +88,7 @@ cc_library(\n     deps = [\n         \":bfloat16_lib\",\n         \":numpy_lib\",\n+        \":py_util\",\n         \"//tensorflow/c:c_api_no_xla\",\n         \"//tensorflow/core:lib\",\n         \"//tensorflow/core:protos_all_cc\",\n",
            "@@ -13,8 +13,12 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+// clang-format off\n // Must be included first.\n #include \"tensorflow/python/lib/core/numpy.h\"\n+// clang-format on\n+\n+#include \"tensorflow/python/lib/core/ndarray_tensor_bridge.h\"\n \n #include <vector>\n \n@@ -22,7 +26,7 @@ limitations under the License.\n #include \"tensorflow/core/lib/core/errors.h\"\n #include \"tensorflow/core/platform/mutex.h\"\n #include \"tensorflow/python/lib/core/bfloat16.h\"\n-#include \"tensorflow/python/lib/core/ndarray_tensor_bridge.h\"\n+#include \"tensorflow/python/lib/core/py_util.h\"\n \n namespace tensorflow {\n \n@@ -214,6 +218,20 @@ Status ArrayFromMemory(int dim_size, npy_intp* dims, void* data, DataType dtype,\n   }\n   auto* np_array = reinterpret_cast<PyArrayObject*>(\n       PyArray_SimpleNewFromData(dim_size, dims, type_num, data));\n+  if (np_array == nullptr) {\n+    string shape_str = absl::StrJoin(\n+        absl::Span<npy_intp>{dims, static_cast<size_t>(dim_size)}, \", \");\n+    if (PyErr_Occurred()) {\n+      string exception_str = PyExceptionFetch();\n+      PyErr_Clear();\n+      return errors::InvalidArgument(\n+          \"Failed to create numpy array from tensor of shape [\", shape_str,\n+          \"]. Numpy error: \", exception_str);\n+    }\n+    return errors::Internal(\n+        \"Failed to create numpy array from tensor of shape [\", shape_str, \"]\");\n+  }\n+\n   PyArray_CLEARFLAGS(np_array, NPY_ARRAY_OWNDATA);\n   if (PyType_Ready(&TensorReleaserType) == -1) {\n     return errors::Unknown(\"Python type initialization failed.\");\n"
        ],
        "Title": "\n          Seg fault in `ndarray_tensor_bridge` due to zero and large inputs\n        "
    },
    {
        "Bug description": "tf.raw_ops.DynamicStitch  specifies input sizes when it is  registered .",
        "Sample Code": "# indices = 1*[tf.random.uniform([1,2], dtype=tf.dtypes.int32, maxval=100)]\nindices = [tf.constant([[0, 1]]),]\n\n# data = 2*[tf.random.uniform([1,2], dtype=tf.dtypes.float32, maxval=100)]\ndata = [tf.constant([[5, 6]]), tf.constant([[7, 8]])]\n\ntf.raw_ops.DynamicStitch(\n    indices=indices, \n    , \n    data=data)",
        "Bug fix": [
            "@@ -313,6 +313,10 @@ bool IsHostMemoryArg(const EagerOperation& op, const NodeDef* node_def,\n   const auto& host_memory_args = kernel_def->host_memory_arg();\n   const OpDef& op_def = OpRegistry::Global()->LookUp(op.Name())->op_def;\n   const int arg_id = OpPortIdToArgId(*node_def, op_def.input_arg(), port_id);\n+  // Fail if argument ID not found.\n+  if (arg_id < 0) {\n+    return false;\n+  }\n   return std::find(host_memory_args.begin(), host_memory_args.end(),\n                    op_def.input_arg(arg_id).name()) != host_memory_args.end();\n }\n",
            "@@ -18,6 +18,7 @@ import numpy as np\n \n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import data_flow_ops\n@@ -308,6 +309,29 @@ class ParallelDynamicStitchTest(DynamicStitchTestBase, test.TestCase):\n     for datum, grad in zip(data, self.evaluate(grads[3:])):\n       self.assertAllEqual(7.0 * self.evaluate(datum), grad)\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def testMismatchedDataAndIndexListSizes(self):\n+    indices = [\n+        constant_op.constant([2]),\n+        constant_op.constant([1]),\n+        constant_op.constant([0]),\n+        constant_op.constant([3]),\n+    ]\n+    data = [\n+        constant_op.constant([1.0]),\n+        constant_op.constant([2.0]),\n+        constant_op.constant([3.0]),\n+        constant_op.constant([4.0])\n+    ]\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        \"expected inputs .* do not match|List argument .* must match\"):\n+      self.evaluate(data_flow_ops.dynamic_stitch(indices[0:2], data))\n+\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        \"expected inputs .* do not match|List argument .* must match\"):\n+      self.evaluate(data_flow_ops.dynamic_stitch(indices, data[0:2]))\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          OOB seg fault in `DynamicStitch` due to missing validation\n        "
    },
    {
        "Bug description": "When the  BaseCandidateSamplerOp  function  receives a value in  true_classes  larger than  range_max , a heap oob vuln occurs.",
        "Sample Code": "true_classes=[[0x100000,1]],\n    num_true = 2,\n    num_sampled = 2,\n    unique = False,\n    range_max = 2,\n    seed = 2,\n    ,\n    seed2 = 2)",
        "Bug fix": [
            "@@ -73,6 +73,14 @@ class BaseCandidateSamplerOp : public OpKernel {\n \n     gtl::ArraySlice<int64_t> true_candidate(\n         true_classes.matrix<int64_t>().data(), batch_size * num_true_);\n+\n+    for (const auto& candidate : true_candidate) {\n+      OP_REQUIRES(context, candidate >= 0 && candidate < sampler_->range(),\n+                  errors::InvalidArgument(\"`true_candidate` out of range [\", 0,\n+                                          \", \", sampler_->range(),\n+                                          \"), received \", candidate));\n+    }\n+\n     gtl::MutableArraySlice<int64_t> sampled_candidate(\n         out_sampled_candidates->vec<int64_t>().data(), num_sampled_);\n     gtl::MutableArraySlice<float> true_expected_count(\n",
            "@@ -18,6 +18,7 @@ import numpy as np\n \n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import candidate_sampling_ops\n@@ -127,6 +128,27 @@ class RangeSamplerOpsTest(test.TestCase):\n     # twice very rarely.\n     self.assertLessEqual(num_same, 2)\n \n+  def testCandidateOutOfRange(self):\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"out of range\"):\n+      self.evaluate(\n+          candidate_sampling_ops.log_uniform_candidate_sampler(\n+              true_classes=[[0, 10]],\n+              num_true=2,\n+              num_sampled=1000,\n+              unique=False,\n+              range_max=2))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"out of range\"):\n+      self.evaluate(\n+          candidate_sampling_ops.log_uniform_candidate_sampler(\n+              true_classes=[[0, -10]],\n+              num_true=2,\n+              num_sampled=1000,\n+              unique=False,\n+              range_max=2))\n+\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          ThreadUnsafeUnigramCandidateSampler Heap OOB\n        "
    },
    {
        "Bug description": "When converting transposed convolutions using per-channel weight quantization the converter segfaults and crashes the Python process.",
        "Sample Code": "class QuantConv2DTransposed(tf.keras.layers.Layer):\n    def build(self, input_shape):\n        self.kernel = self.add_weight(\"kernel\", [3, 3, input_shape[-1], 24])\n\n    def call(self, inputs):\n        filters = tf.quantization.fake_quant_with_min_max_vars_per_channel(\n            self.kernel, -3.0 * tf.ones([24]), 3.0 * tf.ones([24]), narrow_range=True\n        )\n        filters = tf.transpose(filters, (0, 1, 3, 2))\n        return tf.nn.conv2d_transpose(inputs, filters, [*inputs.shape[:-1], 24], 1)\n\ninp = tf.keras.Input(shape=(6, 8, 48), batch_size=1)\nx = tf.quantization.fake_quant_with_min_max_vars(inp, -3.0, 3.0, narrow_range=True)\nx = QuantConv2DTransposed()(x)\nx = tf.quantization.fake_quant_with_min_max_vars(x, -3.0, 3.0, narrow_range=True)\n\nmodel = tf.keras.Model(inp, x)\n\nmodel.save(\"/tmp/testing\")\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"/tmp/testing\")\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# terminated by signal SIGSEGV (Address boundary error)\n\ntflite_model = converter.convert()",
        "Bug fix": [
            "@@ -168,12 +168,10 @@ quant::UniformQuantizedPerAxisType ResetAxisAndBroadcast(\n         BroadcastVector<int64_t>(shaped.getDimSize(quant_dim), zero_points)) {\n       return {};\n     }\n-  } else if ((new_shape.size() == shape.size() + 1) && new_shape.back() == 1) {\n-    // This is a trivial shift left, then we shift the quant_dim as well.\n-    if (std::equal(shape.begin(), shape.end(), new_shape.begin()) &&\n-        quant_dim == -1) {\n-      quant_dim = shape.size() + quant_dim;\n-    } else {\n+  } else if ((new_shape.size() == shape.size() + 1) && new_shape.front() == 1) {\n+    // Handle the [A, B, C] -> [1, A, B, C] reshape case.\n+    if (!(std::equal(shape.begin(), shape.end(), new_shape.begin() + 1) &&\n+          quant_dim == new_shape.size() - 1)) {\n       return {};\n     }\n   } else {\n@@ -343,6 +341,10 @@ TypeAttr CastQuantizedTypeAttrFromExpressedType(Builder builder,\n   // Reset the quantization dimensions if it is per-axis.\n   if (auto per_axis =\n           qtype.dyn_cast_or_null<quant::UniformQuantizedPerAxisType>()) {\n+    // For the pass-through ops, we don't know which the dimension will be the\n+    // new quantization dimension. Only if the new quantization dimension can\n+    // be inferred, it is safe to reset the per-axis quantized type.\n+    if (axis == -1) return {};\n     qtype =\n         ResetAxisAndBroadcast(source_type.getShape(), per_axis, target, axis);\n   }\n",
            "@@ -1370,6 +1370,8 @@ void PrepareTFPass::runOnOperation() {\n \n   patterns.add<RemoveIdentity>(ctx);\n   TFL::populateWithGenerated(patterns);\n+  // Remove redundant reshape ops.\n+  TF::ReshapeOp::getCanonicalizationPatterns(patterns, ctx);\n   // TODO(karimnosseir): Split to separate pass probably after\n   // deciding on long term plan for this optimization.\n   // This will allow optimizing any TF_Mul->TF_Conv in the graph\n@@ -1399,6 +1401,8 @@ void PrepareTFPass::runOnOperation() {\n            ConvertRfftToRfft2d, RemoveIdentity>(ctx);\n   phase_2_patterns.add<ConvertTFConv2D, ConvertTFDepthwiseConv2dNative>(\n       ctx, allow_bf16_and_f16_type_legalization_);\n+  // Remove redundant reshape ops.\n+  TF::ReshapeOp::getCanonicalizationPatterns(phase_2_patterns, ctx);\n \n   (void)applyPatternsAndFoldGreedily(func, std::move(phase_2_patterns));\n }\n",
            "@@ -2311,6 +2311,44 @@ class FromSavedModelTest(lite_v2_test_util.ModelTest):\n         list(output_details[0]['shape_signature']),\n         list(model.layers[-1].output_shape))\n \n+  @test_util.run_v2_only\n+  def testKerasConv2DTransposedWithMismatchQuantizedAxes(self):\n+\n+    class QuantConv2DTransposed(tf.keras.layers.Layer):\n+\n+      def build(self, input_shape):\n+        self.kernel = self.add_weight('kernel', [3, 3, input_shape[-1], 24])\n+\n+      def call(self, inputs):\n+        filters = tf.quantization.fake_quant_with_min_max_vars_per_channel(\n+            self.kernel,\n+            -3.0 * tf.ones([24]),\n+            3.0 * tf.ones([24]),\n+            narrow_range=True)\n+        filters = tf.transpose(filters, (0, 1, 3, 2))\n+        return tf.nn.conv2d_transpose(inputs, filters, [*inputs.shape[:-1], 24],\n+                                      1)\n+\n+    inp = tf.keras.Input(shape=(6, 8, 48), batch_size=1)\n+    x = tf.quantization.fake_quant_with_min_max_vars(\n+        inp, -3.0, 3.0, narrow_range=True)\n+    x = QuantConv2DTransposed()(x)\n+    x = tf.quantization.fake_quant_with_min_max_vars(\n+        x, -3.0, 3.0, narrow_range=True)\n+\n+    model = tf.keras.Model(inp, x)\n+\n+    saved_model_dir = os.path.join(self.get_temp_dir(),\n+                                   'keras_conv2d_transpose')\n+    model.save(saved_model_dir)\n+    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n+    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n+\n+    with self.assertRaises(convert.ConverterError) as error:\n+      _ = converter.convert()\n+    self.assertIn('mismatched quantized axes of input and output',\n+                  str(error.exception))\n+\n   def _createModelWithInputShape(self, shape):\n     \"\"\"Create a simple SavedModel with a certain shape.\"\"\"\n     saved_model_dir = os.path.join(self.get_temp_dir(), 'input_shape_model')\n"
        ],
        "Title": "\n          Segfault TFLite converter on per-channel quantized transposed convolutions\n        "
    },
    {
        "Bug description": "If  QuantizeAndDequantizeV3  is given a nonscalar  num_bits  input tensor, it results in a  CHECK  fail that can be used to trigger a denial of service attack.",
        "Sample Code": "signed_input = True\nrange_given = False\nnarrow_range = False\naxis = -1\ninput = tf.constant(-3.5, shape=[1], dtype=tf.float32)\ninput_min = tf.constant(-3.5, shape=[1], dtype=tf.float32)\ninput_max = tf.constant(-3.5, shape=[1], dtype=tf.float32)\nnum_bits = tf.constant([], shape=[0], dtype=tf.int32)\n)\ntf.raw_ops.QuantizeAndDequantizeV3(input=input, input_min=input_min, input_max=input_max, num_bits=num_bits, signed_input=signed_input, range_given=range_given, narrow_range=narrow_range, axis=axis)",
        "Bug fix": [
            "@@ -21,19 +21,23 @@ limitations under the License.\n #define EIGEN_USE_GPU\n #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n \n-#include \"tensorflow/core/kernels/quantize_and_dequantize_op.h\"\n-\n #include \"tensorflow/core/framework/op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/type_traits.h\"\n #include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/quantize_and_dequantize_op.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n \n namespace tensorflow {\n+namespace {\n \n-typedef Eigen::ThreadPoolDevice CPUDevice;\n-typedef Eigen::GpuDevice GPUDevice;\n+using CpuDevice = ::Eigen::ThreadPoolDevice;\n+using GpuDevice = ::Eigen::GpuDevice;\n+using ::tensorflow::errors::InvalidArgument;\n+\n+}  // namespace\n \n // Simulate quantization precision loss in a float tensor by:\n // 1. Quantize the tensor to fixed point numbers, which should match the target\n@@ -49,8 +53,8 @@ class QuantizeAndDequantizeV2Op : public OpKernel {\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"axis\", &axis_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"num_bits\", &num_bits_));\n     OP_REQUIRES(ctx, num_bits_ > 0 && num_bits_ < (signed_input_ ? 62 : 63),\n-                errors::InvalidArgument(\"num_bits is out of range: \", num_bits_,\n-                                        \" with signed_input_ \", signed_input_));\n+                InvalidArgument(\"num_bits is out of range: \", num_bits_,\n+                                \" with signed_input_ \", signed_input_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"range_given\", &range_given_));\n \n     string round_mode_string;\n@@ -58,10 +62,10 @@ class QuantizeAndDequantizeV2Op : public OpKernel {\n     OP_REQUIRES(\n         ctx,\n         (round_mode_string == \"HALF_UP\" || round_mode_string == \"HALF_TO_EVEN\"),\n-        errors::InvalidArgument(\"Round mode string must be \"\n-                                \"'HALF_UP' or \"\n-                                \"'HALF_TO_EVEN', is '\" +\n-                                round_mode_string + \"'\"));\n+        InvalidArgument(\"Round mode string must be \"\n+                        \"'HALF_UP' or \"\n+                        \"'HALF_TO_EVEN', is '\" +\n+                        round_mode_string + \"'\"));\n     if (round_mode_string == \"HALF_UP\") {\n       round_mode_ = ROUND_HALF_UP;\n     } else if (round_mode_string == \"HALF_TO_EVEN\") {\n@@ -72,12 +76,10 @@ class QuantizeAndDequantizeV2Op : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input = ctx->input(0);\n-    OP_REQUIRES(\n-        ctx, axis_ >= -1,\n-        errors::InvalidArgument(\"Axis must be at least -1. Found \", axis_));\n-    OP_REQUIRES(\n-        ctx, (axis_ == -1 || axis_ < input.shape().dims()),\n-        errors::InvalidArgument(\"Shape must be at least rank \", axis_ + 1,\n+    OP_REQUIRES(ctx, axis_ >= -1,\n+                InvalidArgument(\"Axis must be at least -1. Found \", axis_));\n+    OP_REQUIRES(ctx, (axis_ == -1 || axis_ < input.shape().dims()),\n+                InvalidArgument(\"Shape must be at least rank \", axis_ + 1,\n                                 \" but is rank \", input.shape().dims()));\n     const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n     Tensor input_min_tensor;\n@@ -91,21 +93,21 @@ class QuantizeAndDequantizeV2Op : public OpKernel {\n         auto min_val = input_min_tensor.scalar<T>()();\n         auto max_val = input_max_tensor.scalar<T>()();\n         OP_REQUIRES(ctx, min_val <= max_val,\n-                    errors::InvalidArgument(\"Invalid range: input_min \",\n-                                            min_val, \" > input_max \", max_val));\n+                    InvalidArgument(\"Invalid range: input_min \", min_val,\n+                                    \" > input_max \", max_val));\n       } else {\n-        OP_REQUIRES(ctx, input_min_tensor.dim_size(0) == depth,\n-                    errors::InvalidArgument(\n-                        \"input_min_tensor has incorrect size, was \",\n-                        input_min_tensor.dim_size(0), \" expected \", depth,\n-                        \" to match dim \", axis_, \" of the input \",\n-                        input_min_tensor.shape()));\n-        OP_REQUIRES(ctx, input_max_tensor.dim_size(0) == depth,\n-                    errors::InvalidArgument(\n-                        \"input_max_tensor has incorrect size, was \",\n-                        input_max_tensor.dim_size(0), \" expected \", depth,\n-                        \" to match dim \", axis_, \" of the input \",\n-                        input_max_tensor.shape()));\n+        OP_REQUIRES(\n+            ctx, input_min_tensor.dim_size(0) == depth,\n+            InvalidArgument(\"input_min_tensor has incorrect size, was \",\n+                            input_min_tensor.dim_size(0), \" expected \", depth,\n+                            \" to match dim \", axis_, \" of the input \",\n+                            input_min_tensor.shape()));\n+        OP_REQUIRES(\n+            ctx, input_max_tensor.dim_size(0) == depth,\n+            InvalidArgument(\"input_max_tensor has incorrect size, was \",\n+                            input_max_tensor.dim_size(0), \" expected \", depth,\n+                            \" to match dim \", axis_, \" of the input \",\n+                            input_max_tensor.shape()));\n       }\n     } else {\n       auto range_shape = (axis_ == -1) ? TensorShape({}) : TensorShape({depth});\n@@ -158,38 +160,34 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\n     Tensor* input_backprop = nullptr;\n     OP_REQUIRES_OK(ctx,\n                    ctx->allocate_output(0, input.shape(), &input_backprop));\n-    OP_REQUIRES(\n-        ctx, axis_ >= -1,\n-        errors::InvalidArgument(\"Axis must be at least -1. Found \", axis_));\n+    OP_REQUIRES(ctx, axis_ >= -1,\n+                InvalidArgument(\"Axis must be at least -1. Found \", axis_));\n     OP_REQUIRES(ctx, (axis_ == -1 || axis_ < input.shape().dims()),\n-                errors::InvalidArgument(\n+                InvalidArgument(\n                     \"Axis should be -1 or 0 or a positive value less than \",\n                     input.shape().dims(), \"but given axis value was \", axis_));\n \n-    OP_REQUIRES(\n-        ctx, input.IsSameSize(gradient),\n-        errors::InvalidArgument(\"gradient and input must be the same size\"));\n+    OP_REQUIRES(ctx, input.IsSameSize(gradient),\n+                InvalidArgument(\"gradient and input must be the same size\"));\n     const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n     const Tensor& input_min_tensor = ctx->input(2);\n     OP_REQUIRES(ctx,\n                 input_min_tensor.dims() == 0 || input_min_tensor.dims() == 1,\n-                errors::InvalidArgument(\n+                InvalidArgument(\n                     \"Input min tensor must have dimension 0 or 1. Received \",\n                     input_min_tensor.dims(), \".\"));\n     const Tensor& input_max_tensor = ctx->input(3);\n     OP_REQUIRES(ctx,\n                 input_max_tensor.dims() == 0 || input_max_tensor.dims() == 1,\n-                errors::InvalidArgument(\n+                InvalidArgument(\n                     \"Input max tensor must have dimension 0 or 1. Received \",\n                     input_max_tensor.dims(), \".\"));\n     if (axis_ != -1) {\n-      OP_REQUIRES(\n-          ctx, input_min_tensor.dim_size(0) == depth,\n-          errors::InvalidArgument(\"min has incorrect size, expected \", depth,\n+      OP_REQUIRES(ctx, input_min_tensor.dim_size(0) == depth,\n+                  InvalidArgument(\"min has incorrect size, expected \", depth,\n                                   \" was \", input_min_tensor.dim_size(0)));\n-      OP_REQUIRES(\n-          ctx, input_max_tensor.dim_size(0) == depth,\n-          errors::InvalidArgument(\"max has incorrect size, expected \", depth,\n+      OP_REQUIRES(ctx, input_max_tensor.dim_size(0) == depth,\n+                  InvalidArgument(\"max has incorrect size, expected \", depth,\n                                   \" was \", input_max_tensor.dim_size(0)));\n     }\n \n@@ -203,12 +201,12 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\n                    ctx->allocate_output(2, min_max_shape, &input_max_backprop));\n \n     if (axis_ == -1) {\n-      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_min_tensor.shape()),\n-                  errors::InvalidArgument(\n-                      \"input_min must be a scalar if axis is unspecified\"));\n-      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_max_tensor.shape()),\n-                  errors::InvalidArgument(\n-                      \"input_max must be a scalar if axis is unspecified\"));\n+      OP_REQUIRES(\n+          ctx, TensorShapeUtils::IsScalar(input_min_tensor.shape()),\n+          InvalidArgument(\"input_min must be a scalar if axis is unspecified\"));\n+      OP_REQUIRES(\n+          ctx, TensorShapeUtils::IsScalar(input_max_tensor.shape()),\n+          InvalidArgument(\"input_max must be a scalar if axis is unspecified\"));\n       functor::QuantizeAndDequantizeOneScaleGradientFunctor<Device, T> f;\n       f(ctx->eigen_device<Device>(), gradient.template flat<T>(),\n         input.template flat<T>(), input_min_tensor.scalar<T>(),\n@@ -252,21 +250,25 @@ class QuantizeAndDequantizeV3Op : public OpKernel {\n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input = ctx->input(0);\n     OP_REQUIRES(ctx, axis_ < input.dims(),\n-                errors::InvalidArgument(\n+                InvalidArgument(\n                     \"Axis requested is larger than input dimensions. Axis: \",\n                     axis_, \" Input Dimensions: \", input.dims()));\n     const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n \n-    Tensor num_bits_tensor;\n-    num_bits_tensor = ctx->input(3);\n-    int num_bits_val = num_bits_tensor.scalar<int32>()();\n+    // Get num_bits and validate.\n+    const Tensor num_bits_tensor = ctx->input(3);\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(num_bits_tensor.shape()),\n+                InvalidArgument(\"Invalid shape. The `num_bits` tensor should \"\n+                                \"be a scalar. Got dimensions: \",\n+                                num_bits_tensor.dims()));\n \n-    OP_REQUIRES(\n-        ctx, num_bits_val > 0 && num_bits_val < (signed_input_ ? 62 : 63),\n-        errors::InvalidArgument(\"num_bits is out of range: \", num_bits_val,\n-                                \" with signed_input_ \", signed_input_));\n+    const int num_bits_val = num_bits_tensor.scalar<int32>()();\n+    OP_REQUIRES(ctx,\n+                num_bits_val > 0 && num_bits_val < (signed_input_ ? 62 : 63),\n+                InvalidArgument(\"num_bits is out of range: \", num_bits_val,\n+                                \" with `signed_input_` \", signed_input_));\n \n     Tensor input_min_tensor;\n     Tensor input_max_tensor;\n@@ -274,24 +276,24 @@ class QuantizeAndDequantizeV3Op : public OpKernel {\n       input_min_tensor = ctx->input(1);\n       input_max_tensor = ctx->input(2);\n       if (axis_ == -1) {\n-        auto min_val = input_min_tensor.scalar<T>()();\n-        auto max_val = input_max_tensor.scalar<T>()();\n+        const auto min_val = input_min_tensor.scalar<T>()();\n+        const auto max_val = input_max_tensor.scalar<T>()();\n         OP_REQUIRES(ctx, min_val <= max_val,\n-                    errors::InvalidArgument(\"Invalid range: input_min \",\n-                                            min_val, \" > input_max \", max_val));\n+                    InvalidArgument(\"Invalid range: input_min \", min_val,\n+                                    \" > input_max \", max_val));\n       } else {\n-        OP_REQUIRES(ctx, input_min_tensor.dim_size(0) == depth,\n-                    errors::InvalidArgument(\n-                        \"input_min_tensor has incorrect size, was \",\n-                        input_min_tensor.dim_size(0), \" expected \", depth,\n-                        \" to match dim \", axis_, \" of the input \",\n-                        input_min_tensor.shape()));\n-        OP_REQUIRES(ctx, input_max_tensor.dim_size(0) == depth,\n-                    errors::InvalidArgument(\n-                        \"input_max_tensor has incorrect size, was \",\n-                        input_max_tensor.dim_size(0), \" expected \", depth,\n-                        \" to match dim \", axis_, \" of the input \",\n-                        input_max_tensor.shape()));\n+        OP_REQUIRES(\n+            ctx, input_min_tensor.dim_size(0) == depth,\n+            InvalidArgument(\"input_min_tensor has incorrect size, was \",\n+                            input_min_tensor.dim_size(0), \" expected \", depth,\n+                            \" to match dim \", axis_, \" of the input \",\n+                            input_min_tensor.shape()));\n+        OP_REQUIRES(\n+            ctx, input_max_tensor.dim_size(0) == depth,\n+            InvalidArgument(\"input_max_tensor has incorrect size, was \",\n+                            input_max_tensor.dim_size(0), \" expected \", depth,\n+                            \" to match dim \", axis_, \" of the input \",\n+                            input_max_tensor.shape()));\n       }\n     } else {\n       auto range_shape = (axis_ == -1) ? TensorShape({}) : TensorShape({depth});\n@@ -331,15 +333,14 @@ class QuantizeAndDequantizeOp : public OpKernel {\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"signed_input\", &signed_input_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"num_bits\", &num_bits_));\n     OP_REQUIRES(ctx, num_bits_ > 0 && num_bits_ < (signed_input_ ? 62 : 63),\n-                errors::InvalidArgument(\"num_bits is out of range: \", num_bits_,\n-                                        \" with signed_input_ \", signed_input_));\n+                InvalidArgument(\"num_bits is out of range: \", num_bits_,\n+                                \" with signed_input_ \", signed_input_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"range_given\", &range_given_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"input_min\", &input_min_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"input_max\", &input_max_));\n     if (range_given_) {\n-      OP_REQUIRES(\n-          ctx, input_min_ <= input_max_,\n-          errors::InvalidArgument(\"Invalid range: input_min \", input_min_,\n+      OP_REQUIRES(ctx, input_min_ <= input_max_,\n+                  InvalidArgument(\"Invalid range: input_min \", input_min_,\n                                   \" > input_max \", input_max_));\n     }\n   }\n@@ -371,53 +372,53 @@ class QuantizeAndDequantizeOp : public OpKernel {\n   float input_max_;\n };\n \n-// Specializations for CPUDevice.\n+// Specializations for CpuDevice.\n \n namespace functor {\n template <typename T>\n-struct QuantizeAndDequantizeOneScaleFunctor<CPUDevice, T> {\n-  void operator()(const CPUDevice& d, typename TTypes<T>::ConstVec input,\n+struct QuantizeAndDequantizeOneScaleFunctor<CpuDevice, T> {\n+  void operator()(const CpuDevice& d, typename TTypes<T>::ConstVec input,\n                   const bool signed_input, const int num_bits,\n                   const bool range_given, Tensor* input_min_tensor,\n                   Tensor* input_max_tensor, QuantizerRoundMode round_mode,\n                   bool narrow_range, typename TTypes<T>::Vec out) {\n-    QuantizeAndDequantizeOneScaleImpl<CPUDevice, T>::Compute(\n+    QuantizeAndDequantizeOneScaleImpl<CpuDevice, T>::Compute(\n         d, input, signed_input, num_bits, range_given, input_min_tensor,\n         input_max_tensor, round_mode, narrow_range, out);\n   }\n };\n \n template <typename T>\n-struct QuantizeAndDequantizePerChannelFunctor<CPUDevice, T> {\n-  void operator()(const CPUDevice& d, typename TTypes<T, 3>::ConstTensor input,\n+struct QuantizeAndDequantizePerChannelFunctor<CpuDevice, T> {\n+  void operator()(const CpuDevice& d, typename TTypes<T, 3>::ConstTensor input,\n                   bool signed_input, int num_bits, bool range_given,\n                   Tensor* input_min_tensor, Tensor* input_max_tensor,\n                   QuantizerRoundMode round_mode, bool narrow_range,\n                   typename TTypes<T, 3>::Tensor out) {\n-    QuantizeAndDequantizePerChannelImpl<CPUDevice, T>::Compute(\n+    QuantizeAndDequantizePerChannelImpl<CpuDevice, T>::Compute(\n         d, input, signed_input, num_bits, range_given, input_min_tensor,\n         input_max_tensor, round_mode, narrow_range, out);\n   }\n };\n \n template <typename T>\n-struct QuantizeAndDequantizeOneScaleGradientFunctor<CPUDevice, T> {\n-  void operator()(const CPUDevice& d, typename TTypes<T>::ConstFlat gradient,\n+struct QuantizeAndDequantizeOneScaleGradientFunctor<CpuDevice, T> {\n+  void operator()(const CpuDevice& d, typename TTypes<T>::ConstFlat gradient,\n                   typename TTypes<T>::ConstFlat input,\n                   typename TTypes<T>::ConstScalar input_min_tensor,\n                   typename TTypes<T>::ConstScalar input_max_tensor,\n                   typename TTypes<T>::Flat input_backprop,\n                   typename TTypes<T>::Scalar input_min_backprop,\n                   typename TTypes<T>::Scalar input_max_backprop) {\n-    QuantizeAndDequantizeOneScaleGradientImpl<CPUDevice, T>::Compute(\n+    QuantizeAndDequantizeOneScaleGradientImpl<CpuDevice, T>::Compute(\n         d, gradient, input, input_min_tensor, input_max_tensor, input_backprop,\n         input_min_backprop, input_max_backprop);\n   }\n };\n \n template <typename T>\n-struct QuantizeAndDequantizePerChannelGradientFunctor<CPUDevice, T> {\n-  void operator()(const CPUDevice& d,\n+struct QuantizeAndDequantizePerChannelGradientFunctor<CpuDevice, T> {\n+  void operator()(const CpuDevice& d,\n                   typename TTypes<T, 3>::ConstTensor gradient,\n                   typename TTypes<T, 3>::ConstTensor input,\n                   const Tensor* input_min_tensor,\n@@ -425,16 +426,16 @@ struct QuantizeAndDequantizePerChannelGradientFunctor<CPUDevice, T> {\n                   typename TTypes<T, 3>::Tensor input_backprop,\n                   typename TTypes<T>::Flat input_min_backprop,\n                   typename TTypes<T>::Flat input_max_backprop) {\n-    QuantizeAndDequantizePerChannelGradientImpl<CPUDevice, T>::Compute(\n+    QuantizeAndDequantizePerChannelGradientImpl<CpuDevice, T>::Compute(\n         d, gradient, input, input_min_tensor, input_max_tensor, input_backprop,\n         input_min_backprop, input_max_backprop);\n   }\n };\n \n-template struct functor::QuantizeAndDequantizeOneScaleGradientFunctor<CPUDevice,\n+template struct functor::QuantizeAndDequantizeOneScaleGradientFunctor<CpuDevice,\n                                                                       float>;\n template struct functor::QuantizeAndDequantizePerChannelGradientFunctor<\n-    CPUDevice, double>;\n+    CpuDevice, double>;\n \n }  // namespace functor\n \n@@ -442,22 +443,22 @@ template struct functor::QuantizeAndDequantizePerChannelGradientFunctor<\n   REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV2\")                      \\\n                               .Device(DEVICE_CPU)                              \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV2Op<CPUDevice, T>);            \\\n+                          QuantizeAndDequantizeV2Op<CpuDevice, T>);            \\\n   REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV3\")                      \\\n                               .Device(DEVICE_CPU)                              \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV3Op<CPUDevice, T>);            \\\n+                          QuantizeAndDequantizeV3Op<CpuDevice, T>);            \\\n   REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4\")                      \\\n                               .Device(DEVICE_CPU)                              \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV2Op<CPUDevice, T>);            \\\n+                          QuantizeAndDequantizeV2Op<CpuDevice, T>);            \\\n   REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4Grad\")                  \\\n                               .Device(DEVICE_CPU)                              \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV4GradientOp<CPUDevice, T>);    \\\n+                          QuantizeAndDequantizeV4GradientOp<CpuDevice, T>);    \\\n   REGISTER_KERNEL_BUILDER(                                                     \\\n       Name(\"QuantizeAndDequantize\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n-      QuantizeAndDequantizeOp<CPUDevice, T>);\n+      QuantizeAndDequantizeOp<CpuDevice, T>);\n TF_CALL_float(REGISTER_CPU_KERNEL);\n TF_CALL_double(REGISTER_CPU_KERNEL);\n #undef REGISTER_CPU_KERNEL\n@@ -470,29 +471,29 @@ TF_CALL_double(REGISTER_CPU_KERNEL);\n                               .HostMemory(\"input_min\")                         \\\n                               .HostMemory(\"input_max\")                         \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV2Op<GPUDevice, T>);            \\\n+                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\\n   REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV3\")                      \\\n                               .Device(DEVICE_GPU)                              \\\n                               .HostMemory(\"input_min\")                         \\\n                               .HostMemory(\"input_max\")                         \\\n                               .HostMemory(\"num_bits\")                          \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV3Op<GPUDevice, T>);            \\\n+                          QuantizeAndDequantizeV3Op<GpuDevice, T>);            \\\n   REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4\")                      \\\n                               .Device(DEVICE_GPU)                              \\\n                               .HostMemory(\"input_min\")                         \\\n                               .HostMemory(\"input_max\")                         \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV2Op<GPUDevice, T>);            \\\n+                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\\n   REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4Grad\")                  \\\n                               .Device(DEVICE_GPU)                              \\\n                               .HostMemory(\"input_min\")                         \\\n                               .HostMemory(\"input_max\")                         \\\n                               .TypeConstraint<T>(\"T\"),                         \\\n-                          QuantizeAndDequantizeV4GradientOp<GPUDevice, T>);    \\\n+                          QuantizeAndDequantizeV4GradientOp<GpuDevice, T>);    \\\n   REGISTER_KERNEL_BUILDER(                                                     \\\n       Name(\"QuantizeAndDequantize\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\\n-      QuantizeAndDequantizeOp<GPUDevice, T>);\n+      QuantizeAndDequantizeOp<GpuDevice, T>);\n TF_CALL_float(REGISTER_GPU_KERNEL);\n TF_CALL_double(REGISTER_GPU_KERNEL);\n #undef REGISTER_GPU_KERNEL\n",
            "@@ -15,9 +15,11 @@\n \"\"\"Tests for tf.quantize ops.\"\"\"\n import numpy as np\n \n+from tensorflow.python.eager import context\n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n from tensorflow.python.framework import errors\n+from tensorflow.python.framework import ops\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import math_ops\n@@ -407,5 +409,59 @@ class QuantizeDownAndShrinkRangeOpTest(test_util.TensorFlowTestCase):\n               out_type=dtypes.quint8))\n \n \n+class QuantizeAndDequantizeV3OpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_valid(self):\n+    with ops.Graph().as_default(), context.eager_mode():\n+      input_value = constant_op.constant([-0.8, -0.5, 0, 0.3, 0.8, -2.0],\n+                                         shape=(6,),\n+                                         dtype=dtypes.float32),\n+      input_min = constant_op.constant(-127, shape=(), dtype=dtypes.float32)\n+      input_max = constant_op.constant(127, shape=(), dtype=dtypes.float32)\n+      num_bits = constant_op.constant(8, shape=(), dtype=dtypes.int32)\n+\n+      quantized = array_ops.quantize_and_dequantize_v3(\n+          input_value,\n+          input_min,\n+          input_max,\n+          num_bits,\n+          signed_input=True,\n+          range_given=False)\n+      self.assertSequenceAlmostEqual(\n+          input_value[0].numpy(), quantized.numpy()[0], delta=0.05)\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    input_value = constant_op.constant([-0.8, -0.5, 0, 0.3, 0.8, -2.0],\n+                                       shape=(6,),\n+                                       dtype=dtypes.float32),\n+    input_min = constant_op.constant(-127, shape=(), dtype=dtypes.float32)\n+    input_max = constant_op.constant(127, shape=(), dtype=dtypes.float32)\n+    # Tensor with invalid shape and invalid number of elements.\n+    num_bits = constant_op.constant([], shape=(0,), dtype=dtypes.int32)\n+\n+    # Test that running the op raises error. It raises different errors\n+    # depending on whether the shape inference is run first or the op's\n+    # Compute() is run first.\n+    try:\n+      array_ops.quantize_and_dequantize_v3(\n+          input_value, input_min, input_max, num_bits, signed_input=True)\n+    except Exception as ex:  # pylint: disable=broad-except\n+      if isinstance(ex, errors.InvalidArgumentError):\n+        self.assertRegex(str(ex), \"The `num_bits` tensor should be a scalar.\")\n+      elif isinstance(ex, ValueError):\n+        self.assertRegex(str(ex), \"Shape must be rank 0\")\n+      else:\n+        self.fail(\n+            \"Raised exception other than expected: %s. \"\n+            \"Expected exceptions are errors.InvalidArgumentError or ValueError\",\n+            ex.__name__)\n+    else:\n+      self.fail(\n+          \"Did not raise an exception where it is expected to raise either \"\n+          \"a ValueError or errors.InvalidArgumentError.\")\n+\n+\n if __name__ == \"__main__\":\n   googletest.main()\n"
        ],
        "Title": "\n          `CHECK` fail in `QuantizeAndDequantizeV3`\n        "
    }
]
[
    {
        "Bug description": "When  tensorflow::full_type::SubstituteFromAttrs  receives a  FullTypeDef& t  that is not exactly three args, it triggers a  CHECK -fail instead of returning a status.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          `CHECK`-fail in `tensorflow::full_type::SubstituteFromAttrs`\n        "
    },
    {
        "Bug description": "When  RangeSize  receives values that do not fit into an  int64_t , it crashes.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Integer overflow in math ops\n        "
    },
    {
        "Bug description": "When  mlir::tfg::TFOp::nameAttr  receives null type list attributes, it crashes.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -699,7 +699,8 @@ StatusOr<unsigned> GraphDefImporter::ArgNumType(const NamedAttrList &attrs,\n                                                 SmallVectorImpl<Type> &types) {\n   // Check whether a type list attribute is specified.\n   if (!arg_def.type_list_attr().empty()) {\n-    if (auto v = attrs.get(arg_def.type_list_attr()).dyn_cast<ArrayAttr>()) {\n+    if (auto v =\n+            attrs.get(arg_def.type_list_attr()).dyn_cast_or_null<ArrayAttr>()) {\n       for (Attribute attr : v) {\n         if (auto dtype = attr.dyn_cast<TypeAttr>()) {\n           types.push_back(UnrankedTensorType::get(dtype.getValue()));\n@@ -716,7 +717,8 @@ StatusOr<unsigned> GraphDefImporter::ArgNumType(const NamedAttrList &attrs,\n   unsigned num = 1;\n   // Check whether a number attribute is specified.\n   if (!arg_def.number_attr().empty()) {\n-    if (auto v = attrs.get(arg_def.number_attr()).dyn_cast<IntegerAttr>()) {\n+    if (auto v =\n+            attrs.get(arg_def.number_attr()).dyn_cast_or_null<IntegerAttr>()) {\n       num = v.getValue().getZExtValue();\n     } else {\n       return NotFound(\"Type attr not found: \", arg_def.number_attr());\n@@ -731,7 +733,7 @@ StatusOr<unsigned> GraphDefImporter::ArgNumType(const NamedAttrList &attrs,\n     return InvalidArgument(\"Arg '\", arg_def.name(),\n                            \"' has invalid type and no type attribute\");\n   } else {\n-    if (auto v = attrs.get(arg_def.type_attr()).dyn_cast<TypeAttr>()) {\n+    if (auto v = attrs.get(arg_def.type_attr()).dyn_cast_or_null<TypeAttr>()) {\n       dtype = v.getValue();\n     } else {\n       return NotFound(\"Type attr not found: \", arg_def.type_attr());\n",
            "@@ -0,0 +1,21 @@\n+# RUN: not tfg-translate -graphdef-to-mlir %s 2>&1 | FileCheck %s\n+\n+# CHECK: Type attr not found\n+\n+library {\n+  function {\n+    signature {\n+      name: \"\\344\\264\\264\"\n+      description: \"value\"\n+      is_distributed_communication: true\n+    }\n+    node_def {\n+      op: \"Const\"\n+      input: \"|\"\n+    }\n+    control_ret {\n+      key: \"\"\n+      value: \"\"\n+    }\n+  }\n+}\n\\ No newline at end of file\n"
        ],
        "Title": "\n          Null-dereference in `mlir::tfg::TFOp::nameAttr`\n        "
    },
    {
        "Bug description": "When  mlir::tfg::GraphDefImporter::ConvertNodeDef  tries to convert NodeDefs without an op name, it crashes.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -746,6 +746,9 @@ StatusOr<unsigned> GraphDefImporter::ArgNumType(const NamedAttrList &attrs,\n Status GraphDefImporter::ConvertNodeDef(OpBuilder &builder, ConversionState &s,\n                                         const NodeDef &node) {\n   VLOG(4) << \"Importing: \" << node.name();\n+  if (node.op().empty())\n+    return InvalidArgument(\"Node \", node.name(), \" has an empty op name\");\n+\n   OperationState state(ConvertLocation(node), absl::StrCat(\"tfg.\", node.op()));\n \n   // The GraphImporter does light shape inference, but here we will defer all of\n",
            "@@ -0,0 +1,20 @@\n+# RUN: not tfg-translate -graphdef-to-mlir %s 2>&1 | FileCheck %s\n+\n+# CHECK: Node  has an empty op name\n+\n+library {\n+  function {\n+    signature {\n+      name: \"\\\\344\\\\264\\\\264\"\n+      description: \"value\"\n+      is_distributed_communication: true\n+    }\n+    node_def {\n+      input: \"|\"\n+    }\n+    control_ret {\n+      key: \"\"\n+      value: \"\"\n+    }\n+  }\n+}\n\\ No newline at end of file\n"
        ],
        "Title": "\n          Null-dereference in `mlir::tfg::GraphDefImporter::ConvertNodeDef`\n        "
    },
    {
        "Bug description": "When  mlir::tfg::ConvertGenericFunctionToFunctionDef  is given empty function attributes, it crashes.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -33,6 +33,7 @@ limitations under the License.\n #include \"tensorflow/core/ir/ops.h\"\n #include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/platform/status.h\"\n+#include \"tensorflow/core/platform/statusor.h\"\n \n using tensorflow::AttrValue;\n using tensorflow::FunctionDef;\n@@ -40,6 +41,7 @@ using tensorflow::NodeDef;\n using tensorflow::OpDef;\n using tensorflow::OpDef_AttrDef;\n using tensorflow::Status;\n+using tensorflow::StatusOr;\n using tensorflow::errors::InvalidArgument;\n using tensorflow::protobuf::RepeatedPtrField;\n \n@@ -166,9 +168,12 @@ Status ImportNodes(ValueMapManager value_manager,\n     if (node.op().empty()) return InvalidArgument(\"empty op type\");\n     OperationState state(unknown_loc, absl::StrCat(\"tfg.\", node.op()));\n     // Fetch the inputs, creating placeholder if an input hasn't been visited.\n-    for (const std::string& input : node.input())\n+    for (const std::string& input : node.input()) {\n+      if (input.empty())\n+        return InvalidArgument(\"Node '\", node.name(), \"' has an empty input\");\n       state.operands.push_back(\n           value_manager.GetValueOrCreatePlaceholder(input));\n+    }\n     // Retrieve the entry in the nodes_map for this node and infer the result\n     // count from what was inferred during the first traversal above.\n     state.types.push_back(placeholder_ty);\n@@ -461,21 +466,31 @@ Status ImportGenericFunction(\n                               Value());\n   for (const auto& ret_val : func.ret()) {\n     auto position = output_name_to_position.find(ret_val.first);\n-    if (position == output_name_to_position.end())\n+    if (position == output_name_to_position.end()) {\n       return InvalidArgument(\n           \"Can't import function, returned value references unknown output \"\n           \"argument \",\n           ret_val.first);\n+    }\n+    if (ret_val.second.empty()) {\n+      return InvalidArgument(\"Function '\", func.signature().name(),\n+                             \"' has empty result name\");\n+    }\n     ret_vals[position->second] =\n         value_manager.GetValueOrCreatePlaceholder(ret_val.second);\n   }\n   for (const auto& ret_val : func.control_ret()) {\n     auto position = control_output_to_position.find(ret_val.first);\n-    if (position == control_output_to_position.end())\n+    if (position == control_output_to_position.end()) {\n       return InvalidArgument(\n           \"Can't import function, returned value references unknown output \"\n           \"argument \",\n           ret_val.first);\n+    }\n+    if (ret_val.second.empty()) {\n+      return InvalidArgument(\"Function '\", func.signature().name(),\n+                             \"' has empty control result name\");\n+    }\n     Value result = value_manager.GetValueOrCreatePlaceholder(\n         (Twine(\"^\") + ret_val.second).str());\n     if (!result.getType().isa<ControlType>())\n",
            "@@ -0,0 +1,26 @@\n+# RUN: not tfg-translate -graphdef-to-mlir %s 2>&1 | FileCheck %s\n+\n+# CHECK: Function 'foo' has empty control result name\n+\n+library {\n+  function {\n+    signature {\n+      name: \"foo\"\n+      control_output: \"output\"\n+    }\n+    node_def {\n+      name: \"y\"\n+      op: \"NoOp\"\n+      attr {\n+        key: \"T\"\n+        value {\n+          placeholder: \"T\"\n+        }\n+      }\n+    }\n+    control_ret {\n+      key: \"output\"\n+      value: \"\"\n+    }\n+  }\n+}\n",
            "@@ -0,0 +1,22 @@\n+# RUN: not tfg-translate -graphdef-to-mlir %s 2>&1 | FileCheck %s\n+\n+# CHECK: Node 'y' has an empty input\n+\n+library {\n+  function {\n+    signature {\n+      name: \"foo\"\n+    }\n+    node_def {\n+      name: \"y\"\n+      input: \"\"\n+      op: \"Identity\"\n+      attr {\n+        key: \"T\"\n+        value {\n+          placeholder: \"T\"\n+        }\n+      }\n+    }\n+  }\n+}\n",
            "@@ -0,0 +1,29 @@\n+# RUN: not tfg-translate -graphdef-to-mlir %s 2>&1 | FileCheck %s\n+\n+# CHECK: Function 'foo' has empty result name\n+\n+library {\n+  function {\n+    signature {\n+      name: \"foo\"\n+      output_arg {\n+        name: \"output\"\n+        type: DT_INT32\n+      }\n+    }\n+    node_def {\n+      name: \"y\"\n+      op: \"NoOp\"\n+      attr {\n+        key: \"T\"\n+        value {\n+          placeholder: \"T\"\n+        }\n+      }\n+    }\n+    ret {\n+      key: \"output\"\n+      value: \"\"\n+    }\n+  }\n+}\n"
        ],
        "Title": "\n          Assertion fail on MLIR empty edge names\n        "
    },
    {
        "Bug description": "When  mlir::tfg::ConvertGenericFunctionToFunctionDef  is given empty function attributes, it gives a null dereference.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -333,6 +333,8 @@ Status ImportGenericFunction(\n   // Import the function attributes with a `tf.` prefix to match the current\n   // infrastructure expectations.\n   for (const auto& namedAttr : func.attr()) {\n+    if (namedAttr.first.empty())\n+      return InvalidArgument(\"Invalid function attribute name\");\n     const std::string& name = \"tf.\" + namedAttr.first;\n     const AttrValue& tf_attr = namedAttr.second;\n     TF_ASSIGN_OR_RETURN(Attribute attr,\n",
            "@@ -0,0 +1,52 @@\n+# RUN: not tfg-translate -graphdef-to-mlir %s 2>&1 | FileCheck %s\n+\n+# CHECK: Invalid function attribute name\n+\n+library {\n+  function {\n+    signature {\n+      name: \"foo\"\n+      input_arg {\n+        name: \"a\"\n+      }\n+      output_arg {\n+        name: \"d\"\n+      }\n+    }\n+    node_def {\n+      op: \"Const\"\n+      attr {\n+        key: \"_b\"\n+        value {\n+          placeholder: \"T\"\n+        }\n+      }\n+      attr {\n+        key: \"dtype\"\n+        value {\n+          type: DT_INT32\n+        }\n+      }\n+      attr {\n+        key: \"value\"\n+        value {\n+          tensor {\n+            dtype: DT_INT32\n+            tensor_shape {\n+            }\n+          }\n+        }\n+      }\n+    }\n+    ret {\n+      key: \"d\"\n+      value: \"a\"\n+    }\n+    attr {\n+      key: \"\"\n+      value {\n+        s: \"a\"\n+      }\n+    }\n+  }\n+}\n"
        ],
        "Title": "\n          Null dereference on MLIR on empty function attributes\n        "
    },
    {
        "Bug description": "When  tf.quantization.fake_quant_with_min_max_vars_gradient  receives input  min  or  max  that is nonscalar, it gives a  CHECK  fail that can trigger a denial of service attack.",
        "Sample Code": "import numpy as np \narg_0=tf.constant(value=np.random.random(size=(2, 2)), shape=(2, 2), dtype=tf.float32)\narg_1=tf.constant(value=np.random.random(size=(2, 2)), shape=(2, 2), dtype=tf.float32)\narg_2=tf.constant(value=np.random.random(size=(2, 2)), shape=(2, 2), dtype=tf.float32)\narg_3=tf.constant(value=np.random.random(size=(2, 2)), shape=(2, 2), dtype=tf.float32)\narg_4=8\narg_5=False\narg_6=''\ntf.quantization.fake_quant_with_min_max_vars_gradient(gradients=arg_0, inputs=arg_1,\n,\nmin=arg_2, max=arg_3, num_bits=arg_4, narrow_range=arg_5, name=arg_6)",
        "Bug fix": [
            "@@ -261,6 +261,12 @@ class FakeQuantWithMinMaxVarsGradientOp : public OpKernel {\n                 InvalidArgument(\"gradient and input must be the same size\"));\n     const Tensor& min = context->input(2);\n     const Tensor& max = context->input(3);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min.shape()),\n+        InvalidArgument(\"`min` must be rank 0 but is rank \", min.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max.shape()),\n+        InvalidArgument(\"`max` must be rank 0 but is rank \", max.dims()));\n \n     Tensor* grad_wrt_input;\n     OP_REQUIRES_OK(context,\n@@ -414,10 +420,16 @@ class FakeQuantWithMinMaxVarsPerChannelGradientOp : public OpKernel {\n                 InvalidArgument(\"gradient and input must be the same size\"));\n     const int depth = input.dim_size(input.dims() - 1);  // last dimension size.\n     const Tensor& min = context->input(2);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(min.shape()),\n+        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));\n     OP_REQUIRES(context, min.dim_size(0) == depth,\n                 InvalidArgument(\"min has incorrect size, expected \", depth,\n                                 \" was \", min.dim_size(0)));\n     const Tensor& max = context->input(3);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(max.shape()),\n+        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));\n     OP_REQUIRES(context, max.dim_size(0) == depth,\n                 InvalidArgument(\"max has incorrect size, expected \", depth,\n                                 \" was \", max.dim_size(0)));\n",
            "@@ -77,6 +77,71 @@ class FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):\n               inputs=inputs, min=[0.0], max=[1.0, 1.1]))\n \n \n+class FakeQuantWithMinMaxVarsGradientOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    gradients = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+    inputs = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be equal rank|must be rank 0\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_gradient(\n+              gradients=gradients,\n+              inputs=inputs,\n+              min=0.0,\n+              max=[[1.0], [2.0], [4.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_gradient(\n+              gradients=gradients,\n+              inputs=inputs,\n+              min=[[1.0], [2.0], [4.0]],\n+              max=[[1.0], [2.0], [4.0]]))\n+\n+\n+class FakeQuantWithMinMaxVarsPerChannelGradientOpTest(\n+    test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    gradients = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+    inputs = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Shapes must be equal rank|must be rank 1\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(\n+              gradients=gradients, inputs=inputs, min=[[0.0]], max=[1.0]))\n+\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        \"Dimension 0 in both shapes must be equal|incorrect size\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(\n+              gradients=gradients, inputs=inputs, min=[0.0, 0.1], max=[1.0]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Shapes must be equal rank|must be rank 1\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(\n+              gradients=gradients, inputs=inputs, min=[1.0], max=[[1.0]]))\n+\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        \"Dimension 0 in both shapes must be equal|incorrect size\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(\n+              gradients=gradients, inputs=inputs, min=[0.0], max=[1.0, 1.1]))\n+\n+\n class QuantizedBiasedAddTest(test_util.TensorFlowTestCase):\n \n   @test_util.run_in_graph_and_eager_modes\n@@ -337,10 +402,9 @@ class QuantizeDownAndShrinkRangeOpTest(test_util.TensorFlowTestCase):\n     with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                 \"must be rank 0\"):\n       self.evaluate(\n-          math_ops.quantize_down_and_shrink_range(input=inputs,\n-                                                  input_min=[],\n-                                                  input_max=4.0,\n-                                                  out_type=dtypes.quint8))\n+          math_ops.quantize_down_and_shrink_range(\n+              input=inputs, input_min=[], input_max=4.0,\n+              out_type=dtypes.quint8))\n \n \n if __name__ == \"__main__\":\n"
        ],
        "Title": "\n          `CHECK` fail in `FakeQuantWithMinMaxVarsGradient`\n        "
    },
    {
        "Bug description": "When  tf.random.gamma  receives large input shape and rates, it gives a  CHECK  fail that can trigger a denial of service attack.",
        "Sample Code": "arg_0=tf.random.uniform(shape=(4,), dtype=tf.int32, maxval=65536)\narg_1=tf.random.uniform(shape=(4, 4), dtype=tf.float64, maxval=None)\narg_2=tf.random.uniform(shape=(4, 4, 4, 4, 4), dtype=tf.float64, maxval=None)\narg_3=tf.float64\narg_4=48\narg_5='None'\n\ntf.random.gamma(shape=arg_0, alpha=arg_1, beta=arg_2, dtype=arg_3, seed=arg_4, name=arg_5)",
        "Bug fix": [
            "@@ -166,7 +166,7 @@ class RandomGammaOp : public OpKernel {\n     }\n     const int64_t samples_per_alpha = samples_shape.num_elements();\n \n-    samples_shape.AppendShape(alpha_t.shape());\n+    OP_REQUIRES_OK(ctx, samples_shape.AppendShapeWithStatus(alpha_t.shape()));\n     // Allocate output samples.\n     Tensor* samples_t = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, samples_shape, &samples_t));\n",
            "@@ -296,8 +296,8 @@ class RandomPoissonOp : public OpKernel {\n     TensorShape samples_shape;\n     OP_REQUIRES_OK(ctx, tensor::MakeShape(shape_t, &samples_shape));\n     const int64_t num_samples = samples_shape.num_elements();\n+    OP_REQUIRES_OK(ctx, samples_shape.AppendShapeWithStatus(rate_t.shape()));\n \n-    samples_shape.AppendShape(rate_t.shape());\n     // Allocate output samples.\n     Tensor* samples_t = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, samples_shape, &samples_t));\n",
            "@@ -16,7 +16,10 @@\n \n import numpy as np\n \n+from tensorflow.python.eager import context\n+from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import random_seed\n from tensorflow.python.framework import test_util\n@@ -216,6 +219,16 @@ class RandomGammaTest(test.TestCase):\n         self.assertEqual(0, math_ops.reduce_sum(math_ops.cast(\n             math_ops.less_equal(x, 0.), dtype=dtypes.int64)).eval())\n \n+  def testSizeTooLarge(self):\n+    # Grappler asserts on size overflow, so this error is only caught when\n+    # running eagerly.\n+    if context.executing_eagerly():\n+      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                  \"overflow\"):\n+        rate = constant_op.constant(1.0, shape=(4, 4, 4, 4, 4))\n+        self.evaluate(\n+            random_ops.random_gamma(\n+                shape=[46902, 51188, 34063, 59195], alpha=rate))\n \n if __name__ == \"__main__\":\n   test.main()\n",
            "@@ -17,6 +17,7 @@ import numpy as np\n \n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import test_util\n from tensorflow.python.kernel_tests.random import util\n@@ -171,6 +172,14 @@ class RandomPoissonTest(test.TestCase):\n     sample = random_ops.random_poisson(shape=[2], lam=np.inf)\n     self.assertAllEqual([np.inf, np.inf], self.evaluate(sample))\n \n+  def testSizeTooLarge(self):\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"overflow\"):\n+      rate = constant_op.constant(1.0, shape=(4, 4, 4, 4, 4))\n+      self.evaluate(\n+          random_ops.random_poisson(\n+              shape=[46902, 51188, 34063, 59195], lam=rate))\n+\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          `CHECK` fail in `tf.random.gamma`\n        "
    },
    {
        "Bug description": "When  RandomPoissonV2  receives large input shape and rates, it gives a  CHECK  fail that can trigger a denial of service attack.",
        "Sample Code": "arg_0=tf.random.uniform(shape=(4,), dtype=tf.int32, maxval=65536)\narg_1=tf.random.uniform(shape=(4, 4, 4, 4, 4), dtype=tf.float32, maxval=None)\narg_2=0\narg_3=0\narg_4=tf.int32\narg_5=None\ntf.raw_ops.RandomPoissonV2(shape=arg_0, rate=arg_1, seed=arg_2,\n                           ,\n                           seed2=arg_3, dtype=arg_4, name=arg_5)",
        "Bug fix": [
            "@@ -166,7 +166,7 @@ class RandomGammaOp : public OpKernel {\n     }\n     const int64_t samples_per_alpha = samples_shape.num_elements();\n \n-    samples_shape.AppendShape(alpha_t.shape());\n+    OP_REQUIRES_OK(ctx, samples_shape.AppendShapeWithStatus(alpha_t.shape()));\n     // Allocate output samples.\n     Tensor* samples_t = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, samples_shape, &samples_t));\n",
            "@@ -296,8 +296,8 @@ class RandomPoissonOp : public OpKernel {\n     TensorShape samples_shape;\n     OP_REQUIRES_OK(ctx, tensor::MakeShape(shape_t, &samples_shape));\n     const int64_t num_samples = samples_shape.num_elements();\n+    OP_REQUIRES_OK(ctx, samples_shape.AppendShapeWithStatus(rate_t.shape()));\n \n-    samples_shape.AppendShape(rate_t.shape());\n     // Allocate output samples.\n     Tensor* samples_t = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, samples_shape, &samples_t));\n",
            "@@ -16,7 +16,10 @@\n \n import numpy as np\n \n+from tensorflow.python.eager import context\n+from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import random_seed\n from tensorflow.python.framework import test_util\n@@ -216,6 +219,16 @@ class RandomGammaTest(test.TestCase):\n         self.assertEqual(0, math_ops.reduce_sum(math_ops.cast(\n             math_ops.less_equal(x, 0.), dtype=dtypes.int64)).eval())\n \n+  def testSizeTooLarge(self):\n+    # Grappler asserts on size overflow, so this error is only caught when\n+    # running eagerly.\n+    if context.executing_eagerly():\n+      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                  \"overflow\"):\n+        rate = constant_op.constant(1.0, shape=(4, 4, 4, 4, 4))\n+        self.evaluate(\n+            random_ops.random_gamma(\n+                shape=[46902, 51188, 34063, 59195], alpha=rate))\n \n if __name__ == \"__main__\":\n   test.main()\n",
            "@@ -17,6 +17,7 @@ import numpy as np\n \n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import test_util\n from tensorflow.python.kernel_tests.random import util\n@@ -171,6 +172,14 @@ class RandomPoissonTest(test.TestCase):\n     sample = random_ops.random_poisson(shape=[2], lam=np.inf)\n     self.assertAllEqual([np.inf, np.inf], self.evaluate(sample))\n \n+  def testSizeTooLarge(self):\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"overflow\"):\n+      rate = constant_op.constant(1.0, shape=(4, 4, 4, 4, 4))\n+      self.evaluate(\n+          random_ops.random_poisson(\n+              shape=[46902, 51188, 34063, 59195], lam=rate))\n+\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          `CHECK` fail in `RandomPoissonV2`\n        "
    },
    {
        "Bug description": "When  Unbatch  receives a nonscalar input  id , it gives a  CHECK  fail that can trigger a denial of service attack.",
        "Sample Code": "import numpy as np\narg_0=tf.constant(value=np.random.random(size=(3, 3, 1)), dtype=tf.float64)\narg_1=tf.constant(value=np.random.randint(0,100,size=(3, 3, 1)), dtype=tf.int64)\narg_2=tf.constant(value=np.random.randint(0,100,size=(3, 3,  1)), dtype=tf.int64)\narg_3=47\narg_4=''\narg_5=''\ntf.raw_ops.Unbatch(batched_tensor=arg_0, batch_index=arg_1, id=arg_2, \n                   , \n                   timeout_micros=arg_3, container=arg_4, shared_name=arg_5)",
        "Bug fix": [
            "@@ -23,6 +23,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/op_requires.h\"\n #include \"tensorflow/core/framework/resource_mgr.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/tensor_util.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/batching_util/adaptive_shared_batch_scheduler.h\"\n@@ -654,6 +655,12 @@ class UnbatchResource : public ResourceBase {\n           batch_index_t.shape().dim_size(1), \".\");\n     }\n \n+    if (!TensorShapeUtils::IsScalar(context->input(2).shape())) {\n+      return errors::InvalidArgument(\n+          \"Input id should be scalar; \"\n+          \"Got: \",\n+          context->input(2).DebugString(), \".\");\n+    }\n     const int64_t batch_key = context->input(2).scalar<int64_t>()();\n     const bool nonempty_input = batch_index_t.dim_size(0) > 0;\n \n",
            "@@ -236,6 +236,26 @@ class BatchOpsTest(test.TestCase):\n       self.assertEqual(thread_results[0], [2])\n       self.assertEqual(main_results[0], [3])\n \n+  def testUnbatchInvalidIdArg(self):\n+    \"\"\"Tests that unbatch work together.\"\"\"\n+    if context.executing_eagerly():\n+      batched_tensor = constant_op.constant(\n+          value=np.random.random(size=(3, 3, 1)), dtype=dtypes.float64)\n+      batched_index = constant_op.constant(\n+          value=np.random.randint(0, 100, size=(3, 3, 1)), dtype=dtypes.int64)\n+      arg_id = constant_op.constant(\n+          value=np.random.randint(0, 100, size=(3, 3, 1)), dtype=dtypes.int64)\n+\n+      with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                  \"Input id should be scalar;\"):\n+        batch_ops.unbatch(\n+            batched_tensor=batched_tensor,\n+            batch_index=batched_index,\n+            id=arg_id,\n+            timeout_micros=50,\n+            container=\"\",\n+            shared_name=\"\")\n+\n   def testBatchDecoratedWithCapturedInput(self):\n     \"\"\"Tests that the batch_function decorator works.\"\"\"\n     if context.executing_eagerly():\n"
        ],
        "Title": "\n          `CHECK` fail in `Unbatch`\n        "
    }
]
[
    {
        "Bug description": "When  DrawBoundingBoxes  receives an input  boxes  that is not of dtype  float , it gives a  CHECK  fail that can trigger a denial of service attack.",
        "Sample Code": "import numpy as np\narg_0=tf.constant(value=np.random.random(size=(1, 3, 2, 3)), shape=(1, 3, 2, 3), dtype=tf.half)\narg_1=tf.constant(value=np.random.random(size=(1, 2, 4)), shape=(1, 2, 4), dtype=tf.float32)\narg_2=''\n\ntf.raw_ops.DrawBoundingBoxes(images=arg_0, boxes=arg_1, name=arg_2)",
        "Bug fix": [
            "@@ -119,7 +119,7 @@ class DrawBoundingBoxesOp : public OpKernel {\n \n     for (int64_t b = 0; b < batch_size; ++b) {\n       const int64_t num_boxes = boxes.dim_size(1);\n-      const auto tboxes = boxes.tensor<T, 3>();\n+      const auto tboxes = boxes.tensor<float, 3>();\n       for (int64_t bb = 0; bb < num_boxes; ++bb) {\n         int64_t color_index = bb % color_table.size();\n         const int64_t min_box_row =\n",
            "@@ -50,11 +50,16 @@ class DrawBoundingBoxOpTest(test.TestCase):\n     image[height - 1, 0:width, 0:depth] = color\n     return image\n \n-  def _testDrawBoundingBoxColorCycling(self, img, colors=None):\n+  def _testDrawBoundingBoxColorCycling(self,\n+                                       img,\n+                                       dtype=dtypes.float32,\n+                                       colors=None):\n     \"\"\"Tests if cycling works appropriately.\n \n     Args:\n       img: 3-D numpy image on which to draw.\n+      dtype: image dtype (float, half).\n+      colors: color table.\n     \"\"\"\n     color_table = colors\n     if colors is None:\n@@ -82,7 +87,7 @@ class DrawBoundingBoxOpTest(test.TestCase):\n       bboxes = math_ops.cast(bboxes, dtypes.float32)\n       bboxes = array_ops.expand_dims(bboxes, 0)\n       image = ops.convert_to_tensor(image)\n-      image = image_ops_impl.convert_image_dtype(image, dtypes.float32)\n+      image = image_ops_impl.convert_image_dtype(image, dtype)\n       image = array_ops.expand_dims(image, 0)\n       image = image_ops.draw_bounding_boxes(image, bboxes, colors=colors)\n       with self.cached_session(use_gpu=False) as sess:\n@@ -118,6 +123,14 @@ class DrawBoundingBoxOpTest(test.TestCase):\n                          [0, 0, 0.5, 1]])\n     self._testDrawBoundingBoxColorCycling(image, colors=colors)\n \n+  def testDrawBoundingBoxHalf(self):\n+    \"\"\"Test if RGBA color cycling works correctly with provided colors.\"\"\"\n+    image = np.zeros([10, 10, 4], \"float32\")\n+    colors = np.asarray([[0.5, 0, 0.5, 1], [0.5, 0.5, 0, 1], [0.5, 0, 0, 1],\n+                         [0, 0, 0.5, 1]])\n+    self._testDrawBoundingBoxColorCycling(\n+        image, dtype=dtypes.half, colors=colors)\n+\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          `CHECK` fail in `DrawBoundingBoxes`\n        "
    },
    {
        "Bug description": "Eig  can be fed an incorrect  Tout  input, resulting in a  CHECK  fail that can trigger a denial of service attack.",
        "Sample Code": "import numpy as np \narg_0=tf.constant(value=np.random.random(size=(2, 2)), shape=(2, 2), dtype=tf.float32)\narg_1=tf.complex128\narg_2=True\narg_3=''\n\ntf.raw_ops.Eig(input=arg_0, Tout=arg_1, compute_v=arg_2, name=arg_3)",
        "Bug fix": [
            "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"tensorflow/core/kernels/linalg/linalg_ops_common.h\"\n \n+#include <initializer_list>\n #include <utility>\n \n #include \"third_party/eigen3/Eigen/Core\"\n@@ -22,7 +23,9 @@ limitations under the License.\n #include \"tensorflow/core/framework/kernel_def_builder.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/platform/logging.h\"\n #include \"tensorflow/core/platform/types.h\"\n \n@@ -152,6 +155,10 @@ void LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs(\n     input_matrix_shapes->emplace_back(\n         std::initializer_list<int64_t>({num_rows, num_cols}));\n     inputs->emplace_back(&in);\n+    OP_REQUIRES(\n+        context, in.dtype() == DataTypeToEnum<InputScalar>::v(),\n+        errors::InvalidArgument(\"Invalid input dtype \", in.dtype(), \" vs \",\n+                                DataTypeToEnum<InputScalar>::v()));\n   }\n   // Have the derived class validate that the inputs are as expected.\n   ValidateInputMatrixShapes(context, *input_matrix_shapes);\n@@ -212,6 +219,11 @@ void LinearAlgebraOp<InputScalar, OutputScalar>::PrepareOutputs(\n       OP_REQUIRES_OK(context, context->allocate_output(\n                                   output_idx, output_tensor_shape, &out));\n     }\n+    OP_REQUIRES(\n+        context, out->dtype() == DataTypeToEnum<OutputScalar>::v(),\n+        errors::InvalidArgument(\"Invalid output dtype \", out->dtype(), \" vs \",\n+                                DataTypeToEnum<OutputScalar>::v()));\n+\n     outputs->emplace_back(out);\n   }\n }\n",
            "@@ -18,8 +18,10 @@ import numpy as np\n \n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes as dtypes_lib\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import gen_linalg_ops\n from tensorflow.python.ops import gradient_checker_v2\n from tensorflow.python.ops import linalg_ops\n from tensorflow.python.ops import math_ops\n@@ -88,6 +90,16 @@ class EigTest(test.TestCase):\n       self.assertAllClose(matrix,\n                           np.matmul(np.matmul(v, np.diag(e)), v.transpose()))\n \n+  def testMismatchedDtypes(self):\n+    tensor = constant_op.constant([[0, 1], [2, 3]], dtype=dtypes_lib.float32)\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Invalid output dtype\"):\n+      self.evaluate(\n+          gen_linalg_ops.eig(\n+              input=tensor,\n+              Tout=dtypes_lib.complex128,  # Expected dtype: complex64.\n+              compute_v=True))\n+\n \n def SortEigenValues(e):\n   perm = np.argsort(e.real + e.imag, -1)\n"
        ],
        "Title": "\n          `CHECK` fail in `Eig`\n        "
    },
    {
        "Bug description": "When  Conv2DBackpropInput  receives empty  out_backprop  inputs (e.g.  [3, 1, 0, 1] ), the current CPU/GPU kernels  CHECK  fail (one with dnnl, the other with cudnn). This can be used to trigger a denial of service attack.",
        "Sample Code": "import numpy as np\ninput_sizes = [3, 1, 1, 2]\nfilter = np.ones([1, 3, 2, 3])\nout_backprop = np.ones([3, 1, 0, 3])\nstrides = [1, 1, 2, 1]\npadding = 'VALID'\n\ntf.raw_ops.Conv2DBackpropInput(\n   input_sizes = input_sizes,\n   filter = filter,\n   out_backprop = out_backprop,\n   strides = strides,\n   padding = padding\n)\n)",
        "Bug fix": [
            "@@ -37,6 +37,7 @@ limitations under the License.\n #include \"tensorflow/core/kernels/conv_2d.h\"\n #include \"tensorflow/core/kernels/conv_grad_ops.h\"\n #include \"tensorflow/core/kernels/conv_grad_shape_utils.h\"\n+#include \"tensorflow/core/kernels/fill_functor.h\"\n #ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS\n #include \"tensorflow/core/kernels/xsmm_conv2d.h\"\n #endif\n@@ -436,6 +437,15 @@ class Conv2DBackpropInputOp : public OpKernel {\n       return;\n     }\n \n+    // If shapes are valid but `out_backprop` is empty, in_backprop should be\n+    // set to all zeros.  Otherwise, cudnn/dnnl fail with an empty input.\n+    if (out_backprop.NumElements() == 0) {\n+      functor::SetZeroFunctor<Device, T> set_zero;\n+      set_zero(context->eigen_device<Device>(),\n+               in_backprop->template flat<T>());\n+      return;\n+    }\n+\n     // For now we take the stride from the second and third dimensions only (we\n     // do not support striding on the batch or depth dimension).\n     const int stride_rows = GetTensorDim(strides_, data_format_, 'H');\n@@ -554,6 +564,15 @@ class Conv2DCustomBackpropInputOp : public OpKernel {\n       return;\n     }\n \n+    // If shapes are valid but `out_backprop` is empty, in_backprop should be\n+    // set to all zeros.  Otherwise, cudnn/dnnl fail with an empty input.\n+    if (out_backprop.NumElements() == 0) {\n+      functor::SetZeroFunctor<Device, T> set_zero;\n+      set_zero(context->eigen_device<Device>(),\n+               in_backprop->template flat<T>());\n+      return;\n+    }\n+\n // TODO(ezhulenev): Remove custom kernel and move XSMM support to\n // LaunchConv2DBackpropInputOp functor.\n #if defined TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS && \\\n",
            "@@ -1103,6 +1103,23 @@ class Conv2DTest(test.TestCase):\n           use_gpu=use_gpu,\n           err=1e-5)\n \n+  @test_util.run_in_graph_and_eager_modes\n+  @test_util.disable_xla(\"b/239598470\")\n+  def testConv2DBackpropInputDegenerateBackpropInput(self):\n+    input_sizes = [3, 1, 1, 2]\n+    expected_output = np.zeros(input_sizes).flatten()\n+    for (data_format, use_gpu) in GetTestConfigs():\n+      self._RunAndVerifyBackpropInput(\n+          input_sizes=input_sizes,\n+          filter_sizes=[1, 3, 2, 3],\n+          output_sizes=[3, 1, 0, 3],\n+          strides=[1, 2],\n+          padding=\"VALID\",\n+          expected=expected_output,\n+          data_format=data_format,\n+          use_gpu=use_gpu,\n+          err=1e-5)\n+\n   # Testing for backprops\n   def _RunAndVerifyBackpropFilter(self,\n                                   input_sizes,\n"
        ],
        "Title": "\n          `CHECK` fail in `Conv2DBackpropInput`\n        "
    },
    {
        "Bug description": "If  EmptyTensorList  receives an input  element_shape  with more than one dimension, it gives a  CHECK  fail that can be used to trigger a denial of service attack.",
        "Sample Code": " tensorflow as tf\n\ntf.raw_ops.EmptyTensorList(element_shape=tf.ones(dtype=tf.int32, shape=[1, 0]), max_num_elements=tf.constant(1),element_dtype=tf.int32)",
        "Bug fix": [
            "@@ -21,7 +21,11 @@ limitations under the License.\n \n #include \"tensorflow/core/kernels/list_kernels.h\"\n \n+#include <algorithm>\n+#include <iterator>\n #include <limits>\n+#include <memory>\n+#include <utility>\n \n #include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n #include \"tensorflow/core/framework/allocator.h\"\n@@ -30,10 +34,6 @@ limitations under the License.\n #include \"tensorflow/core/framework/tensor_types.h\"\n #include \"tensorflow/core/framework/variant.h\"\n #include \"tensorflow/core/framework/variant_op_registry.h\"\n-#include \"tensorflow/core/kernels/concat_lib.h\"\n-#include \"tensorflow/core/lib/core/coding.h\"\n-#include \"tensorflow/core/lib/core/errors.h\"\n-#include \"tensorflow/core/util/util.h\"\n \n namespace tensorflow {\n \n@@ -49,6 +49,9 @@ Status TensorShapeFromTensor(const Tensor& t, PartialTensorShape* out) {\n     return errors::InvalidArgument(\n         \"The only valid scalar shape tensor is the fully unknown shape \"\n         \"specified as -1.\");\n+  } else if (t.shape().dims() != 1) {\n+    return errors::InvalidArgument(\"Shape must be at most rank 1 but is rank \",\n+                                   t.shape().dims());\n   }\n   if (t.dtype() == DT_INT32) {\n     return PartialTensorShape::MakePartialShape(t.vec<int32>().data(),\n",
            "@@ -1458,6 +1458,15 @@ class ListOpsTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n       t = list_ops.tensor_list_concat(l, element_dtype=dtypes.float32)\n       self.evaluate(t)\n \n+  def testEmptyTensorListInvalidShape(self):\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                r\"Shape must be at most rank 1 but is rank 2\"):\n+      t = gen_list_ops.EmptyTensorList(\n+          element_shape=array_ops.ones(dtype=dtypes.int32, shape=[1, 0]),\n+          max_num_elements=constant_op.constant(1),\n+          element_dtype=dtypes.int32)\n+      self.evaluate(t)\n+\n   def testEvenSplit(self):\n \n     def RunTest(input_tensor, lengths, expected_stacked_output):\n"
        ],
        "Title": "\n          `CHECK` fail in `EmptyTensorList`\n        "
    },
    {
        "Bug description": "If  tf.sparse.cross  receives an input  separator  that is not a scalar, it gives a  CHECK  fail that can be used to trigger a denial of service attack.",
        "Sample Code": " tensorflow as tf\n\ntf.sparse.cross(inputs=[],name='a',separator=tf.constant(['a', 'b'],dtype=tf.string))",
        "Bug fix": [
            "@@ -24,12 +24,14 @@ limitations under the License.\n #include \"tensorflow/core/framework/kernel_def_builder.h\"\n #include \"tensorflow/core/framework/op_def_builder.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/op_requires.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/framework/types.pb.h\"\n #include \"tensorflow/core/lib/core/stringpiece.h\"\n #include \"tensorflow/core/lib/strings/str_util.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/platform/fingerprint.h\"\n #include \"tensorflow/core/platform/strong_hash.h\"\n #include \"tensorflow/core/util/work_sharder.h\"\n@@ -832,6 +834,10 @@ class SparseCrossV2Op : public OpKernel {\n \n     const Tensor* sep_t;\n     OP_REQUIRES_OK(context, context->input(\"sep\", &sep_t));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(sep_t->shape()),\n+                errors::InvalidArgument(\"Input separator should be a scalar. \"\n+                                        \"Received: \",\n+                                        sep_t->DebugString()));\n     const tstring separator = sep_t->scalar<tstring>()();\n \n     std::vector<std::unique_ptr<ColumnInterface<tstring>>> columns =\n",
            "@@ -873,6 +873,14 @@ class SparseCrossV2OpTest(BaseSparseCrossOpTest):\n     with self.cached_session():\n       self._assert_sparse_tensor_empty(self.evaluate(out))\n \n+  def testNonScalarInput(self):\n+    with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                'Input separator should be a scalar.'):\n+      self.evaluate(sparse_ops.sparse_cross(\n+          inputs=[],\n+          name='a',\n+          separator=constant_op.constant(['a', 'b'], dtype=dtypes.string)))\n+\n \n class SparseCrossHashedOpTest(BaseSparseCrossOpTest):\n \n"
        ],
        "Title": "\n          `CHECK` fail in `tf.sparse.cross`\n        "
    },
    {
        "Bug description": "If  Conv2D  is given empty  input  and the  filter  and  padding  sizes are valid, the output is all-zeros. This causes division-by-zero floating point exceptions that can be used to trigger a denial of service attack.",
        "Sample Code": "import numpy as np\nwith tf.device(\"CPU\"): # also can be triggerred on GPU\n   input = np.ones([1, 0, 2, 1])\n   filter = np.ones([1, 1, 1, 1])\n   strides = ([1, 1, 1, 1])\n   padding = \"EXPLICIT\"\n   explicit_paddings = [0 , 0, 1, 1, 1, 1, 0, 0]\n   data_format = \"NHWC\"\n   res = tf.raw_ops.Conv2D(\n       input=input,\n       filter=filter,\n       strides=strides,\n       padding=padding,\n        explicit_paddings=explicit_paddings,\n       data_format=data_format,\n  ),\n  )",
        "Bug fix": [
            "@@ -44,6 +44,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/conv_2d.h\"\n #include \"tensorflow/core/kernels/deep_conv2d.h\"\n+#include \"tensorflow/core/kernels/fill_functor.h\"\n #include \"tensorflow/core/kernels/ops_util.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n #include \"tensorflow/core/lib/gtl/array_slice.h\"\n@@ -701,6 +702,15 @@ class Conv2DOp : public BinaryOp<T> {\n       return;\n     }\n \n+    // If the input is empty, result can only be due to padding.\n+    if (input.NumElements() == 0) {\n+      // Zero-out output and return.\n+      functor::SetZeroFunctor<Device, T>()(context->eigen_device<Device>(),\n+                                           output->template flat<T>());\n+\n+      return;\n+    }\n+\n #ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS\n     if (params_.padding != EXPLICIT &&\n         LaunchXsmmConvOp<Device, T>::Run(\n",
            "@@ -759,6 +759,15 @@ class Conv2DTest(test.TestCase):\n         padding=[[2, 1], [1, 2]],\n         dilations=[2, 3])\n \n+  @test_util.run_in_graph_and_eager_modes()\n+  def testConv2dOnlyPaddingReturnsZeros(self):\n+    self._VerifyValues(\n+        tensor_in_sizes=[1, 0, 2, 1],\n+        filter_in_sizes=[1, 1, 1, 1],\n+        strides=[1, 1],\n+        padding=[[1, 1], [1, 1]],\n+        expected=[0, 0, 0, 0, 0, 0, 0, 0])\n+\n   def testConv2DExplicitPaddingWithLayoutOptimizer(self):\n     # Test with Grappler's layout optimizer, to ensure the layout optimizer\n     # handles explicit padding correctly.\n"
        ],
        "Title": "\n          Floating point exception in `Conv2D`\n        "
    },
    {
        "Bug description": "When  AudioSummaryV2  receives an input  sample_rate  with more than one element, it gives a  CHECK  fails that can be used to trigger a denial of service attack.",
        "Sample Code": "arg_0=''\narg_1=tf.random.uniform(shape=(1,1), dtype=tf.float32, maxval=None)\narg_2=tf.random.uniform(shape=(2,1), dtype=tf.float32, maxval=None)\narg_3=3\narg_4=''\ntf.raw_ops.AudioSummaryV2(tag=arg_0, tensor=arg_1, sample_rate=arg_2,\n                          ,\n                          max_outputs=arg_3, name=arg_4)",
        "Bug fix": [
            "@@ -49,6 +49,11 @@ class SummaryAudioOp : public OpKernel {\n     float sample_rate = sample_rate_attr_;\n     if (!has_sample_rate_attr_) {\n       const Tensor& sample_rate_tensor = c->input(2);\n+      OP_REQUIRES(c,\n+                  sample_rate_tensor.IsAligned() &&\n+                      sample_rate_tensor.NumElements() == 1,\n+                  errors::InvalidArgument(\n+                      \"sample_rate must be rank-0 or contain a single value\"));\n       sample_rate = sample_rate_tensor.scalar<float>()();\n     }\n     OP_REQUIRES(c, sample_rate > 0.0f,\n",
            "@@ -23,6 +23,7 @@ tensorflow/python/kernel_tests/summary_v1_*.py.\n from tensorflow.core.framework import summary_pb2\n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import meta_graph\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import test_util\n@@ -183,6 +184,11 @@ class SummaryTest(test.TestCase):\n         'family/outer/family/inner/audio/{}'.format(i) for i in range(3))\n     self.assertEqual(tags, expected)\n \n+  def testAudioSummaryWithInvalidSampleRate(self):\n+    with self.assertRaises(errors.InvalidArgumentError):\n+      invalid_sample_rate = [22000.0, 22000.0]\n+      self.evaluate(summary_lib.audio('', [[1.0]], invalid_sample_rate))\n+\n   @test_util.run_deprecated_v1\n   def testTextSummary(self):\n     with self.cached_session():\n"
        ],
        "Title": "\n          `CHECK` fail in `AudioSummaryV2`\n        "
    },
    {
        "Bug description": "When  CollectiveGather  receives an scalar input  input , it gives a  CHECK  fails that can be used to trigger a denial of service attack.",
        "Sample Code": "arg_0=1\narg_1=1\narg_2=1\narg_3=1\narg_4=(3, 3,3)\narg_5='auto'\narg_6=0\narg_7=''\ntf.raw_ops.CollectiveGather(input=arg_0, group_size=arg_1, group_key=arg_2,\n                            instance_key=arg_3, shape=arg_4,\n                            ,\n                            communication_hint=arg_5, timeout_seconds=arg_6, name=arg_7)",
        "Bug fix": [
            "@@ -176,6 +176,10 @@ class CollectiveGatherOpKernel : public CollectiveOpV1Kernel {\n   void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,\n                         DoneCallback done) override {\n     auto output_shape = c->input(0).shape();\n+    OP_REQUIRES_ASYNC(c, output_shape.dims() > 0,\n+                      errors::InvalidArgument(\"input should have rank > 0, \",\n+                                              \"recieved \", output_shape.dims()),\n+                      done);\n     output_shape.set_dim(\n         0, output_shape.dim_size(0) * col_params_->group.group_size);\n     col_params_->instance.shape = output_shape;\n",
            "@@ -451,6 +451,20 @@ class CollectiveOpTest(test.TestCase):\n     ])\n     context.ensure_initialized()\n \n+  @test_util.run_v2_only\n+  def testCollectiveGatherShapeCheckFailure(self):\n+    with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                'input should have rank > 0'):\n+      collective_ops.gen_collective_ops.CollectiveGather(\n+          input=1,\n+          group_size=1,\n+          group_key=1,\n+          instance_key=1,\n+          shape=(3, 3, 3),\n+          communication_hint='auto',\n+          timeout_seconds=0,\n+          name='')\n+\n     @def_function.function\n     def run_all_reduce():\n       group_key = 10\n"
        ],
        "Title": "\n          `CHECK` fail in `CollectiveGather`\n        "
    },
    {
        "Bug description": "When  SetSize  receives an input  set_shape  that is not a 1D tensor, it gives a  CHECK  fails that can be used to trigger a denial of service attack.",
        "Sample Code": "arg_0=1\narg_1=[1,1]\narg_2=1\narg_3=True\narg_4=''\ntf.raw_ops.SetSize(set_indices=arg_0, set_values=arg_1, set_shape=arg_2,\n                   ,\n                   validate_indices=arg_3, name=arg_4)",
        "Bug fix": [
            "@@ -70,8 +70,12 @@ Status SparseTensorFromContext(OpKernelContext* ctx, const int32_t base_index,\n                                sparse::SparseTensor* tensor) {\n   // Assume row-major order.\n   TensorShape shape;\n-  TF_RETURN_IF_ERROR(TensorShape::BuildTensorShape(\n-      ctx->input(base_index + 2).vec<int64_t>(), &shape));\n+  const Tensor& shape_tensor = ctx->input(base_index + 2);\n+  if (shape_tensor.dims() != 1) {\n+    return errors::InvalidArgument(\"Shape must be a 1D tensor.\");\n+  }\n+  TF_RETURN_IF_ERROR(\n+      TensorShape::BuildTensorShape(shape_tensor.vec<int64_t>(), &shape));\n   CheckRankAtLeast2(ctx, shape);\n   std::vector<int64_t> order(shape.dims());\n   std::iota(order.begin(), order.end(), 0);\n",
            "@@ -23,6 +23,7 @@ from tensorflow.python.framework import errors_impl\n from tensorflow.python.framework import sparse_tensor as sparse_tensor_lib\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import gen_set_ops\n from tensorflow.python.ops import math_ops\n from tensorflow.python.ops import sets\n from tensorflow.python.ops import sparse_ops\n@@ -1303,6 +1304,18 @@ class SetOpsTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n         result.values,\n         _constant([1, 3, 5, 7, 9, 0, 2, 4, 5, 6, 6, 8, 9], dtype))\n \n+  def test_raw_ops_setsize_invalid_shape(self):\n+    with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n+                                \"Shape must be a 1D tensor\"):\n+      invalid_shape = 1\n+      self.evaluate(\n+          gen_set_ops.set_size(\n+              set_indices=1,\n+              set_values=[1, 1],\n+              set_shape=invalid_shape,\n+              validate_indices=True,\n+              name=\"\"))\n+\n \n if __name__ == \"__main__\":\n   googletest.main()\n"
        ],
        "Title": "\n          `CHECK` fail in `SetSize`\n        "
    },
    {
        "Bug description": "When  TensorListFromTensor  receives an  element_shape  of a rank greater than one, it gives a  CHECK  fail that can trigger a denial of service attack.",
        "Sample Code": "arg_0=tf.random.uniform(shape=(6, 6, 2), dtype=tf.bfloat16, maxval=None)\narg_1=tf.random.uniform(shape=(6, 9, 1, 3), dtype=tf.int64, maxval=65536)\narg_2=''\n\ntf.raw_ops.TensorListFromTensor(tensor=arg_0, element_shape=arg_1, name=arg_2)",
        "Bug fix": [
            "@@ -769,6 +769,11 @@ class TensorListFromTensor : public OpKernel {\n     attr.set_on_host(true);\n     OP_REQUIRES_OK(c, c->allocate_output(0, {}, &output_tensor, attr));\n     PartialTensorShape element_shape;\n+    OP_REQUIRES(\n+        c, !TensorShapeUtils::IsMatrixOrHigher(c->input(1).shape()),\n+        errors::InvalidArgument(\n+            \"TensorListFromTensor: element_shape must be at most rank 1 but \",\n+            \"has the shape of \", c->input(1).shape().DebugString()));\n     OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(1), &element_shape));\n     TensorList output_list;\n     const Tensor& t = c->input(0);\n",
            "@@ -584,6 +584,17 @@ class ListOpsTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n     self.assertAllEqual(e, 1.0)\n     self.assertAllEqual(list_ops.tensor_list_length(l), 0)\n \n+  def testTensorListFromTensorFailsWhenElementShapeIsNotVector(self):\n+    t = constant_op.constant([1.0, 2.0])\n+    # In Eager mode, InvalidArgumentError is generated by the Compute function.\n+    # In graph mode, ValueError is generated by the shape function.\n+    with self.assertRaisesRegex(\n+        (errors.InvalidArgumentError, ValueError),\n+        \"must be at most rank 1\"):\n+      # Wrong element_shape. Should be at most rank 1.\n+      l = list_ops.tensor_list_from_tensor(t, element_shape=[[1]])\n+      self.evaluate(l)\n+\n   @test_util.run_gpu_only\n   def testFromTensorGPU(self):\n     with context.device(\"gpu:0\"):\n"
        ],
        "Title": "\n          `CHECK` fail in `TensorListFromTensor`\n        "
    }
]
[
    {
        "Bug description": "When  TensorListScatter  and  TensorListScatterV2  receive an  element_shape  of a rank greater than one, they give a  CHECK  fail that can trigger a denial of service attack.",
        "Sample Code": "arg_0=tf.random.uniform(shape=(2, 2, 2), dtype=tf.float16, maxval=None)\narg_1=tf.random.uniform(shape=(2, 2, 2), dtype=tf.int32, maxval=65536)\narg_2=tf.random.uniform(shape=(2, 2, 2), dtype=tf.int32, maxval=65536)\narg_3=''\ntf.raw_ops.TensorListScatter(tensor=arg_0, indices=arg_1, \n, \nelement_shape=arg_2, name=arg_3)",
        "Bug fix": [
            "@@ -895,6 +895,11 @@ class TensorListScatter : public OpKernel {\n     OP_REQUIRES_OK(c, c->allocate_output(0, {}, &output_tensor, attr));\n     Tensor indices = c->input(1);\n     PartialTensorShape element_shape;\n+    OP_REQUIRES(\n+        c, !TensorShapeUtils::IsMatrixOrHigher(c->input(2).shape()),\n+        errors::InvalidArgument(\n+            \"TensorListScatter: element_shape must be at most rank 1 but has \",\n+            \"the shape of \", c->input(2).shape().DebugString()));\n     OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(2), &element_shape));\n     // TensorListScatterV2 passes the num_elements input, TensorListScatter does\n     // not.\n",
            "@@ -481,6 +481,30 @@ class ListOpsTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n     # TensorListScatter should return a list with size num_elements.\n     self.assertAllEqual(list_ops.tensor_list_length(l), 5)\n \n+  def testScatterFailsWhenElementShapeIsNotVector(self):\n+    c0 = constant_op.constant([1.0, 2.0])\n+    # In Eager mode, InvalidArgumentError is generated by the Compute function.\n+    # In graph mode, ValueError is generated by the shape function.\n+    with self.assertRaisesRegex(\n+        (errors.InvalidArgumentError, ValueError),\n+        \"must be at most rank 1\"):\n+      l = gen_list_ops.tensor_list_scatter(\n+          # Wrong element_shape. Should be at most rank 1.\n+          c0, [1, 3], element_shape=[[1]])\n+      self.evaluate(l)\n+\n+  def testScatterV2FailsWhenElementShapeIsNotVector(self):\n+    c0 = constant_op.constant([1.0, 2.0])\n+    # In Eager mode, InvalidArgumentError is generated by the Compute function.\n+    # In graph mode, ValueError is generated by the shape function.\n+    with self.assertRaisesRegex(\n+        (errors.InvalidArgumentError, ValueError),\n+        \"must be at most rank 1\"):\n+      l = gen_list_ops.tensor_list_scatter_v2(\n+          # Wrong element_shape. Should be at most rank 1.\n+          c0, [1, 3], element_shape=[[1]], num_elements=2)\n+      self.evaluate(l)\n+\n   def testScatterFailsWhenIndexLargerThanNumElements(self):\n     c0 = constant_op.constant([1.0, 2.0])\n     with self.assertRaisesRegex(\n"
        ],
        "Title": "\n          `CHECK` fail in `TensorListScatter` and `TensorListScatterV2`\n        "
    },
    {
        "Bug description": "When  tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient  receives input  min  or  max  of rank other than 1, it gives a  CHECK  fail that can trigger a denial of service attack.",
        "Sample Code": "arg_0=tf.random.uniform(shape=(1,1), dtype=tf.float32, maxval=None)\narg_1=tf.random.uniform(shape=(1,1), dtype=tf.float32, maxval=None)\narg_2=tf.random.uniform(shape=(1,1), dtype=tf.float32, maxval=None)\narg_3=tf.random.uniform(shape=(1,1), dtype=tf.float32, maxval=None)\narg_4=8\narg_5=False\narg_6=None\ntf.quantization.fake_quant_with_min_max_vars_per_channel_gradient(gradients=arg_0, \n            inputs=arg_1, min=arg_2,  max=arg_3, num_bits=arg_4, \n            , \n            narrow_range=arg_5, name=arg_6)",
        "Bug fix": [
            "@@ -261,6 +261,12 @@ class FakeQuantWithMinMaxVarsGradientOp : public OpKernel {\n                 InvalidArgument(\"gradient and input must be the same size\"));\n     const Tensor& min = context->input(2);\n     const Tensor& max = context->input(3);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min.shape()),\n+        InvalidArgument(\"`min` must be rank 0 but is rank \", min.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max.shape()),\n+        InvalidArgument(\"`max` must be rank 0 but is rank \", max.dims()));\n \n     Tensor* grad_wrt_input;\n     OP_REQUIRES_OK(context,\n@@ -414,10 +420,16 @@ class FakeQuantWithMinMaxVarsPerChannelGradientOp : public OpKernel {\n                 InvalidArgument(\"gradient and input must be the same size\"));\n     const int depth = input.dim_size(input.dims() - 1);  // last dimension size.\n     const Tensor& min = context->input(2);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(min.shape()),\n+        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));\n     OP_REQUIRES(context, min.dim_size(0) == depth,\n                 InvalidArgument(\"min has incorrect size, expected \", depth,\n                                 \" was \", min.dim_size(0)));\n     const Tensor& max = context->input(3);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(max.shape()),\n+        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));\n     OP_REQUIRES(context, max.dim_size(0) == depth,\n                 InvalidArgument(\"max has incorrect size, expected \", depth,\n                                 \" was \", max.dim_size(0)));\n",
            "@@ -77,6 +77,71 @@ class FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):\n               inputs=inputs, min=[0.0], max=[1.0, 1.1]))\n \n \n+class FakeQuantWithMinMaxVarsGradientOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    gradients = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+    inputs = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be equal rank|must be rank 0\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_gradient(\n+              gradients=gradients,\n+              inputs=inputs,\n+              min=0.0,\n+              max=[[1.0], [2.0], [4.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_gradient(\n+              gradients=gradients,\n+              inputs=inputs,\n+              min=[[1.0], [2.0], [4.0]],\n+              max=[[1.0], [2.0], [4.0]]))\n+\n+\n+class FakeQuantWithMinMaxVarsPerChannelGradientOpTest(\n+    test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    gradients = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+    inputs = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Shapes must be equal rank|must be rank 1\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(\n+              gradients=gradients, inputs=inputs, min=[[0.0]], max=[1.0]))\n+\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        \"Dimension 0 in both shapes must be equal|incorrect size\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(\n+              gradients=gradients, inputs=inputs, min=[0.0, 0.1], max=[1.0]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Shapes must be equal rank|must be rank 1\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(\n+              gradients=gradients, inputs=inputs, min=[1.0], max=[[1.0]]))\n+\n+    with self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError),\n+        \"Dimension 0 in both shapes must be equal|incorrect size\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(\n+              gradients=gradients, inputs=inputs, min=[0.0], max=[1.0, 1.1]))\n+\n+\n class QuantizedBiasedAddTest(test_util.TensorFlowTestCase):\n \n   @test_util.run_in_graph_and_eager_modes\n@@ -337,10 +402,9 @@ class QuantizeDownAndShrinkRangeOpTest(test_util.TensorFlowTestCase):\n     with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                 \"must be rank 0\"):\n       self.evaluate(\n-          math_ops.quantize_down_and_shrink_range(input=inputs,\n-                                                  input_min=[],\n-                                                  input_max=4.0,\n-                                                  out_type=dtypes.quint8))\n+          math_ops.quantize_down_and_shrink_range(\n+              input=inputs, input_min=[], input_max=4.0,\n+              out_type=dtypes.quint8))\n \n \n if __name__ == \"__main__\":\n"
        ],
        "Title": "\n          `CHECK` fail in `FakeQuantWithMinMaxVarsPerChannelGradient`\n        "
    },
    {
        "Bug description": "When  MaxPool  receives a window size input array  ksize  with dimensions greater than its input tensor  input , the GPU kernel gives a  CHECK  fail that can be used to trigger a denial of service attack.",
        "Sample Code": "import numpy as np\n\ninput = np.ones([1, 1, 1, 1])\nksize = [1, 1, 2, 2]\nstrides = [1, 1, 1, 1]\npadding = 'VALID'\ndata_format = 'NCHW'\n\n\n\ntf.raw_ops.MaxPool(input=input, ksize=ksize, strides=strides, padding=padding, data_format=data_format)",
        "Bug fix": [
            "@@ -1268,6 +1268,13 @@ class MaxPoolingNoMaskOp<GPUDevice, T> : public OpKernel {\n         ShapeFromFormat(data_format_, params.tensor_in_batch, params.out_height,\n                         params.out_width, params.depth);\n \n+    // Degenerate pooling output should return an empty tensor.\n+    if (out_shape.num_elements() == 0) {\n+      Tensor* output = nullptr;\n+      OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n+      return;\n+    }\n+\n     // Assuming qint8 <--> NCHW_VECT_C (int8x4) here.\n     constexpr bool is_int8x4 = std::is_same<T, qint8>::value;\n     OP_REQUIRES(context, (is_int8x4 == (data_format_ == FORMAT_NCHW_VECT_C)),\n",
            "@@ -772,6 +772,18 @@ class PoolingTest(test.TestCase, parameterized.TestCase):\n         expected=[],\n         **kwargs)\n \n+  @parameterized.parameters(\n+      GetTestConfigsDicts(nn_ops.max_pool, gen_nn_ops.max_pool_v2))\n+  @test_util.run_deprecated_v1\n+  def testMaxPoolInvalidFilterSize(self, **kwargs):\n+    with self.cached_session(use_gpu=test.is_gpu_available()):\n+      t = constant_op.constant(1.0, shape=[1, 1, 1, 1])\n+      with self.assertRaisesRegex(\n+          (errors_impl.InvalidArgumentError, ValueError),\n+          \"Negative dimension size\"):\n+        t = self.evaluate(\n+            nn_ops.max_pool(t, ksize=[1, 1, 2, 1], strides=1, padding=\"VALID\"))\n+\n   # Tests for DepthwiseMaxPooling on CPU only.\n   @parameterized.parameters(\n       GetTestConfigsDicts(\n"
        ],
        "Title": "\n          `CHECK` fail in `MaxPool`\n        "
    },
    {
        "Bug description": "When  tf.linalg.matrix_rank  receives an empty input  a , the GPU kernel gives a  CHECK  fail that can be used to trigger a denial of service attack.",
        "Sample Code": "a = tf.constant([], shape=[0, 1, 1], dtype=tf.float32)\n)\ntf.linalg.matrix_rank(a=a)",
        "Bug fix": [
            "@@ -395,6 +395,12 @@ class SvdOpGpu : public AsyncOpKernel {\n     OP_REQUIRES_OK_ASYNC(context, context->allocate_output(2, shapeV, &outputV),\n                          done);\n \n+    // If there are zero batches, we are done.\n+    if (shapeRaw.num_elements() == 0) {\n+      done();\n+      return;\n+    }\n+\n     if (n == 0 || m == 0) {\n       if (n == m || !compute_uv_ || !full_matrices_) {\n         // S, U, and V are all empty. Nothing to do.\n",
            "@@ -108,6 +108,14 @@ class SvdOpTest(test.TestCase):\n     for i in range(0, len(val), 2):\n       self.assertAllEqual(val[i], val[i + 1])\n \n+  @test_util.run_in_graph_and_eager_modes(use_gpu=True)\n+  def testEmptyBatches(self):\n+    matrices = constant_op.constant(1.0, shape=[0, 2, 2])\n+    s, u, v = self.evaluate(linalg_ops.svd(matrices))\n+    self.assertAllEqual(s, np.zeros([0, 2]))\n+    self.assertAllEqual(u, np.zeros([0, 2, 2]))\n+    self.assertAllEqual(v, np.zeros([0, 2, 2]))\n+\n \n def _GetSvdOpTest(dtype_, shape_, use_static_shape_, compute_uv_,\n                   full_matrices_):\n"
        ],
        "Title": "\n          `CHECK` fail in `tf.linalg.matrix_rank`\n        "
    },
    {
        "Bug description": "DenseBincount  assumes its input tensor  weights  to either have the same shape as its input tensor  input  or to be length-0. A different  weights  shape will trigger a  CHECK  fail that can be used to trigger a denial of service attack.",
        "Sample Code": "binary_output = True\ninput = tf.random.uniform(shape=[0, 0], minval=-10000, maxval=10000, dtype=tf.int32, seed=-2460)\nsize = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.int32, seed=-10000)\nweights = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.float32, seed=-10000)\n)\ntf.raw_ops.DenseBincount(input=input, size=size, weights=weights, binary_output=binary_output)",
        "Bug fix": [
            "@@ -80,6 +80,17 @@ class DenseBincountOp : public XlaOpKernel {\n     OP_REQUIRES_OK(ctx, weights_shape_or.status());\n \n     auto weights_shape = weights_shape_or.ValueOrDie();\n+    OP_REQUIRES(ctx,\n+                xla::ShapeUtil::CompatibleIgnoringElementType(weights_shape,\n+                                                              input_shape) ||\n+                    (weights_shape.dimensions_size() > 0 &&\n+                     weights_shape.dimensions(0) == 0),\n+                errors::InvalidArgument(\n+                    \"`weights` must be the same shape as `arr` or a length-0 \"\n+                    \"`Tensor`, in which case it acts as all weights equal to \"\n+                    \"1. Received \",\n+                    weights_shape.DebugString()));\n+\n     auto weights_size = weights_shape.dimensions(0);\n     bool has_weights = false;\n     if (weights_size) {\n",
            "@@ -280,6 +280,14 @@ class DenseBincountOp : public OpKernel {\n     OP_REQUIRES(ctx, size_t.dims() == 0,\n                 errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                         size_t.dims()));\n+    OP_REQUIRES(ctx,\n+                weights.shape() == data.shape() || weights.NumElements() == 0,\n+                errors::InvalidArgument(\n+                    \"`weights` must be the same shape as `arr` or a length-0 \"\n+                    \"`Tensor`, in which case it acts as all weights equal to \"\n+                    \"1. Received \",\n+                    weights.shape().DebugString()));\n+\n     Tidx size = size_t.scalar<Tidx>()();\n     OP_REQUIRES(\n         ctx, size >= 0,\n",
            "@@ -24,6 +24,7 @@ from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import bincount_ops\n from tensorflow.python.ops import gen_math_ops\n+from tensorflow.python.ops import random_ops\n from tensorflow.python.ops import sparse_ops\n from tensorflow.python.ops.ragged import ragged_factory_ops\n from tensorflow.python.ops.ragged import ragged_tensor\n@@ -152,6 +153,31 @@ class BincountTest(test_util.TensorFlowTestCase):\n       v2 = gen_math_ops.bincount([1, 2, 3, 1, 6, 8], s, [])\n       self.assertAllEqual(v2.get_shape().as_list(), [None])\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    binary_output = True\n+    inp = random_ops.random_uniform(\n+        shape=[10, 10],\n+        minval=-10000,\n+        maxval=10000,\n+        dtype=dtypes.int32,\n+        seed=-2460)\n+    size = random_ops.random_uniform(\n+        shape=[], minval=-10000, maxval=10000, dtype=dtypes.int32, seed=-10000)\n+    weights = random_ops.random_uniform(\n+        shape=[],\n+        minval=-10000,\n+        maxval=10000,\n+        dtype=dtypes.float32,\n+        seed=-10000)\n+    with self.assertRaises(errors.InvalidArgumentError):\n+      self.evaluate(\n+          gen_math_ops.dense_bincount(\n+              input=inp,\n+              size=size,\n+              weights=weights,\n+              binary_output=binary_output))\n+\n \n class BincountOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n \n"
        ],
        "Title": "\n          `CHECK` fail in `DenseBincount`\n        "
    },
    {
        "Bug description": "If  RaggedBincount  is given an empty input tensor  splits , it results in a segfault that can be used to trigger a denial of service attack.",
        "Sample Code": "binary_output = True\nsplits = tf.random.uniform(shape=[0], minval=-10000, maxval=10000, dtype=tf.int64, seed=-7430)\nvalues = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.int32, seed=-10000)\nsize = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.int32, seed=-10000)\nweights = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.float32, seed=-10000)\n)\ntf.raw_ops.RaggedBincount(splits=splits, values=values, size=size, weights=weights, binary_output=binary_output)",
        "Bug fix": [
            "@@ -493,6 +493,9 @@ class RaggedBincountOp : public OpKernel {\n     int num_values = values.size();\n     int batch_idx = 0;\n \n+    OP_REQUIRES(ctx, splits.size() > 0,\n+                errors::InvalidArgument(\"Splits must be non-empty\"));\n+\n     OP_REQUIRES(ctx, splits(0) == 0,\n                 errors::InvalidArgument(\"Splits must start with 0, not with \",\n                                         splits(0)));\n",
            "@@ -734,6 +734,18 @@ class RaggedBincountOpTest(test_util.TensorFlowTestCase,\n               binary_output=False,\n               name=None))\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def test_splits_empty(self):  # b/238450914\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Splits must be non-empty\"):\n+      self.evaluate(\n+          gen_math_ops.ragged_bincount(\n+              splits=[],  # Invalid splits\n+              values=[1],\n+              size=1,\n+              weights=[1],\n+              binary_output=False,\n+              name=None))\n \n if __name__ == \"__main__\":\n   googletest.main()\n"
        ],
        "Title": "\n          Segfault in `RaggedBincount`\n        "
    },
    {
        "Bug description": "If  LRNGrad  is given an  output_image  input tensor that is not 4-D, it results in a  CHECK  fail that can be used to trigger a denial of service attack.",
        "Sample Code": "depth_radius = 1\nbias = 1.59018219\nalpha = 0.117728651\nbeta = 0.404427052\ninput_grads = tf.random.uniform(shape=[4, 4, 4, 4], minval=-10000, maxval=10000, dtype=tf.float32, seed=-2033)\ninput_image = tf.random.uniform(shape=[4, 4, 4, 4], minval=-10000, maxval=10000, dtype=tf.float32, seed=-2033)\noutput_image = tf.random.uniform(shape=[4, 4, 4, 4, 4, 4], minval=-10000, maxval=10000, dtype=tf.float32, seed=-2033)\n)\ntf.raw_ops.LRNGrad(input_grads=input_grads, input_image=input_image, output_image=output_image, depth_radius=depth_radius, bias=bias, alpha=alpha, beta=beta)",
        "Bug fix": [
            "@@ -668,7 +668,8 @@ class LRNGradOp : public OpKernel {\n         in_image.dim_size(0) == batch && in_image.dim_size(1) == rows &&\n             in_image.dim_size(2) == cols && in_image.dim_size(3) == depth &&\n             out_image.dim_size(0) == batch && out_image.dim_size(1) == rows &&\n-            out_image.dim_size(2) == cols && out_image.dim_size(3) == depth,\n+            out_image.dim_size(2) == cols && out_image.dim_size(3) == depth &&\n+            out_image.dims() == 4,\n         errors::InvalidArgument(\n             \"input_grads, input_image, and out_image should have the same \"\n             \"shape\"));\n",
            "@@ -20,11 +20,13 @@ import numpy as np\n \n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors_impl\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import gradient_checker\n from tensorflow.python.ops import gradients_impl\n from tensorflow.python.ops import nn\n+from tensorflow.python.ops import random_ops\n import tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\n from tensorflow.python.platform import test\n \n@@ -111,6 +113,41 @@ class LRNOpTest(test.TestCase):\n     self.assertAllClose(r, expected)\n     self.assertShapeEqual(expected, grad)\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def testIncompatibleInputAndOutputImageShapes(self):\n+    depth_radius = 1\n+    bias = 1.59018219\n+    alpha = 0.117728651\n+    beta = 0.404427052\n+    input_grads = random_ops.random_uniform(\n+        shape=[4, 4, 4, 4],\n+        minval=-10000,\n+        maxval=10000,\n+        dtype=dtypes.float32,\n+        seed=-2033)\n+    input_image = random_ops.random_uniform(\n+        shape=[4, 4, 4, 4],\n+        minval=-10000,\n+        maxval=10000,\n+        dtype=dtypes.float32,\n+        seed=-2033)\n+    invalid_output_image = random_ops.random_uniform(\n+        shape=[4, 4, 4, 4, 4, 4],\n+        minval=-10000,\n+        maxval=10000,\n+        dtype=dtypes.float32,\n+        seed=-2033)\n+    with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):\n+      self.evaluate(\n+          nn.lrn_grad(\n+              input_grads=input_grads,\n+              input_image=input_image,\n+              output_image=invalid_output_image,\n+              depth_radius=depth_radius,\n+              bias=bias,\n+              alpha=alpha,\n+              beta=beta))\n+\n   def _RunAndVerifyGradients(self, dtype):\n     with self.cached_session():\n       # random shape\n"
        ],
        "Title": "\n          `CHECK` fail in `LRNGrad`\n        "
    },
    {
        "Bug description": "ParameterizedTruncatedNormal  assumes  shape  is of type  int32 . A valid  shape  of type  int64  results in a mismatched type  CHECK  fail that can be used to trigger a denial of service attack.",
        "Sample Code": "seed = 1618\nseed2 = 0\nshape = tf.random.uniform(shape=[3], minval=-10000, maxval=10000, dtype=tf.int64, seed=4894)\nmeans = tf.random.uniform(shape=[3, 3, 3], minval=-10000, maxval=10000, dtype=tf.float32, seed=-2971)\nstdevs = tf.random.uniform(shape=[3, 3, 3], minval=-10000, maxval=10000, dtype=tf.float32, seed=-2971)\nminvals = tf.random.uniform(shape=[3, 3, 3], minval=-10000, maxval=10000, dtype=tf.float32, seed=-2971)\nmaxvals = tf.random.uniform(shape=[3, 3, 3], minval=-10000, maxval=10000, dtype=tf.float32, seed=-2971)\n)\ntf.raw_ops.ParameterizedTruncatedNormal(shape=shape, means=means, stdevs=stdevs, minvals=minvals, maxvals=maxvals, seed=seed, seed2=seed2)",
        "Bug fix": [
            "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor_util.h\"\n #include \"tensorflow/core/kernels/stateless_random_ops.h\"\n #include \"tensorflow/core/lib/random/random_distributions.h\"\n #include \"tensorflow/core/platform/logging.h\"\n@@ -630,20 +631,18 @@ class ParameterizedTruncatedNormalOp : public OpKernel {\n     OP_REQUIRES(ctx, shape_tensor.NumElements() > 0,\n                 errors::InvalidArgument(\"Shape tensor must not be empty, got \",\n                                         shape_tensor.DebugString()));\n-    int32_t num_batches = shape_tensor.flat<int32>()(0);\n+    TensorShape tensor_shape;\n+    OP_REQUIRES_OK(ctx, tensor::MakeShape(shape_tensor, &tensor_shape));\n \n+    int32_t num_batches = tensor_shape.dim_size(0);\n     int32_t samples_per_batch = 1;\n-    const int32_t num_dims = shape_tensor.dim_size(0);\n+    const int32_t num_dims = tensor_shape.dims();\n     for (int32_t i = 1; i < num_dims; i++) {\n-      samples_per_batch *= shape_tensor.flat<int32>()(i);\n+      samples_per_batch *= tensor_shape.dim_size(i);\n     }\n     const int32_t num_elements = num_batches * samples_per_batch;\n \n     // Allocate the output before fudging num_batches and samples_per_batch.\n-    auto shape_vec = shape_tensor.flat<int32>();\n-    TensorShape tensor_shape;\n-    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n-                            shape_vec.data(), shape_vec.size(), &tensor_shape));\n     Tensor* samples_tensor;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));\n \n",
            "@@ -303,6 +303,29 @@ class ParameterizedTruncatedNormalTest(test.TestCase):\n       self.assertAllGreater(samples, 0.)\n       self.assertAllGreater(samples_stateless, 0.)\n \n+  def testShapeTypes(self):\n+    for shape_dtype in [np.int32, np.int64]:\n+      shape = np.array([1000], dtype=shape_dtype)\n+      sample_op = random_ops.parameterized_truncated_normal(\n+          shape=shape, means=0.0, stddevs=0.1, minvals=-1., maxvals=1.)\n+      new_seed = random_ops.random_uniform([2],\n+                                           seed=1234,\n+                                           minval=0,\n+                                           maxval=(2**31 - 1),\n+                                           dtype=np.int32)\n+      sample_op_stateless = stateless.stateless_parameterized_truncated_normal(\n+          shape=shape,\n+          seed=new_seed,\n+          means=0.0,\n+          stddevs=0.1,\n+          minvals=-1.,\n+          maxvals=1.)\n+\n+      samples = self.evaluate(sample_op)\n+      stateless_samples = self.evaluate(sample_op_stateless)\n+      self.assertAllEqual(samples.shape, shape)\n+      self.assertAllEqual(stateless_samples.shape, shape)\n+\n   def testStatelessParameterizedTruncatedNormalHasGrads(self):\n     mean = variables.Variable(0.01)\n     stddev = variables.Variable(1.)\n"
        ],
        "Title": "\n          `CHECK` fail in `ParameterizedTruncatedNormal`\n        "
    },
    {
        "Bug description": "If  Save  or  SaveSlices  is run over tensors of an unsupported  dtype , it results in a  CHECK  fail that can be used to trigger a denial of service attack.",
        "Sample Code": "filename = tf.constant(\"\")\ntensor_names = tf.constant(\"\")\n# Save\ndata = tf.cast(tf.random.uniform(shape=[1], minval=-10000, maxval=10000, dtype=tf.int64, seed=-2021), tf.uint64)\ntf.raw_ops.Save(filename=filename, tensor_names=tensor_names, data=data, )\n# SaveSlices\nshapes_and_slices = tf.constant(\"\")\ndata = tf.cast(tf.random.uniform(shape=[1], minval=-10000, maxval=10000, dtype=tf.int64, seed=9712), tf.uint32)\n)\ntf.raw_ops.SaveSlices(filename=filename, tensor_names=tensor_names, shapes_and_slices=shapes_and_slices, data=data, )",
        "Bug fix": [
            "@@ -131,6 +131,16 @@ Status TensorSliceWriter::Finish() {\n \n /* static */\n size_t TensorSliceWriter::MaxBytesPerElement(DataType dt) {\n+  size_t max_bytes_per_element =\n+      TensorSliceWriter::MaxBytesPerElementOrZero(dt);\n+  if (max_bytes_per_element == 0) {\n+    LOG(FATAL) << \"MaxBytesPerElement not implemented for dtype: \" << dt;\n+  }\n+  return max_bytes_per_element;\n+}\n+\n+/* static */\n+size_t TensorSliceWriter::MaxBytesPerElementOrZero(DataType dt) {\n   switch (dt) {\n     case DT_FLOAT:\n       return 4;\n@@ -170,9 +180,8 @@ size_t TensorSliceWriter::MaxBytesPerElement(DataType dt) {\n     case DT_STRING:\n     case DT_BFLOAT16:\n     default:\n-      LOG(FATAL) << \"MaxBytesPerElement not implemented for dtype: \" << dt;\n+      return 0;\n   }\n-  return 0;\n }\n \n template <>\n",
            "@@ -68,6 +68,8 @@ class TensorSliceWriter {\n   static size_t MaxBytesPerElement(DataType dt);\n \n  private:\n+  static size_t MaxBytesPerElementOrZero(DataType dt);\n+\n   static constexpr size_t kMaxMessageBytes = 1LL << 31;\n   // Filling in the TensorProto in a SavedSlice will add the following\n   // header bytes, in addition to the data:\n@@ -162,9 +164,15 @@ Status TensorSliceWriter::Add(const string& name, const TensorShape& shape,\n template <typename T>\n Status TensorSliceWriter::SaveData(const T* data, int64_t num_elements,\n                                    SavedSlice* ss) {\n-  size_t size_bound =\n-      ss->ByteSize() + kTensorProtoHeaderBytes +\n-      (MaxBytesPerElement(DataTypeToEnum<T>::value) * num_elements);\n+  size_t max_bytes_per_element =\n+      MaxBytesPerElementOrZero(DataTypeToEnum<T>::value);\n+  if (max_bytes_per_element == 0) {\n+    return errors::InvalidArgument(\n+        \"Tensor slice serialization not implemented for dtype \",\n+        DataTypeToEnum<T>::value);\n+  }\n+  size_t size_bound = ss->ByteSize() + kTensorProtoHeaderBytes +\n+                      (max_bytes_per_element * num_elements);\n   if (size_bound > kMaxMessageBytes) {\n     return errors::InvalidArgument(\n         \"Tensor slice is too large to serialize (conservative estimate: \",\n",
            "@@ -15,17 +15,19 @@ limitations under the License.\n \n #include \"tensorflow/core/util/tensor_slice_writer.h\"\n \n+#include <algorithm>\n #include <array>\n+#include <memory>\n+#include <vector>\n \n #include \"tensorflow/core/framework/tensor_shape.pb.h\"\n #include \"tensorflow/core/framework/versions.pb.h\"\n #include \"tensorflow/core/lib/core/status_test_util.h\"\n-#include \"tensorflow/core/lib/core/stringpiece.h\"\n-#include \"tensorflow/core/lib/io/path.h\"\n-#include \"tensorflow/core/lib/strings/str_util.h\"\n #include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/path.h\"\n #include \"tensorflow/core/platform/protobuf.h\"\n #include \"tensorflow/core/platform/test.h\"\n+#include \"tensorflow/core/protobuf/error_codes.pb.h\"\n #include \"tensorflow/core/public/version.h\"\n #include \"tensorflow/core/util/saved_tensor_slice_util.h\"\n #include \"tensorflow/core/util/tensor_slice_reader.h\"\n@@ -362,6 +364,17 @@ TEST(TensorSliceWriteTest, SizeErrors) {\n   }\n }\n \n+TEST(TensorSliceWriterTest, InvalidInput) {\n+  SavedSlice ss;\n+  std::array<uint32_t, 1> data;\n+  std::fill(data.begin(), data.end(), 1234);\n+  Status s = TensorSliceWriter::SaveData(data.data(), data.size(), &ss);\n+  EXPECT_EQ(s.code(), error::INVALID_ARGUMENT);\n+  EXPECT_TRUE(absl::StrContains(\n+      s.error_message(),\n+      \"Tensor slice serialization not implemented for dtype\"));\n+}\n+\n }  // namespace checkpoint\n \n }  // namespace tensorflow\n"
        ],
        "Title": "\n          `CHECK` fail in `Save` and `SaveSlices`\n        "
    },
    {
        "Bug description": "If  SparseBincount  is given inputs for  indices ,  values , and  dense_shape  that do not make a valid sparse tensor, it results in a segfault that can be used to trigger a denial of service attack.",
        "Sample Code": "binary_output = True\nindices = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.int64, seed=-1288)\nvalues = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.int32, seed=-9366)\ndense_shape = tf.random.uniform(shape=[0], minval=-10000, maxval=10000, dtype=tf.int64, seed=-9878)\nsize = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.int32, seed=-10000)\nweights = tf.random.uniform(shape=[], minval=-10000, maxval=10000, dtype=tf.float32, seed=-10000)\n)\ntf.raw_ops.SparseBincount(indices=indices, values=values, dense_shape=dense_shape, size=size, weights=weights, binary_output=binary_output)",
        "Bug fix": [
            "@@ -4421,6 +4421,7 @@ tf_kernel_library(\n     deps = [\n         \":fill_functor\",\n         \":gpu_prim_hdrs\",\n+        \":sparse_utils\",\n         \"//tensorflow/core:framework\",\n         \"//tensorflow/core:lib\",\n         \"//tensorflow/core:lib_internal\",\n@@ -5007,6 +5008,7 @@ cc_library(\n SPARSE_DEPS = [\n     \"//tensorflow/core:framework\",\n     \"//tensorflow/core:lib\",\n+    \":sparse_utils\",\n ]\n \n tf_kernel_library(\n@@ -6480,6 +6482,7 @@ filegroup(\n         \"sparse_reorder_op.h\",\n         \"sparse_slice_op.h\",\n         \"sparse_tensor_dense_matmul_op.h\",\n+        \"sparse_utils.h\",\n         \"string_util.h\",\n         \"string_to_hash_bucket_op.h\",\n         \"string_to_hash_bucket_fast_op.h\",\n@@ -6718,6 +6721,7 @@ filegroup(\n         \"random_ops_util.h\",\n         \"random_poisson_op.cc\",\n         \"shuffle_common.h\",\n+        \"sparse_utils.cc\",\n         \"random_shuffle_op.cc\",\n         \"reduce_join_op.cc\",\n         \"reduction_ops_all.cc\",\n",
            "@@ -23,6 +23,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/bincount_op.h\"\n #include \"tensorflow/core/kernels/fill_functor.h\"\n+#include \"tensorflow/core/kernels/sparse_utils.h\"\n #include \"tensorflow/core/lib/core/threadpool.h\"\n #include \"tensorflow/core/platform/types.h\"\n #include \"tensorflow/core/util/determinism.h\"\n@@ -369,7 +370,8 @@ class SparseBincountOp : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& indices = ctx->input(0);\n-    const auto values = ctx->input(1).flat<Tidx>();\n+    const Tensor& values = ctx->input(1);\n+    const auto values_flat = values.flat<Tidx>();\n     const Tensor& dense_shape = ctx->input(2);\n     const Tensor& size_t = ctx->input(3);\n     const auto weights = ctx->input(4).flat<T>();\n@@ -382,6 +384,9 @@ class SparseBincountOp : public OpKernel {\n     OP_REQUIRES(\n         ctx, size >= 0,\n         errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n+    OP_REQUIRES_OK(\n+        ctx, sparse_utils::ValidateSparseTensor<int64_t>(\n+                 indices, values, dense_shape, /*validate_indices=*/true));\n \n     bool is_1d = dense_shape.NumElements() == 1;\n \n@@ -394,11 +399,11 @@ class SparseBincountOp : public OpKernel {\n       if (binary_output_) {\n         OP_REQUIRES_OK(ctx,\n                        functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n-                           ctx, values, weights, out, size));\n+                           ctx, values_flat, weights, out, size));\n       } else {\n         OP_REQUIRES_OK(\n             ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n-                     ctx, values, weights, out, size));\n+                     ctx, values_flat, weights, out, size));\n       }\n     } else {\n       const auto shape = dense_shape.flat<int64_t>();\n@@ -410,7 +415,7 @@ class SparseBincountOp : public OpKernel {\n       const auto indices_mat = indices.matrix<int64_t>();\n       for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {\n         const int64_t batch = indices_mat(i, 0);\n-        const Tidx bin = values(i);\n+        const Tidx bin = values_flat(i);\n         OP_REQUIRES(\n             ctx, batch < out.dimension(0),\n             errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,\n",
            "@@ -366,7 +366,7 @@ class SparseBincountOpTest(test_util.TensorFlowTestCase,\n     num_rows = 128\n     size = 1000\n     n_elems = 4096\n-    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n+    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n     inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n \n     np_out = np.bincount(inp_vals, minlength=size)\n@@ -390,7 +390,7 @@ class SparseBincountOpTest(test_util.TensorFlowTestCase,\n     num_rows = 128\n     size = 1000\n     n_elems = 4096\n-    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n+    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n     inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n     inp_weight = np.random.random((n_elems,))\n \n@@ -415,7 +415,7 @@ class SparseBincountOpTest(test_util.TensorFlowTestCase,\n     num_rows = 128\n     size = 10\n     n_elems = 4096\n-    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n+    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n     inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n \n     np_out = np.ones((size,))\n@@ -440,7 +440,7 @@ class SparseBincountOpTest(test_util.TensorFlowTestCase,\n     num_rows = 128\n     size = 10\n     n_elems = 4096\n-    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n+    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n     inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n     inp_weight = np.random.random((n_elems,))\n \n@@ -532,6 +532,27 @@ class SparseBincountOpTest(test_util.TensorFlowTestCase,\n               weights=[0, 0],\n               binary_output=False))\n \n+  def test_sparse_bincount_input_validation(self):\n+    np.random.seed(42)\n+    num_rows = 128\n+    size = 1000\n+    n_elems = 4096\n+    inp_indices = np.random.randint(0, num_rows, (n_elems, 1))\n+    inp_vals = np.random.randint(0, size, (n_elems,))\n+\n+    # Insert negative index.\n+    inp_indices[10, 0] = -2\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"out of bounds\"):\n+      self.evaluate(\n+          gen_math_ops.sparse_bincount(\n+              indices=inp_indices,\n+              values=inp_vals,\n+              dense_shape=[num_rows],\n+              size=size,\n+              weights=[]))\n+\n \n class RaggedBincountOpTest(test_util.TensorFlowTestCase,\n                            parameterized.TestCase):\n"
        ],
        "Title": "\n          Segfault in `SparseBincount`\n        "
    }
]
[
    {
        "Bug description": "If  RaggedTensorToVariant  is given a  rt_nested_splits  list that contains tensors of ranks other than one, it results in a  CHECK  fail that can be used to trigger a denial of service attack.",
        "Sample Code": "batched_input = True\nrt_nested_splits = tf.constant([0,32,64], shape=[3], dtype=tf.int64)\nrt_dense_values = tf.constant([0,32,64], shape=[3], dtype=tf.int64)\n)\ntf.raw_ops.RaggedTensorToVariant(rt_nested_splits=rt_nested_splits, rt_dense_values=rt_dense_values, batched_input=batched_input)",
        "Bug fix": [
            "@@ -188,6 +188,10 @@ class RaggedTensorToVariantOp : public OpKernel {\n     batched_ragged_input.mutable_nested_splits()->reserve(\n         ragged_nested_splits_len);\n     for (int i = 0; i < ragged_nested_splits_len; i++) {\n+      OP_REQUIRES(context, ragged_nested_splits_in[i].dims() == 1,\n+                  errors::InvalidArgument(\"Requires nested_row_splits[\", i, \"]\",\n+                                          \" to be rank 1 but is rank \",\n+                                          ragged_nested_splits_in[i].dims()));\n       batched_ragged_input.append_splits(ragged_nested_splits_in[i]);\n     }\n \n",
            "@@ -1468,6 +1468,21 @@ class RaggedTensorTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n         for i in range(3):\n           self.assertAllEqual(sess.run(rt[i]), out)\n \n+  def testToVariantInvalidParams(self):\n+    self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                           r'be rank 1 but is rank 0',\n+                           gen_ragged_conversion_ops.ragged_tensor_to_variant,\n+                           rt_nested_splits=[0, 1, 2],\n+                           rt_dense_values=[0, 1, 2],\n+                           batched_input=True)\n+\n+    self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                           r'be rank 1 but is rank 2',\n+                           gen_ragged_conversion_ops.ragged_tensor_to_variant,\n+                           rt_nested_splits=[[[0]], [[1]], [[2]]],\n+                           rt_dense_values=[0, 1, 2],\n+                           batched_input=True)\n+\n   def testFromVariantInvalidParams(self):\n     rt = ragged_factory_ops.constant([[0], [1], [2], [3]])\n     batched_variant = rt._to_variant(batched_input=True)\n"
        ],
        "Title": "\n          `CHECK` fail in `RaggedTensorToVariant`\n        "
    },
    {
        "Bug description": "FractionalMaxPoolGrad  validates its inputs with  CHECK  failures instead of with returning errors. If it gets incorrectly sized inputs, the  CHECK  failure can be used to trigger a denial of service attack:",
        "Sample Code": "overlapping = True\norig_input = tf.constant(.453409232, shape=[1,7,13,1], dtype=tf.float32)\norig_output = tf.constant(.453409232, shape=[1,7,13,1], dtype=tf.float32)\nout_backprop = tf.constant(.453409232, shape=[1,7,13,1], dtype=tf.float32)\nrow_pooling_sequence = tf.constant(0, shape=[5], dtype=tf.int64)\ncol_pooling_sequence = tf.constant(0, shape=[5], dtype=tf.int64)\n)\ntf.raw_ops.FractionalMaxPoolGrad(orig_input=orig_input, orig_output=orig_output, out_backprop=out_backprop, row_pooling_sequence=row_pooling_sequence, col_pooling_sequence=col_pooling_sequence, overlapping=overlapping)",
        "Bug fix": [
            "@@ -19,12 +19,13 @@ limitations under the License.\n #include <random>\n #include <vector>\n \n-#include \"tensorflow/core/kernels/fractional_pool_common.h\"\n-\n #include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n #include \"tensorflow/core/framework/numeric_op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/op_requires.h\"\n+#include \"tensorflow/core/kernels/fractional_pool_common.h\"\n #include \"tensorflow/core/lib/random/random.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/platform/logging.h\"\n #include \"tensorflow/core/platform/mutex.h\"\n #include \"tensorflow/core/util/guarded_philox_random.h\"\n@@ -352,7 +353,9 @@ class FractionalMaxPoolGradOp : public OpKernel {\n         output_size[2] * output_size[1] * output_size[0];\n     for (int64_t i = 0; i < num_reshaped_cols; ++i) {\n       for (int64_t j = 0; j < output_size[3]; ++j) {\n-        DCHECK_EQ(tensor_out_dup_mat(j, i), tensor_out_mat(j, i));\n+        OP_REQUIRES(context, tensor_out_dup_mat(j, i) == tensor_out_mat(j, i),\n+                    errors::InvalidArgument(\n+                        \"tensor_out_dup is not the same as tensor_out\"));\n       }\n     }\n \n@@ -369,11 +372,12 @@ class FractionalMaxPoolGradOp : public OpKernel {\n \n     for (int index = 0; index < num_total_outputs; ++index) {\n       int input_backprop_index = out_arg_max_flat(index);\n-      // According to maxpooling_op.cc, the performance impact below is small.\n-      CHECK(input_backprop_index >= 0 &&\n-            input_backprop_index < num_total_inputs)\n-          << \"Invalid input backprop index: \" << input_backprop_index << \", \"\n-          << num_total_inputs;\n+      OP_REQUIRES(\n+          context,\n+          input_backprop_index >= 0 && input_backprop_index < num_total_inputs,\n+          errors::InvalidArgument(\n+              \"Invalid input backprop index: \", input_backprop_index, \", \",\n+              num_total_inputs));\n       input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n     }\n   }\n",
            "@@ -124,7 +124,7 @@ class FractionalMaxPoolTest(test.TestCase):\n     Returns:\n       None\n     \"\"\"\n-    with self.cached_session() as sess:\n+    with self.cached_session():\n       p, r, c = nn_ops.fractional_max_pool_v2(\n           input_tensor,\n           pooling_ratio,\n@@ -155,7 +155,7 @@ class FractionalMaxPoolTest(test.TestCase):\n           overlapping))\n       rand_mat = self._PRNG.randint(10, size=tensor_shape)\n       pooling_ratio = [1, math.sqrt(2), math.sqrt(2), 1]\n-      with self.cached_session() as sess:\n+      with self.cached_session():\n         p, r, c = nn_ops.fractional_max_pool_v2(\n             rand_mat,\n             pooling_ratio,\n@@ -630,6 +630,29 @@ class FractionalMaxPoolGradTest(test.TestCase):\n       self.assertAllClose(expected_input_backprop_overlapping,\n                           input_backprop_overlapping)\n \n+  def testInvalidSeqRaiseErrorForFractionalMaxPoolGrad(self):\n+    with self.assertRaises(errors.InvalidArgumentError):\n+      with self.cached_session() as _:\n+        overlapping = True\n+        orig_input = constant_op.constant(\n+            .453409232, shape=[1, 7, 13, 1], dtype=dtypes.float32)\n+        orig_output = constant_op.constant(\n+            .453409232, shape=[1, 7, 13, 1], dtype=dtypes.float32)\n+        out_backprop = constant_op.constant(\n+            .453409232, shape=[1, 7, 13, 1], dtype=dtypes.float32)\n+        row_pooling_sequence = constant_op.constant(\n+            0, shape=[5], dtype=dtypes.int64)\n+        col_pooling_sequence = constant_op.constant(\n+            0, shape=[5], dtype=dtypes.int64)\n+        t = gen_nn_ops.FractionalMaxPoolGrad(\n+            orig_input=orig_input,\n+            orig_output=orig_output,\n+            out_backprop=out_backprop,\n+            row_pooling_sequence=row_pooling_sequence,\n+            col_pooling_sequence=col_pooling_sequence,\n+            overlapping=overlapping)\n+        self.evaluate(t)\n+\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          `CHECK` fail in `FractionalMaxPoolGrad`\n        "
    },
    {
        "Bug description": "If  QuantizedRelu  or  QuantizedRelu6  are given nonscalar inputs for  min_features  or  max_features , it results in a segfault that can be used to trigger a denial of service attack.",
        "Sample Code": "out_type = tf.quint8\nfeatures = tf.constant(28, shape=[4,2], dtype=tf.quint8)\nmin_features = tf.constant([], shape=[0], dtype=tf.float32)\nmax_features = tf.constant(-128, shape=[1], dtype=tf.float32)\ntf.raw_ops.QuantizedRelu(features=features, min_features=min_features, max_features=max_features, out_type=out_type)\n)\ntf.raw_ops.QuantizedRelu6(features=features, min_features=min_features, max_features=max_features, out_type=out_type)",
        "Bug fix": [
            "@@ -32,8 +32,21 @@ class QuantizedReluOp : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n-    const float min_input = context->input(1).flat<float>()(0);\n-    const float max_input = context->input(2).flat<float>()(0);\n+    const Tensor& min_input_tensor = context->input(1);\n+    const Tensor& max_input_tensor = context->input(2);\n+\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),\n+        errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",\n+                                min_input_tensor.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),\n+        errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",\n+                                max_input_tensor.dims()));\n+\n+    const float min_input = min_input_tensor.scalar<float>()();\n+    const float max_input = max_input_tensor.scalar<float>()();\n+\n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(context,\n                    context->allocate_output(0, input.shape(), &output));\n@@ -65,8 +78,21 @@ class QuantizedRelu6Op : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n-    const float min_input = context->input(1).flat<float>()(0);\n-    const float max_input = context->input(2).flat<float>()(0);\n+    const Tensor& min_input_tensor = context->input(1);\n+    const Tensor& max_input_tensor = context->input(2);\n+\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),\n+        errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",\n+                                min_input_tensor.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),\n+        errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",\n+                                max_input_tensor.dims()));\n+\n+    const float min_input = min_input_tensor.scalar<float>()();\n+    const float max_input = max_input_tensor.scalar<float>()();\n+\n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(context,\n                    context->allocate_output(0, input.shape(), &output));\n",
            "@@ -55,8 +55,8 @@ TEST_F(QuantizedActivationsTest, TestRelu) {\n \n   AddInputFromArray<quint8>(input_quantized.shape(),\n                             input_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n@@ -86,8 +86,8 @@ TEST_F(QuantizedActivationsTest, TestRelu6) {\n \n   AddInputFromArray<quint8>(input_quantized.shape(),\n                             input_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n",
            "@@ -25,6 +25,7 @@ limitations under the License.\n \n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/meta_support.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n@@ -457,10 +458,28 @@ class QuantizedAddOp : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& x = context->input(0);\n     const Tensor& y = context->input(1);\n-    const float min_x = context->input(2).flat<float>()(0);\n-    const float max_x = context->input(3).flat<float>()(0);\n-    const float min_y = context->input(4).flat<float>()(0);\n-    const float max_y = context->input(5).flat<float>()(0);\n+    const Tensor& min_x_tensor = context->input(2);\n+    const Tensor& max_x_tensor = context->input(3);\n+    const Tensor& min_y_tensor = context->input(4);\n+    const Tensor& max_y_tensor = context->input(5);\n+\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_x_tensor.shape()),\n+                errors::InvalidArgument(\"`min_x` must be rank 0 but is rank \",\n+                                        min_x_tensor.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_x_tensor.shape()),\n+                errors::InvalidArgument(\"`max_x` must be rank 0 but is rank \",\n+                                        max_x_tensor.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_y_tensor.shape()),\n+                errors::InvalidArgument(\"`min_y` must be rank 0 but is rank \",\n+                                        min_y_tensor.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_y_tensor.shape()),\n+                errors::InvalidArgument(\"`max_y` must be rank 0 but is rank \",\n+                                        max_y_tensor.dims()));\n+\n+    const float min_x = min_x_tensor.scalar<float>()();\n+    const float max_x = max_x_tensor.scalar<float>()();\n+    const float min_y = min_y_tensor.scalar<float>()();\n+    const float max_y = max_y_tensor.scalar<float>()();\n \n     BCast bcast(BCast::FromShape(x.shape()), BCast::FromShape(y.shape()));\n     if (!bcast.IsValid()) {\n",
            "@@ -206,5 +206,60 @@ class RequantizeOpTest(test_util.TensorFlowTestCase):\n               out_type=dtypes.qint8))\n \n \n+class QuantizedAddOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    x = constant_op.constant(\n+        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n+    y = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.quint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.quantized_add(\n+              x=x,\n+              y=y,\n+              min_x=[],\n+              max_x=1.0,\n+              min_y=0.0,\n+              max_y=1.0,\n+              Toutput=dtypes.qint32))\n+\n+\n+class QuantizedReluOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_relu(\n+              features=inputs,\n+              min_features=[],\n+              max_features=127.0,\n+              out_type=dtypes.quint8))\n+\n+\n+class QuantizedRelu6OpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_relu6(\n+              features=inputs,\n+              min_features=[],\n+              max_features=127.0,\n+              out_type=dtypes.quint8))\n+\n+\n if __name__ == \"__main__\":\n   googletest.main()\n"
        ],
        "Title": "\n          Segfault in `QuantizedRelu` and `QuantizedRelu6`\n        "
    },
    {
        "Bug description": "If  QuantizeDownAndShrinkRange  is given nonscalar inputs for  input_min  or  input_max , it results in a segfault that can be used to trigger a denial of service attack.",
        "Sample Code": "out_type = tf.quint8\ninput = tf.constant([1], shape=[3], dtype=tf.qint32)\ninput_min = tf.constant([], shape=[0], dtype=tf.float32)\ninput_max = tf.constant(-256, shape=[1], dtype=tf.float32)\n)\ntf.raw_ops.QuantizeDownAndShrinkRange(input=input, input_min=input_min, input_max=input_max, out_type=out_type)",
        "Bug fix": [
            "@@ -40,8 +40,20 @@ class QuantizeDownAndShrinkRangeOp : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input = ctx->input(0);\n-    const float input_min_float = ctx->input(1).flat<float>()(0);\n-    const float input_max_float = ctx->input(2).flat<float>()(0);\n+    const Tensor& input_min = ctx->input(1);\n+    const Tensor& input_max = ctx->input(2);\n+\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsScalar(input_min.shape()),\n+        errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",\n+                                input_min.dims()));\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsScalar(input_max.shape()),\n+        errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",\n+                                input_max.dims()));\n+\n+    const float input_min_float = input_min.scalar<float>()();\n+    const float input_max_float = input_max.scalar<float>()();\n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n     Tensor* output_min = nullptr;\n",
            "@@ -53,8 +53,8 @@ TEST_F(QuantizeDownAndShrinkRangeTest, HandCrafted) {\n   const int value_count = 3;\n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n   TF_ASSERT_OK(RunOpKernel());\n   Tensor expected(allocator(), DT_QUINT8, TensorShape({value_count}));\n   test::FillValues<quint8>(&expected, {0, 128, 255});\n",
            "@@ -261,5 +261,21 @@ class QuantizedRelu6OpTest(test_util.TensorFlowTestCase):\n               out_type=dtypes.quint8))\n \n \n+class QuantizeDownAndShrinkRangeOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.quantize_down_and_shrink_range(input=inputs,\n+                                                  input_min=[],\n+                                                  input_max=4.0,\n+                                                  out_type=dtypes.quint8))\n+\n+\n if __name__ == \"__main__\":\n   googletest.main()\n"
        ],
        "Title": "\n          Segfault in `QuantizeDownAndShrinkRange`\n        "
    },
    {
        "Bug description": "If  QuantizedMatMul  is given nonscalar input for:",
        "Sample Code": "Toutput = tf.qint32\ntranspose_a = False\ntranspose_b = False\nTactivation = tf.quint8\na = tf.constant(7, shape=[3,4], dtype=tf.quint8)\nb = tf.constant(1, shape=[2,3], dtype=tf.quint8)\nmin_a = tf.constant([], shape=[0], dtype=tf.float32)\nmax_a = tf.constant(0, shape=[1], dtype=tf.float32)\nmin_b = tf.constant(0, shape=[1], dtype=tf.float32)\nmax_b = tf.constant(0, shape=[1], dtype=tf.float32)\n)\ntf.raw_ops.QuantizedMatMul(a=a, b=b, min_a=min_a, max_a=max_a, min_b=min_b, max_b=max_b, Toutput=Toutput, transpose_a=transpose_a, transpose_b=transpose_b, Tactivation=Tactivation)",
        "Bug fix": [
            "@@ -20,11 +20,14 @@ limitations under the License.\n #define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\n #include \"public/gemmlowp.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/op_requires.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/meta_support.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n #include \"tensorflow/core/kernels/reference_gemm.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n \n namespace tensorflow {\n \n@@ -75,9 +78,21 @@ class QuantizedMatMulOp : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& a = context->input(0);\n     const Tensor& b = context->input(1);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(2).shape()),\n+                errors::InvalidArgument(\"min_a must be a scalar, but got shape\",\n+                                        context->input(2).shape()));\n     const float min_a = context->input(2).flat<float>()(0);\n+    OP_REQUIRES(context, context->input(3).NumElements() == 1,\n+                errors::InvalidArgument(\"max_a must be a scalar, but got shape\",\n+                                        context->input(3).shape()));\n     const float max_a = context->input(3).flat<float>()(0);\n+    OP_REQUIRES(context, context->input(4).NumElements() == 1,\n+                errors::InvalidArgument(\"min_b must be a scalar, but got shape\",\n+                                        context->input(4).shape()));\n     const float min_b = context->input(4).flat<float>()(0);\n+    OP_REQUIRES(context, context->input(5).NumElements() == 1,\n+                errors::InvalidArgument(\"max_b must be a scalar, but got shape\",\n+                                        context->input(5).shape()));\n     const float max_b = context->input(5).flat<float>()(0);\n \n     // Make sure that we have valid quantization ranges for the input buffers.\n",
            "@@ -62,10 +62,10 @@ TEST_F(QuantizedMatMulTest, Small_NoParams) {\n   // | 15 | 16 | 17 | 18 |\n   AddInputFromArray<quint8>(TensorShape({3, 4}),\n                             {7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18});\n-  AddInputFromArray<float>(TensorShape({1}), {0});\n-  AddInputFromArray<float>(TensorShape({1}), {255.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {0});\n-  AddInputFromArray<float>(TensorShape({1}), {255.0f});\n+  AddInputFromArray<float>(TensorShape({}), {0});\n+  AddInputFromArray<float>(TensorShape({}), {255.0f});\n+  AddInputFromArray<float>(TensorShape({}), {0});\n+  AddInputFromArray<float>(TensorShape({}), {255.0f});\n \n   TF_ASSERT_OK(RunOpKernel());\n   // Here are the results we expect, from hand calculations:\n@@ -118,10 +118,10 @@ TEST_F(QuantizedMatMulTest, VerySmall_WithParams) {\n   // The B matrix is:\n   // |   1 |\n   AddInputFromArray<quint8>(TensorShape({b_rows, b_cols}), {0});\n-  AddInputFromArray<float>(TensorShape({1}), {-12.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {243.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-12.0f});\n+  AddInputFromArray<float>(TensorShape({}), {243.0f});\n+  AddInputFromArray<float>(TensorShape({}), {1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n   TF_ASSERT_OK(RunOpKernel());\n   // We're requesting C = A.transposed() * B,\n   // so we expect to get these results:\n@@ -162,12 +162,50 @@ TEST_F(QuantizedMatMulTest, VerySmall_BadRange) {\n   // The B matrix is:\n   // |   1 |\n   AddInputFromArray<quint8>(TensorShape({b_rows, b_cols}), {0});\n-  AddInputFromArray<float>(TensorShape({1}), {-12.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {243.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-12.0f});\n+  AddInputFromArray<float>(TensorShape({}), {243.0f});\n   // Here we set the range so that the min and max are equal, so we expect to\n   // see an error when we run.\n-  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {1.0f});\n+  EXPECT_EQ(::tensorflow::error::INVALID_ARGUMENT, RunOpKernel().code());\n+}\n+\n+// This test multiplies two 1x1 8bit matrices, but sets invalid quantized min\n+// and max values, so we expect to get an error\n+TEST_F(QuantizedMatMulTest, VerySmall_BadMinMax) {\n+  // These parameters reflect a typical production usage of eight-bit matmuls\n+  // in an Inception-style network.\n+  const bool transpose_a = true;\n+  const int a_rows = 1;\n+  const int a_cols = 1;\n+  const int b_rows = 1;\n+  const int b_cols = 1;\n+  const bool transpose_b = false;\n+  TF_ASSERT_OK(NodeDefBuilder(\"quantized_mat_mul_op\", \"QuantizedMatMul\")\n+                   .Input(FakeInput(DT_QUINT8))\n+                   .Input(FakeInput(DT_QUINT8))\n+                   .Input(FakeInput(DT_FLOAT))\n+                   .Input(FakeInput(DT_FLOAT))\n+                   .Input(FakeInput(DT_FLOAT))\n+                   .Input(FakeInput(DT_FLOAT))\n+                   .Attr(\"Toutput\", DataTypeToEnum<qint32>::v())\n+                   .Attr(\"transpose_a\", transpose_a)\n+                   .Attr(\"transpose_b\", transpose_b)\n+                   .Finalize(node_def()));\n+  TF_ASSERT_OK(InitOp());\n+  // The A matrix is:\n+  // |  -1 |\n+  AddInputFromArray<quint8>(TensorShape({a_rows, a_cols}), {11});\n+  // The B matrix is:\n+  // |   1 |\n+  AddInputFromArray<quint8>(TensorShape({b_rows, b_cols}), {0});\n+  // Here we set the error of a non scalar min_a value, so we expect to see an\n+  // error when we run.\n+  AddInputFromArray<float>(TensorShape({1}), {2});\n+  AddInputFromArray<float>(TensorShape({}), {243.0f});\n+  AddInputFromArray<float>(TensorShape({}), {1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n   EXPECT_EQ(::tensorflow::error::INVALID_ARGUMENT, RunOpKernel().code());\n }\n \n@@ -233,10 +271,10 @@ TEST_F(QuantizedMatMulTest, Small_WithParams) {\n                                                                3,\n                                                                6,\n                                                            });\n-  AddInputFromArray<float>(TensorShape({1}), {-12.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {243.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {0});\n-  AddInputFromArray<float>(TensorShape({1}), {255.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-12.0f});\n+  AddInputFromArray<float>(TensorShape({}), {243.0f});\n+  AddInputFromArray<float>(TensorShape({}), {0});\n+  AddInputFromArray<float>(TensorShape({}), {255.0f});\n   TF_ASSERT_OK(RunOpKernel());\n   // We're requesting C = A.transposed() * B,\n   // so we expect to get these results:\n@@ -326,10 +364,10 @@ TEST_F(QuantizedMatMulTest, Medium_WithParams) {\n \n   AddInputFromArray<quint8>(a_quantized.shape(), a_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(b_quantized.shape(), b_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {a_min});\n-  AddInputFromArray<float>(TensorShape({1}), {a_max});\n-  AddInputFromArray<float>(TensorShape({1}), {b_min});\n-  AddInputFromArray<float>(TensorShape({1}), {b_max});\n+  AddInputFromArray<float>(TensorShape({}), {a_min});\n+  AddInputFromArray<float>(TensorShape({}), {a_max});\n+  AddInputFromArray<float>(TensorShape({}), {b_min});\n+  AddInputFromArray<float>(TensorShape({}), {b_max});\n   TF_ASSERT_OK(RunOpKernel());\n \n   Tensor expected_float(DT_FLOAT, {a_cols, b_cols});\n"
        ],
        "Title": "\n          Segfault in `QuantizedMatMul`\n        "
    },
    {
        "Bug description": "If  FakeQuantWithMinMaxVarsPerChannel  is given  min  or  max  tensors of a rank other than one, it results in a  CHECK  fail that can be used to trigger a denial of service attack.",
        "Sample Code": "num_bits = 8\nnarrow_range = False\ninputs = tf.constant(0, shape=[4], dtype=tf.float32)\nmin = tf.constant([], shape=[4,0,0], dtype=tf.float32)\nmax = tf.constant(0, shape=[4], dtype=tf.float32)\n)\ntf.raw_ops.FakeQuantWithMinMaxVarsPerChannel(inputs=inputs, min=min, max=max, num_bits=num_bits, narrow_range=narrow_range)",
        "Bug fix": [
            "@@ -24,6 +24,7 @@ limitations under the License.\n // Above is the related header but clang tidy doesn't recognize it.\n #include \"tensorflow/core/framework/numeric_op.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n #include \"tensorflow/core/lib/monitoring/gauge.h\"\n #include \"tensorflow/core/platform/protobuf.h\"\n@@ -205,6 +206,13 @@ class FakeQuantWithMinMaxVarsOp : public OpKernel {\n     const Tensor& min = context->input(1);\n     const Tensor& max = context->input(2);\n \n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min.shape()),\n+        InvalidArgument(\"`min` must be rank 0 but is rank \", min.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max.shape()),\n+        InvalidArgument(\"`max` must be rank 0 but is rank \", max.dims()));\n+\n     Tensor* output;\n     OP_REQUIRES_OK(context,\n                    context->allocate_output(0, input.shape(), &output));\n@@ -342,10 +350,17 @@ class FakeQuantWithMinMaxVarsPerChannelOp : public OpKernel {\n     const Tensor& input = context->input(0);\n     const int depth = input.dim_size(input.dims() - 1);  // last dimension size.\n     const Tensor& min = context->input(1);\n+    const Tensor& max = context->input(2);\n+\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(min.shape()),\n+        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));\n     OP_REQUIRES(context, min.dim_size(0) == depth,\n                 InvalidArgument(\"min has incorrect size, expected \", depth,\n                                 \" was \", min.dim_size(0)));\n-    const Tensor& max = context->input(2);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(max.shape()),\n+        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));\n     OP_REQUIRES(context, max.dim_size(0) == depth,\n                 InvalidArgument(\"max has incorrect size, expected \", depth,\n                                 \" was \", max.dim_size(0)));\n",
            "@@ -20,6 +20,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/numeric_op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/meta_support.h\"\n #include \"tensorflow/core/kernels/ops_util.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n@@ -38,10 +39,30 @@ class QuantizedBiasAddOp : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n     const Tensor& bias = context->input(1);\n-    const float input_min = context->input(2).flat<float>()(0);\n-    const float input_max = context->input(3).flat<float>()(0);\n-    const float bias_min = context->input(4).flat<float>()(0);\n-    const float bias_max = context->input(5).flat<float>()(0);\n+\n+    const Tensor& min_input = context->input(2);\n+    const Tensor& max_input = context->input(3);\n+    const Tensor& min_bias = context->input(4);\n+    const Tensor& max_bias = context->input(5);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min_input.shape()),\n+        errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",\n+                                min_input.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max_input.shape()),\n+        errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",\n+                                max_input.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_bias.shape()),\n+                errors::InvalidArgument(\n+                    \"`min_bias` must be rank 0 but is rank \", min_bias.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_bias.shape()),\n+                errors::InvalidArgument(\n+                    \"`max_bias` must be rank 0 but is rank \", max_bias.dims()));\n+\n+    const float input_min = min_input.flat<float>()(0);\n+    const float input_max = max_input.flat<float>()(0);\n+    const float bias_min = min_bias.flat<float>()(0);\n+    const float bias_max = max_bias.flat<float>()(0);\n \n     OP_REQUIRES(context, TensorShapeUtils::IsMatrixOrHigher(input.shape()),\n                 errors::InvalidArgument(\"Input tensor must be at least 2D: \",\n",
            "@@ -74,10 +74,10 @@ TEST_F(QuantizedBiasAddTest, Small) {\n                             input_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(bias_quantized.shape(),\n                             bias_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_min});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {bias_min});\n+  AddInputFromArray<float>(TensorShape({}), {bias_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n@@ -156,10 +156,10 @@ TEST_F(QuantizedBiasAddTest, RealData) {\n                             input_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(bias_quantized.shape(),\n                             bias_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_min});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {bias_min});\n+  AddInputFromArray<float>(TensorShape({}), {bias_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n",
            "@@ -25,7 +25,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n-\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n \n #ifdef USE_NEON\n@@ -274,8 +274,16 @@ class QuantizedInstanceNorm : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n \n-    float input_min = context->input(1).flat<float>()(0);\n-    float input_max = context->input(2).flat<float>()(0);\n+    const Tensor& x_min = context->input(1);\n+    const Tensor& x_max = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_min.shape()),\n+                errors::InvalidArgument(\"`x_min` must be rank 0 but is rank \",\n+                                        x_min.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_max.shape()),\n+                errors::InvalidArgument(\"`x_max` must be rank 0 but is rank \",\n+                                        x_max.dims()));\n+    float input_min = x_min.scalar<float>()();\n+    float input_max = x_max.scalar<float>()();\n     float input_scale = (input_max - input_min) / 255.0f;\n \n     OP_REQUIRES(context, input_min < input_max,\n",
            "@@ -18,9 +18,11 @@ limitations under the License.\n #define EIGEN_USE_THREADS\n \n #include <math.h>\n-#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n #include \"tensorflow/core/framework/op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/type_traits.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/meta_support.h\"\n@@ -38,10 +40,34 @@ class RequantizeOp : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input = ctx->input(0);\n-    const float input_min_float = ctx->input(1).flat<float>()(0);\n-    const float input_max_float = ctx->input(2).flat<float>()(0);\n-    const float requested_output_min_float = ctx->input(3).flat<float>()(0);\n-    const float requested_output_max_float = ctx->input(4).flat<float>()(0);\n+\n+    const Tensor& input_min = ctx->input(1);\n+    const Tensor& input_max = ctx->input(2);\n+    const Tensor& requested_output_min = ctx->input(3);\n+    const Tensor& requested_output_max = ctx->input(4);\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsScalar(input_min.shape()),\n+        errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",\n+                                input_min.dims()));\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsScalar(input_max.shape()),\n+        errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",\n+                                input_max.dims()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_min.shape()),\n+                errors::InvalidArgument(\n+                    \"`requested_output_min` must be rank 0 but is rank \",\n+                    requested_output_min.dims()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_max.shape()),\n+                errors::InvalidArgument(\n+                    \"`requested_output_max` must be rank 0 but is rank \",\n+                    requested_output_max.dims()));\n+\n+    const float input_min_float = input_min.flat<float>()(0);\n+    const float input_max_float = input_max.flat<float>()(0);\n+    const float requested_output_min_float =\n+        requested_output_min.flat<float>()(0);\n+    const float requested_output_max_float =\n+        requested_output_max.flat<float>()(0);\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n",
            "@@ -53,10 +53,10 @@ TEST_F(RequantizeTest, HandCraftedRequantize) {\n   // Requantize to -1 to 1.\n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-1.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {1.0f});\n   TF_ASSERT_OK(RunOpKernel());\n   Tensor expected(allocator(), DT_QUINT8, TensorShape({value_count}));\n   test::FillValues<quint8>(&expected, {0, 128, 255});\n@@ -71,10 +71,10 @@ TEST_F(RequantizeTest, InvalidOutputMin) {\n \n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {0.01f});\n-  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {0.01f});\n+  AddInputFromArray<float>(TensorShape({}), {1.0f});\n   EXPECT_EQ(\"requested_output_min must be <= 0, but got 0.01\",\n             RunOpKernel().error_message());\n }\n@@ -85,10 +85,10 @@ TEST_F(RequantizeTest, InvalidOutputMax) {\n \n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-10.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-11.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-10.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-11.0f});\n   EXPECT_EQ(\n       \"requested_output_max must be >= requested_output_min, but got -11 and \"\n       \"-10\",\n",
            "@@ -0,0 +1,24 @@\n+# Tests of TensorFlow quantization ops written using the Python API.\n+\n+# buildifier: disable=same-origin-load\n+load(\"//tensorflow:tensorflow.bzl\", \"tf_py_test\")\n+\n+package(\n+    default_visibility = [\"//tensorflow:internal\"],\n+    licenses = [\"notice\"],\n+)\n+\n+tf_py_test(\n+    name = \"quantization_ops_test\",\n+    size = \"small\",\n+    srcs = [\"quantization_ops_test.py\"],\n+    deps = [\n+        \"//tensorflow/python:array_ops\",\n+        \"//tensorflow/python:client\",\n+        \"//tensorflow/python:client_testlib\",\n+        \"//tensorflow/python:framework\",\n+        \"//tensorflow/python:framework_for_generated_wrappers\",\n+        \"//tensorflow/python:math_ops\",\n+        \"//third_party/py/numpy\",\n+    ],\n+)\n",
            "@@ -0,0 +1,210 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for tf.quantize ops.\"\"\"\n+import numpy as np\n+\n+from tensorflow.python.framework import constant_op\n+from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n+from tensorflow.python.framework import test_util\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import nn_ops\n+from tensorflow.python.platform import googletest\n+\n+\n+class FakeQuantWithMinMaxVarsOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars(\n+              inputs=inputs, min=0.0, max=[[1.0], [2.0], [4.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars(\n+              inputs=inputs, min=[[1.0], [2.0], [4.0]], max=1.0))\n+\n+\n+class FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 1\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[[0.0]], max=[1.0]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Dimensions must be equal|incorrect size\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[0.0, 0.1], max=[1.0]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 1\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[1.0], max=[[1.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Dimensions must be equal|incorrect size\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[0.0], max=[1.0, 1.1]))\n+\n+\n+class QuantizedBiasedAddTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.qint8)\n+    bias = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.qint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=[],\n+              max_input=1.0,\n+              min_bias=0.0,\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=[],\n+              min_bias=0.0,\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=1.0,\n+              min_bias=[],\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=1.0,\n+              min_bias=0.0,\n+              max_bias=[],\n+              out_type=dtypes.qint32))\n+\n+\n+class QuantizedInstanceNormOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.quantized_instance_norm(\n+              x=inputs, x_min=0.0, x_max=[[1.0], [2.0], [4.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.quantized_instance_norm(\n+              x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))\n+\n+\n+class RequantizeOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=[],\n+              input_max=1.0,\n+              requested_output_min=0.0,\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=[],\n+              requested_output_min=0.0,\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=1.0,\n+              requested_output_min=[],\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=1.0,\n+              requested_output_min=0.0,\n+              requested_output_max=[],\n+              out_type=dtypes.qint8))\n+\n+\n+if __name__ == \"__main__\":\n+  googletest.main()\n"
        ],
        "Title": "\n          `CHECK` fail in `FakeQuantWithMinMaxVarsPerChannel`\n        "
    },
    {
        "Bug description": "If  QuantizedBiasAdd  is given  min_input ,  max_input ,  min_bias ,  max_bias  tensors of a nonzero rank, it results in a segfault that can be used to trigger a denial of service attack.",
        "Sample Code": "out_type = tf.qint32\ninput = tf.constant([85,170,255], shape=[3], dtype=tf.quint8)\nbias = tf.constant(43, shape=[2,3], dtype=tf.quint8)\nmin_input = tf.constant([], shape=[0], dtype=tf.float32)\nmax_input = tf.constant(0, shape=[1], dtype=tf.float32)\nmin_bias = tf.constant(0, shape=[1], dtype=tf.float32)\nmax_bias = tf.constant(0, shape=[1], dtype=tf.float32)\n)\ntf.raw_ops.QuantizedBiasAdd(input=input, bias=bias, min_input=min_input, max_input=max_input, min_bias=min_bias, max_bias=max_bias, out_type=out_type)",
        "Bug fix": [
            "@@ -24,6 +24,7 @@ limitations under the License.\n // Above is the related header but clang tidy doesn't recognize it.\n #include \"tensorflow/core/framework/numeric_op.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n #include \"tensorflow/core/lib/monitoring/gauge.h\"\n #include \"tensorflow/core/platform/protobuf.h\"\n@@ -205,6 +206,13 @@ class FakeQuantWithMinMaxVarsOp : public OpKernel {\n     const Tensor& min = context->input(1);\n     const Tensor& max = context->input(2);\n \n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min.shape()),\n+        InvalidArgument(\"`min` must be rank 0 but is rank \", min.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max.shape()),\n+        InvalidArgument(\"`max` must be rank 0 but is rank \", max.dims()));\n+\n     Tensor* output;\n     OP_REQUIRES_OK(context,\n                    context->allocate_output(0, input.shape(), &output));\n@@ -342,10 +350,17 @@ class FakeQuantWithMinMaxVarsPerChannelOp : public OpKernel {\n     const Tensor& input = context->input(0);\n     const int depth = input.dim_size(input.dims() - 1);  // last dimension size.\n     const Tensor& min = context->input(1);\n+    const Tensor& max = context->input(2);\n+\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(min.shape()),\n+        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));\n     OP_REQUIRES(context, min.dim_size(0) == depth,\n                 InvalidArgument(\"min has incorrect size, expected \", depth,\n                                 \" was \", min.dim_size(0)));\n-    const Tensor& max = context->input(2);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(max.shape()),\n+        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));\n     OP_REQUIRES(context, max.dim_size(0) == depth,\n                 InvalidArgument(\"max has incorrect size, expected \", depth,\n                                 \" was \", max.dim_size(0)));\n",
            "@@ -20,6 +20,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/numeric_op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/meta_support.h\"\n #include \"tensorflow/core/kernels/ops_util.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n@@ -38,10 +39,30 @@ class QuantizedBiasAddOp : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n     const Tensor& bias = context->input(1);\n-    const float input_min = context->input(2).flat<float>()(0);\n-    const float input_max = context->input(3).flat<float>()(0);\n-    const float bias_min = context->input(4).flat<float>()(0);\n-    const float bias_max = context->input(5).flat<float>()(0);\n+\n+    const Tensor& min_input = context->input(2);\n+    const Tensor& max_input = context->input(3);\n+    const Tensor& min_bias = context->input(4);\n+    const Tensor& max_bias = context->input(5);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min_input.shape()),\n+        errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",\n+                                min_input.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max_input.shape()),\n+        errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",\n+                                max_input.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_bias.shape()),\n+                errors::InvalidArgument(\n+                    \"`min_bias` must be rank 0 but is rank \", min_bias.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_bias.shape()),\n+                errors::InvalidArgument(\n+                    \"`max_bias` must be rank 0 but is rank \", max_bias.dims()));\n+\n+    const float input_min = min_input.flat<float>()(0);\n+    const float input_max = max_input.flat<float>()(0);\n+    const float bias_min = min_bias.flat<float>()(0);\n+    const float bias_max = max_bias.flat<float>()(0);\n \n     OP_REQUIRES(context, TensorShapeUtils::IsMatrixOrHigher(input.shape()),\n                 errors::InvalidArgument(\"Input tensor must be at least 2D: \",\n",
            "@@ -74,10 +74,10 @@ TEST_F(QuantizedBiasAddTest, Small) {\n                             input_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(bias_quantized.shape(),\n                             bias_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_min});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {bias_min});\n+  AddInputFromArray<float>(TensorShape({}), {bias_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n@@ -156,10 +156,10 @@ TEST_F(QuantizedBiasAddTest, RealData) {\n                             input_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(bias_quantized.shape(),\n                             bias_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_min});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {bias_min});\n+  AddInputFromArray<float>(TensorShape({}), {bias_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n",
            "@@ -25,7 +25,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n-\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n \n #ifdef USE_NEON\n@@ -274,8 +274,16 @@ class QuantizedInstanceNorm : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n \n-    float input_min = context->input(1).flat<float>()(0);\n-    float input_max = context->input(2).flat<float>()(0);\n+    const Tensor& x_min = context->input(1);\n+    const Tensor& x_max = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_min.shape()),\n+                errors::InvalidArgument(\"`x_min` must be rank 0 but is rank \",\n+                                        x_min.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_max.shape()),\n+                errors::InvalidArgument(\"`x_max` must be rank 0 but is rank \",\n+                                        x_max.dims()));\n+    float input_min = x_min.scalar<float>()();\n+    float input_max = x_max.scalar<float>()();\n     float input_scale = (input_max - input_min) / 255.0f;\n \n     OP_REQUIRES(context, input_min < input_max,\n",
            "@@ -18,9 +18,11 @@ limitations under the License.\n #define EIGEN_USE_THREADS\n \n #include <math.h>\n-#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n #include \"tensorflow/core/framework/op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/type_traits.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/meta_support.h\"\n@@ -38,10 +40,34 @@ class RequantizeOp : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input = ctx->input(0);\n-    const float input_min_float = ctx->input(1).flat<float>()(0);\n-    const float input_max_float = ctx->input(2).flat<float>()(0);\n-    const float requested_output_min_float = ctx->input(3).flat<float>()(0);\n-    const float requested_output_max_float = ctx->input(4).flat<float>()(0);\n+\n+    const Tensor& input_min = ctx->input(1);\n+    const Tensor& input_max = ctx->input(2);\n+    const Tensor& requested_output_min = ctx->input(3);\n+    const Tensor& requested_output_max = ctx->input(4);\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsScalar(input_min.shape()),\n+        errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",\n+                                input_min.dims()));\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsScalar(input_max.shape()),\n+        errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",\n+                                input_max.dims()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_min.shape()),\n+                errors::InvalidArgument(\n+                    \"`requested_output_min` must be rank 0 but is rank \",\n+                    requested_output_min.dims()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_max.shape()),\n+                errors::InvalidArgument(\n+                    \"`requested_output_max` must be rank 0 but is rank \",\n+                    requested_output_max.dims()));\n+\n+    const float input_min_float = input_min.flat<float>()(0);\n+    const float input_max_float = input_max.flat<float>()(0);\n+    const float requested_output_min_float =\n+        requested_output_min.flat<float>()(0);\n+    const float requested_output_max_float =\n+        requested_output_max.flat<float>()(0);\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n",
            "@@ -53,10 +53,10 @@ TEST_F(RequantizeTest, HandCraftedRequantize) {\n   // Requantize to -1 to 1.\n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-1.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {1.0f});\n   TF_ASSERT_OK(RunOpKernel());\n   Tensor expected(allocator(), DT_QUINT8, TensorShape({value_count}));\n   test::FillValues<quint8>(&expected, {0, 128, 255});\n@@ -71,10 +71,10 @@ TEST_F(RequantizeTest, InvalidOutputMin) {\n \n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {0.01f});\n-  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {0.01f});\n+  AddInputFromArray<float>(TensorShape({}), {1.0f});\n   EXPECT_EQ(\"requested_output_min must be <= 0, but got 0.01\",\n             RunOpKernel().error_message());\n }\n@@ -85,10 +85,10 @@ TEST_F(RequantizeTest, InvalidOutputMax) {\n \n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-10.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-11.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-10.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-11.0f});\n   EXPECT_EQ(\n       \"requested_output_max must be >= requested_output_min, but got -11 and \"\n       \"-10\",\n",
            "@@ -0,0 +1,24 @@\n+# Tests of TensorFlow quantization ops written using the Python API.\n+\n+# buildifier: disable=same-origin-load\n+load(\"//tensorflow:tensorflow.bzl\", \"tf_py_test\")\n+\n+package(\n+    default_visibility = [\"//tensorflow:internal\"],\n+    licenses = [\"notice\"],\n+)\n+\n+tf_py_test(\n+    name = \"quantization_ops_test\",\n+    size = \"small\",\n+    srcs = [\"quantization_ops_test.py\"],\n+    deps = [\n+        \"//tensorflow/python:array_ops\",\n+        \"//tensorflow/python:client\",\n+        \"//tensorflow/python:client_testlib\",\n+        \"//tensorflow/python:framework\",\n+        \"//tensorflow/python:framework_for_generated_wrappers\",\n+        \"//tensorflow/python:math_ops\",\n+        \"//third_party/py/numpy\",\n+    ],\n+)\n",
            "@@ -0,0 +1,210 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for tf.quantize ops.\"\"\"\n+import numpy as np\n+\n+from tensorflow.python.framework import constant_op\n+from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n+from tensorflow.python.framework import test_util\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import nn_ops\n+from tensorflow.python.platform import googletest\n+\n+\n+class FakeQuantWithMinMaxVarsOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars(\n+              inputs=inputs, min=0.0, max=[[1.0], [2.0], [4.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars(\n+              inputs=inputs, min=[[1.0], [2.0], [4.0]], max=1.0))\n+\n+\n+class FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 1\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[[0.0]], max=[1.0]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Dimensions must be equal|incorrect size\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[0.0, 0.1], max=[1.0]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 1\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[1.0], max=[[1.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Dimensions must be equal|incorrect size\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[0.0], max=[1.0, 1.1]))\n+\n+\n+class QuantizedBiasedAddTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.qint8)\n+    bias = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.qint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=[],\n+              max_input=1.0,\n+              min_bias=0.0,\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=[],\n+              min_bias=0.0,\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=1.0,\n+              min_bias=[],\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=1.0,\n+              min_bias=0.0,\n+              max_bias=[],\n+              out_type=dtypes.qint32))\n+\n+\n+class QuantizedInstanceNormOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.quantized_instance_norm(\n+              x=inputs, x_min=0.0, x_max=[[1.0], [2.0], [4.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.quantized_instance_norm(\n+              x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))\n+\n+\n+class RequantizeOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=[],\n+              input_max=1.0,\n+              requested_output_min=0.0,\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=[],\n+              requested_output_min=0.0,\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=1.0,\n+              requested_output_min=[],\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=1.0,\n+              requested_output_min=0.0,\n+              requested_output_max=[],\n+              out_type=dtypes.qint8))\n+\n+\n+if __name__ == \"__main__\":\n+  googletest.main()\n"
        ],
        "Title": "\n          Segfault in `QuantizedBiasAdd`\n        "
    },
    {
        "Bug description": "If  Requantize  is given  input_min ,  input_max ,  requested_output_min ,  requested_output_max  tensors of a nonzero rank, it results in a segfault that can be used to trigger a denial of service attack.",
        "Sample Code": "out_type = tf.quint8\ninput = tf.constant([1], shape=[3], dtype=tf.qint32)\ninput_min = tf.constant([], shape=[0], dtype=tf.float32)\ninput_max = tf.constant(-256, shape=[1], dtype=tf.float32)\nrequested_output_min = tf.constant(-256, shape=[1], dtype=tf.float32)\nrequested_output_max = tf.constant(-256, shape=[1], dtype=tf.float32)\n)\ntf.raw_ops.Requantize(input=input, input_min=input_min, input_max=input_max, requested_output_min=requested_output_min, requested_output_max=requested_output_max, out_type=out_type)",
        "Bug fix": [
            "@@ -24,6 +24,7 @@ limitations under the License.\n // Above is the related header but clang tidy doesn't recognize it.\n #include \"tensorflow/core/framework/numeric_op.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n #include \"tensorflow/core/lib/monitoring/gauge.h\"\n #include \"tensorflow/core/platform/protobuf.h\"\n@@ -205,6 +206,13 @@ class FakeQuantWithMinMaxVarsOp : public OpKernel {\n     const Tensor& min = context->input(1);\n     const Tensor& max = context->input(2);\n \n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min.shape()),\n+        InvalidArgument(\"`min` must be rank 0 but is rank \", min.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max.shape()),\n+        InvalidArgument(\"`max` must be rank 0 but is rank \", max.dims()));\n+\n     Tensor* output;\n     OP_REQUIRES_OK(context,\n                    context->allocate_output(0, input.shape(), &output));\n@@ -342,10 +350,17 @@ class FakeQuantWithMinMaxVarsPerChannelOp : public OpKernel {\n     const Tensor& input = context->input(0);\n     const int depth = input.dim_size(input.dims() - 1);  // last dimension size.\n     const Tensor& min = context->input(1);\n+    const Tensor& max = context->input(2);\n+\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(min.shape()),\n+        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));\n     OP_REQUIRES(context, min.dim_size(0) == depth,\n                 InvalidArgument(\"min has incorrect size, expected \", depth,\n                                 \" was \", min.dim_size(0)));\n-    const Tensor& max = context->input(2);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(max.shape()),\n+        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));\n     OP_REQUIRES(context, max.dim_size(0) == depth,\n                 InvalidArgument(\"max has incorrect size, expected \", depth,\n                                 \" was \", max.dim_size(0)));\n",
            "@@ -20,6 +20,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/numeric_op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/meta_support.h\"\n #include \"tensorflow/core/kernels/ops_util.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n@@ -38,10 +39,30 @@ class QuantizedBiasAddOp : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n     const Tensor& bias = context->input(1);\n-    const float input_min = context->input(2).flat<float>()(0);\n-    const float input_max = context->input(3).flat<float>()(0);\n-    const float bias_min = context->input(4).flat<float>()(0);\n-    const float bias_max = context->input(5).flat<float>()(0);\n+\n+    const Tensor& min_input = context->input(2);\n+    const Tensor& max_input = context->input(3);\n+    const Tensor& min_bias = context->input(4);\n+    const Tensor& max_bias = context->input(5);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min_input.shape()),\n+        errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",\n+                                min_input.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max_input.shape()),\n+        errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",\n+                                max_input.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_bias.shape()),\n+                errors::InvalidArgument(\n+                    \"`min_bias` must be rank 0 but is rank \", min_bias.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_bias.shape()),\n+                errors::InvalidArgument(\n+                    \"`max_bias` must be rank 0 but is rank \", max_bias.dims()));\n+\n+    const float input_min = min_input.flat<float>()(0);\n+    const float input_max = max_input.flat<float>()(0);\n+    const float bias_min = min_bias.flat<float>()(0);\n+    const float bias_max = max_bias.flat<float>()(0);\n \n     OP_REQUIRES(context, TensorShapeUtils::IsMatrixOrHigher(input.shape()),\n                 errors::InvalidArgument(\"Input tensor must be at least 2D: \",\n",
            "@@ -74,10 +74,10 @@ TEST_F(QuantizedBiasAddTest, Small) {\n                             input_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(bias_quantized.shape(),\n                             bias_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_min});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {bias_min});\n+  AddInputFromArray<float>(TensorShape({}), {bias_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n@@ -156,10 +156,10 @@ TEST_F(QuantizedBiasAddTest, RealData) {\n                             input_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(bias_quantized.shape(),\n                             bias_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_min});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {bias_min});\n+  AddInputFromArray<float>(TensorShape({}), {bias_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n",
            "@@ -25,7 +25,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n-\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n \n #ifdef USE_NEON\n@@ -274,8 +274,16 @@ class QuantizedInstanceNorm : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n \n-    float input_min = context->input(1).flat<float>()(0);\n-    float input_max = context->input(2).flat<float>()(0);\n+    const Tensor& x_min = context->input(1);\n+    const Tensor& x_max = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_min.shape()),\n+                errors::InvalidArgument(\"`x_min` must be rank 0 but is rank \",\n+                                        x_min.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_max.shape()),\n+                errors::InvalidArgument(\"`x_max` must be rank 0 but is rank \",\n+                                        x_max.dims()));\n+    float input_min = x_min.scalar<float>()();\n+    float input_max = x_max.scalar<float>()();\n     float input_scale = (input_max - input_min) / 255.0f;\n \n     OP_REQUIRES(context, input_min < input_max,\n",
            "@@ -18,9 +18,11 @@ limitations under the License.\n #define EIGEN_USE_THREADS\n \n #include <math.h>\n-#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n #include \"tensorflow/core/framework/op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/type_traits.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/meta_support.h\"\n@@ -38,10 +40,34 @@ class RequantizeOp : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input = ctx->input(0);\n-    const float input_min_float = ctx->input(1).flat<float>()(0);\n-    const float input_max_float = ctx->input(2).flat<float>()(0);\n-    const float requested_output_min_float = ctx->input(3).flat<float>()(0);\n-    const float requested_output_max_float = ctx->input(4).flat<float>()(0);\n+\n+    const Tensor& input_min = ctx->input(1);\n+    const Tensor& input_max = ctx->input(2);\n+    const Tensor& requested_output_min = ctx->input(3);\n+    const Tensor& requested_output_max = ctx->input(4);\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsScalar(input_min.shape()),\n+        errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",\n+                                input_min.dims()));\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsScalar(input_max.shape()),\n+        errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",\n+                                input_max.dims()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_min.shape()),\n+                errors::InvalidArgument(\n+                    \"`requested_output_min` must be rank 0 but is rank \",\n+                    requested_output_min.dims()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_max.shape()),\n+                errors::InvalidArgument(\n+                    \"`requested_output_max` must be rank 0 but is rank \",\n+                    requested_output_max.dims()));\n+\n+    const float input_min_float = input_min.flat<float>()(0);\n+    const float input_max_float = input_max.flat<float>()(0);\n+    const float requested_output_min_float =\n+        requested_output_min.flat<float>()(0);\n+    const float requested_output_max_float =\n+        requested_output_max.flat<float>()(0);\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n",
            "@@ -53,10 +53,10 @@ TEST_F(RequantizeTest, HandCraftedRequantize) {\n   // Requantize to -1 to 1.\n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-1.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {1.0f});\n   TF_ASSERT_OK(RunOpKernel());\n   Tensor expected(allocator(), DT_QUINT8, TensorShape({value_count}));\n   test::FillValues<quint8>(&expected, {0, 128, 255});\n@@ -71,10 +71,10 @@ TEST_F(RequantizeTest, InvalidOutputMin) {\n \n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {0.01f});\n-  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {0.01f});\n+  AddInputFromArray<float>(TensorShape({}), {1.0f});\n   EXPECT_EQ(\"requested_output_min must be <= 0, but got 0.01\",\n             RunOpKernel().error_message());\n }\n@@ -85,10 +85,10 @@ TEST_F(RequantizeTest, InvalidOutputMax) {\n \n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-10.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-11.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-10.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-11.0f});\n   EXPECT_EQ(\n       \"requested_output_max must be >= requested_output_min, but got -11 and \"\n       \"-10\",\n",
            "@@ -0,0 +1,24 @@\n+# Tests of TensorFlow quantization ops written using the Python API.\n+\n+# buildifier: disable=same-origin-load\n+load(\"//tensorflow:tensorflow.bzl\", \"tf_py_test\")\n+\n+package(\n+    default_visibility = [\"//tensorflow:internal\"],\n+    licenses = [\"notice\"],\n+)\n+\n+tf_py_test(\n+    name = \"quantization_ops_test\",\n+    size = \"small\",\n+    srcs = [\"quantization_ops_test.py\"],\n+    deps = [\n+        \"//tensorflow/python:array_ops\",\n+        \"//tensorflow/python:client\",\n+        \"//tensorflow/python:client_testlib\",\n+        \"//tensorflow/python:framework\",\n+        \"//tensorflow/python:framework_for_generated_wrappers\",\n+        \"//tensorflow/python:math_ops\",\n+        \"//third_party/py/numpy\",\n+    ],\n+)\n",
            "@@ -0,0 +1,210 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for tf.quantize ops.\"\"\"\n+import numpy as np\n+\n+from tensorflow.python.framework import constant_op\n+from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n+from tensorflow.python.framework import test_util\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import nn_ops\n+from tensorflow.python.platform import googletest\n+\n+\n+class FakeQuantWithMinMaxVarsOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars(\n+              inputs=inputs, min=0.0, max=[[1.0], [2.0], [4.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars(\n+              inputs=inputs, min=[[1.0], [2.0], [4.0]], max=1.0))\n+\n+\n+class FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 1\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[[0.0]], max=[1.0]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Dimensions must be equal|incorrect size\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[0.0, 0.1], max=[1.0]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 1\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[1.0], max=[[1.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Dimensions must be equal|incorrect size\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[0.0], max=[1.0, 1.1]))\n+\n+\n+class QuantizedBiasedAddTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.qint8)\n+    bias = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.qint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=[],\n+              max_input=1.0,\n+              min_bias=0.0,\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=[],\n+              min_bias=0.0,\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=1.0,\n+              min_bias=[],\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=1.0,\n+              min_bias=0.0,\n+              max_bias=[],\n+              out_type=dtypes.qint32))\n+\n+\n+class QuantizedInstanceNormOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.quantized_instance_norm(\n+              x=inputs, x_min=0.0, x_max=[[1.0], [2.0], [4.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.quantized_instance_norm(\n+              x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))\n+\n+\n+class RequantizeOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=[],\n+              input_max=1.0,\n+              requested_output_min=0.0,\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=[],\n+              requested_output_min=0.0,\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=1.0,\n+              requested_output_min=[],\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=1.0,\n+              requested_output_min=0.0,\n+              requested_output_max=[],\n+              out_type=dtypes.qint8))\n+\n+\n+if __name__ == \"__main__\":\n+  googletest.main()\n"
        ],
        "Title": "\n          Segfault in `Requantize`\n        "
    },
    {
        "Bug description": "If  FakeQuantWithMinMaxVars  is given  min  or  max  tensors of a nonzero rank, it results in a  CHECK  fail that can be used to trigger a denial of service attack.",
        "Sample Code": "num_bits = 8\nnarrow_range = False\ninputs = tf.constant(0, shape=[2,3], dtype=tf.float32)\nmin = tf.constant(0, shape=[2,3], dtype=tf.float32)\nmax = tf.constant(0, shape=[2,3], dtype=tf.float32)\n)\ntf.raw_ops.FakeQuantWithMinMaxVars(inputs=inputs, min=min, max=max, num_bits=num_bits, narrow_range=narrow_range)",
        "Bug fix": [
            "@@ -24,6 +24,7 @@ limitations under the License.\n // Above is the related header but clang tidy doesn't recognize it.\n #include \"tensorflow/core/framework/numeric_op.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n #include \"tensorflow/core/lib/monitoring/gauge.h\"\n #include \"tensorflow/core/platform/protobuf.h\"\n@@ -205,6 +206,13 @@ class FakeQuantWithMinMaxVarsOp : public OpKernel {\n     const Tensor& min = context->input(1);\n     const Tensor& max = context->input(2);\n \n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min.shape()),\n+        InvalidArgument(\"`min` must be rank 0 but is rank \", min.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max.shape()),\n+        InvalidArgument(\"`max` must be rank 0 but is rank \", max.dims()));\n+\n     Tensor* output;\n     OP_REQUIRES_OK(context,\n                    context->allocate_output(0, input.shape(), &output));\n@@ -342,10 +350,17 @@ class FakeQuantWithMinMaxVarsPerChannelOp : public OpKernel {\n     const Tensor& input = context->input(0);\n     const int depth = input.dim_size(input.dims() - 1);  // last dimension size.\n     const Tensor& min = context->input(1);\n+    const Tensor& max = context->input(2);\n+\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(min.shape()),\n+        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));\n     OP_REQUIRES(context, min.dim_size(0) == depth,\n                 InvalidArgument(\"min has incorrect size, expected \", depth,\n                                 \" was \", min.dim_size(0)));\n-    const Tensor& max = context->input(2);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(max.shape()),\n+        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));\n     OP_REQUIRES(context, max.dim_size(0) == depth,\n                 InvalidArgument(\"max has incorrect size, expected \", depth,\n                                 \" was \", max.dim_size(0)));\n",
            "@@ -20,6 +20,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/numeric_op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/meta_support.h\"\n #include \"tensorflow/core/kernels/ops_util.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n@@ -38,10 +39,30 @@ class QuantizedBiasAddOp : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n     const Tensor& bias = context->input(1);\n-    const float input_min = context->input(2).flat<float>()(0);\n-    const float input_max = context->input(3).flat<float>()(0);\n-    const float bias_min = context->input(4).flat<float>()(0);\n-    const float bias_max = context->input(5).flat<float>()(0);\n+\n+    const Tensor& min_input = context->input(2);\n+    const Tensor& max_input = context->input(3);\n+    const Tensor& min_bias = context->input(4);\n+    const Tensor& max_bias = context->input(5);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min_input.shape()),\n+        errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",\n+                                min_input.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max_input.shape()),\n+        errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",\n+                                max_input.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_bias.shape()),\n+                errors::InvalidArgument(\n+                    \"`min_bias` must be rank 0 but is rank \", min_bias.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_bias.shape()),\n+                errors::InvalidArgument(\n+                    \"`max_bias` must be rank 0 but is rank \", max_bias.dims()));\n+\n+    const float input_min = min_input.flat<float>()(0);\n+    const float input_max = max_input.flat<float>()(0);\n+    const float bias_min = min_bias.flat<float>()(0);\n+    const float bias_max = max_bias.flat<float>()(0);\n \n     OP_REQUIRES(context, TensorShapeUtils::IsMatrixOrHigher(input.shape()),\n                 errors::InvalidArgument(\"Input tensor must be at least 2D: \",\n",
            "@@ -74,10 +74,10 @@ TEST_F(QuantizedBiasAddTest, Small) {\n                             input_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(bias_quantized.shape(),\n                             bias_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_min});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {bias_min});\n+  AddInputFromArray<float>(TensorShape({}), {bias_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n@@ -156,10 +156,10 @@ TEST_F(QuantizedBiasAddTest, RealData) {\n                             input_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(bias_quantized.shape(),\n                             bias_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_min});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {bias_min});\n+  AddInputFromArray<float>(TensorShape({}), {bias_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n",
            "@@ -25,7 +25,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n-\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n \n #ifdef USE_NEON\n@@ -274,8 +274,16 @@ class QuantizedInstanceNorm : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n \n-    float input_min = context->input(1).flat<float>()(0);\n-    float input_max = context->input(2).flat<float>()(0);\n+    const Tensor& x_min = context->input(1);\n+    const Tensor& x_max = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_min.shape()),\n+                errors::InvalidArgument(\"`x_min` must be rank 0 but is rank \",\n+                                        x_min.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_max.shape()),\n+                errors::InvalidArgument(\"`x_max` must be rank 0 but is rank \",\n+                                        x_max.dims()));\n+    float input_min = x_min.scalar<float>()();\n+    float input_max = x_max.scalar<float>()();\n     float input_scale = (input_max - input_min) / 255.0f;\n \n     OP_REQUIRES(context, input_min < input_max,\n",
            "@@ -18,9 +18,11 @@ limitations under the License.\n #define EIGEN_USE_THREADS\n \n #include <math.h>\n-#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n #include \"tensorflow/core/framework/op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/type_traits.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/meta_support.h\"\n@@ -38,10 +40,34 @@ class RequantizeOp : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input = ctx->input(0);\n-    const float input_min_float = ctx->input(1).flat<float>()(0);\n-    const float input_max_float = ctx->input(2).flat<float>()(0);\n-    const float requested_output_min_float = ctx->input(3).flat<float>()(0);\n-    const float requested_output_max_float = ctx->input(4).flat<float>()(0);\n+\n+    const Tensor& input_min = ctx->input(1);\n+    const Tensor& input_max = ctx->input(2);\n+    const Tensor& requested_output_min = ctx->input(3);\n+    const Tensor& requested_output_max = ctx->input(4);\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsScalar(input_min.shape()),\n+        errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",\n+                                input_min.dims()));\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsScalar(input_max.shape()),\n+        errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",\n+                                input_max.dims()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_min.shape()),\n+                errors::InvalidArgument(\n+                    \"`requested_output_min` must be rank 0 but is rank \",\n+                    requested_output_min.dims()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_max.shape()),\n+                errors::InvalidArgument(\n+                    \"`requested_output_max` must be rank 0 but is rank \",\n+                    requested_output_max.dims()));\n+\n+    const float input_min_float = input_min.flat<float>()(0);\n+    const float input_max_float = input_max.flat<float>()(0);\n+    const float requested_output_min_float =\n+        requested_output_min.flat<float>()(0);\n+    const float requested_output_max_float =\n+        requested_output_max.flat<float>()(0);\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n",
            "@@ -53,10 +53,10 @@ TEST_F(RequantizeTest, HandCraftedRequantize) {\n   // Requantize to -1 to 1.\n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-1.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {1.0f});\n   TF_ASSERT_OK(RunOpKernel());\n   Tensor expected(allocator(), DT_QUINT8, TensorShape({value_count}));\n   test::FillValues<quint8>(&expected, {0, 128, 255});\n@@ -71,10 +71,10 @@ TEST_F(RequantizeTest, InvalidOutputMin) {\n \n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {0.01f});\n-  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {0.01f});\n+  AddInputFromArray<float>(TensorShape({}), {1.0f});\n   EXPECT_EQ(\"requested_output_min must be <= 0, but got 0.01\",\n             RunOpKernel().error_message());\n }\n@@ -85,10 +85,10 @@ TEST_F(RequantizeTest, InvalidOutputMax) {\n \n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-10.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-11.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-10.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-11.0f});\n   EXPECT_EQ(\n       \"requested_output_max must be >= requested_output_min, but got -11 and \"\n       \"-10\",\n",
            "@@ -0,0 +1,24 @@\n+# Tests of TensorFlow quantization ops written using the Python API.\n+\n+# buildifier: disable=same-origin-load\n+load(\"//tensorflow:tensorflow.bzl\", \"tf_py_test\")\n+\n+package(\n+    default_visibility = [\"//tensorflow:internal\"],\n+    licenses = [\"notice\"],\n+)\n+\n+tf_py_test(\n+    name = \"quantization_ops_test\",\n+    size = \"small\",\n+    srcs = [\"quantization_ops_test.py\"],\n+    deps = [\n+        \"//tensorflow/python:array_ops\",\n+        \"//tensorflow/python:client\",\n+        \"//tensorflow/python:client_testlib\",\n+        \"//tensorflow/python:framework\",\n+        \"//tensorflow/python:framework_for_generated_wrappers\",\n+        \"//tensorflow/python:math_ops\",\n+        \"//third_party/py/numpy\",\n+    ],\n+)\n",
            "@@ -0,0 +1,210 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for tf.quantize ops.\"\"\"\n+import numpy as np\n+\n+from tensorflow.python.framework import constant_op\n+from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n+from tensorflow.python.framework import test_util\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import nn_ops\n+from tensorflow.python.platform import googletest\n+\n+\n+class FakeQuantWithMinMaxVarsOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars(\n+              inputs=inputs, min=0.0, max=[[1.0], [2.0], [4.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars(\n+              inputs=inputs, min=[[1.0], [2.0], [4.0]], max=1.0))\n+\n+\n+class FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 1\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[[0.0]], max=[1.0]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Dimensions must be equal|incorrect size\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[0.0, 0.1], max=[1.0]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 1\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[1.0], max=[[1.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Dimensions must be equal|incorrect size\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[0.0], max=[1.0, 1.1]))\n+\n+\n+class QuantizedBiasedAddTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.qint8)\n+    bias = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.qint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=[],\n+              max_input=1.0,\n+              min_bias=0.0,\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=[],\n+              min_bias=0.0,\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=1.0,\n+              min_bias=[],\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=1.0,\n+              min_bias=0.0,\n+              max_bias=[],\n+              out_type=dtypes.qint32))\n+\n+\n+class QuantizedInstanceNormOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.quantized_instance_norm(\n+              x=inputs, x_min=0.0, x_max=[[1.0], [2.0], [4.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.quantized_instance_norm(\n+              x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))\n+\n+\n+class RequantizeOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=[],\n+              input_max=1.0,\n+              requested_output_min=0.0,\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=[],\n+              requested_output_min=0.0,\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=1.0,\n+              requested_output_min=[],\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=1.0,\n+              requested_output_min=0.0,\n+              requested_output_max=[],\n+              out_type=dtypes.qint8))\n+\n+\n+if __name__ == \"__main__\":\n+  googletest.main()\n"
        ],
        "Title": "\n          `CHECK` fail in `FakeQuantWithMinMaxVars`\n        "
    },
    {
        "Bug description": "If  QuantizedInstanceNorm  is given  x_min  or  x_max  tensors of a nonzero rank, it results in a segfault that can be used to trigger a denial of service attack.",
        "Sample Code": "output_range_given = False\ngiven_y_min = 0\ngiven_y_max = 0\nvariance_epsilon = 1e-05\nmin_separation = 0.001\nx = tf.constant(88, shape=[1,4,4,32], dtype=tf.quint8)\nx_min = tf.constant([], shape=[0], dtype=tf.float32)\nx_max = tf.constant(0, shape=[], dtype=tf.float32)\n)\ntf.raw_ops.QuantizedInstanceNorm(x=x, x_min=x_min, x_max=x_max, output_range_given=output_range_given, given_y_min=given_y_min, given_y_max=given_y_max, variance_epsilon=variance_epsilon, min_separation=min_separation)",
        "Bug fix": [
            "@@ -24,6 +24,7 @@ limitations under the License.\n // Above is the related header but clang tidy doesn't recognize it.\n #include \"tensorflow/core/framework/numeric_op.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n #include \"tensorflow/core/lib/monitoring/gauge.h\"\n #include \"tensorflow/core/platform/protobuf.h\"\n@@ -205,6 +206,13 @@ class FakeQuantWithMinMaxVarsOp : public OpKernel {\n     const Tensor& min = context->input(1);\n     const Tensor& max = context->input(2);\n \n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min.shape()),\n+        InvalidArgument(\"`min` must be rank 0 but is rank \", min.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max.shape()),\n+        InvalidArgument(\"`max` must be rank 0 but is rank \", max.dims()));\n+\n     Tensor* output;\n     OP_REQUIRES_OK(context,\n                    context->allocate_output(0, input.shape(), &output));\n@@ -342,10 +350,17 @@ class FakeQuantWithMinMaxVarsPerChannelOp : public OpKernel {\n     const Tensor& input = context->input(0);\n     const int depth = input.dim_size(input.dims() - 1);  // last dimension size.\n     const Tensor& min = context->input(1);\n+    const Tensor& max = context->input(2);\n+\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(min.shape()),\n+        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));\n     OP_REQUIRES(context, min.dim_size(0) == depth,\n                 InvalidArgument(\"min has incorrect size, expected \", depth,\n                                 \" was \", min.dim_size(0)));\n-    const Tensor& max = context->input(2);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(max.shape()),\n+        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));\n     OP_REQUIRES(context, max.dim_size(0) == depth,\n                 InvalidArgument(\"max has incorrect size, expected \", depth,\n                                 \" was \", max.dim_size(0)));\n",
            "@@ -20,6 +20,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/numeric_op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/meta_support.h\"\n #include \"tensorflow/core/kernels/ops_util.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n@@ -38,10 +39,30 @@ class QuantizedBiasAddOp : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n     const Tensor& bias = context->input(1);\n-    const float input_min = context->input(2).flat<float>()(0);\n-    const float input_max = context->input(3).flat<float>()(0);\n-    const float bias_min = context->input(4).flat<float>()(0);\n-    const float bias_max = context->input(5).flat<float>()(0);\n+\n+    const Tensor& min_input = context->input(2);\n+    const Tensor& max_input = context->input(3);\n+    const Tensor& min_bias = context->input(4);\n+    const Tensor& max_bias = context->input(5);\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min_input.shape()),\n+        errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",\n+                                min_input.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max_input.shape()),\n+        errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",\n+                                max_input.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_bias.shape()),\n+                errors::InvalidArgument(\n+                    \"`min_bias` must be rank 0 but is rank \", min_bias.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_bias.shape()),\n+                errors::InvalidArgument(\n+                    \"`max_bias` must be rank 0 but is rank \", max_bias.dims()));\n+\n+    const float input_min = min_input.flat<float>()(0);\n+    const float input_max = max_input.flat<float>()(0);\n+    const float bias_min = min_bias.flat<float>()(0);\n+    const float bias_max = max_bias.flat<float>()(0);\n \n     OP_REQUIRES(context, TensorShapeUtils::IsMatrixOrHigher(input.shape()),\n                 errors::InvalidArgument(\"Input tensor must be at least 2D: \",\n",
            "@@ -74,10 +74,10 @@ TEST_F(QuantizedBiasAddTest, Small) {\n                             input_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(bias_quantized.shape(),\n                             bias_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_min});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {bias_min});\n+  AddInputFromArray<float>(TensorShape({}), {bias_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n@@ -156,10 +156,10 @@ TEST_F(QuantizedBiasAddTest, RealData) {\n                             input_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(bias_quantized.shape(),\n                             bias_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_min});\n-  AddInputFromArray<float>(TensorShape({1}), {bias_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {bias_min});\n+  AddInputFromArray<float>(TensorShape({}), {bias_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n",
            "@@ -25,7 +25,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n-\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n \n #ifdef USE_NEON\n@@ -274,8 +274,16 @@ class QuantizedInstanceNorm : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n \n-    float input_min = context->input(1).flat<float>()(0);\n-    float input_max = context->input(2).flat<float>()(0);\n+    const Tensor& x_min = context->input(1);\n+    const Tensor& x_max = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_min.shape()),\n+                errors::InvalidArgument(\"`x_min` must be rank 0 but is rank \",\n+                                        x_min.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_max.shape()),\n+                errors::InvalidArgument(\"`x_max` must be rank 0 but is rank \",\n+                                        x_max.dims()));\n+    float input_min = x_min.scalar<float>()();\n+    float input_max = x_max.scalar<float>()();\n     float input_scale = (input_max - input_min) / 255.0f;\n \n     OP_REQUIRES(context, input_min < input_max,\n",
            "@@ -18,9 +18,11 @@ limitations under the License.\n #define EIGEN_USE_THREADS\n \n #include <math.h>\n-#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n #include \"tensorflow/core/framework/op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/type_traits.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/meta_support.h\"\n@@ -38,10 +40,34 @@ class RequantizeOp : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input = ctx->input(0);\n-    const float input_min_float = ctx->input(1).flat<float>()(0);\n-    const float input_max_float = ctx->input(2).flat<float>()(0);\n-    const float requested_output_min_float = ctx->input(3).flat<float>()(0);\n-    const float requested_output_max_float = ctx->input(4).flat<float>()(0);\n+\n+    const Tensor& input_min = ctx->input(1);\n+    const Tensor& input_max = ctx->input(2);\n+    const Tensor& requested_output_min = ctx->input(3);\n+    const Tensor& requested_output_max = ctx->input(4);\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsScalar(input_min.shape()),\n+        errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",\n+                                input_min.dims()));\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsScalar(input_max.shape()),\n+        errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",\n+                                input_max.dims()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_min.shape()),\n+                errors::InvalidArgument(\n+                    \"`requested_output_min` must be rank 0 but is rank \",\n+                    requested_output_min.dims()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_max.shape()),\n+                errors::InvalidArgument(\n+                    \"`requested_output_max` must be rank 0 but is rank \",\n+                    requested_output_max.dims()));\n+\n+    const float input_min_float = input_min.flat<float>()(0);\n+    const float input_max_float = input_max.flat<float>()(0);\n+    const float requested_output_min_float =\n+        requested_output_min.flat<float>()(0);\n+    const float requested_output_max_float =\n+        requested_output_max.flat<float>()(0);\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n",
            "@@ -53,10 +53,10 @@ TEST_F(RequantizeTest, HandCraftedRequantize) {\n   // Requantize to -1 to 1.\n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-1.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {1.0f});\n   TF_ASSERT_OK(RunOpKernel());\n   Tensor expected(allocator(), DT_QUINT8, TensorShape({value_count}));\n   test::FillValues<quint8>(&expected, {0, 128, 255});\n@@ -71,10 +71,10 @@ TEST_F(RequantizeTest, InvalidOutputMin) {\n \n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {0.01f});\n-  AddInputFromArray<float>(TensorShape({1}), {1.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {0.01f});\n+  AddInputFromArray<float>(TensorShape({}), {1.0f});\n   EXPECT_EQ(\"requested_output_min must be <= 0, but got 0.01\",\n             RunOpKernel().error_message());\n }\n@@ -85,10 +85,10 @@ TEST_F(RequantizeTest, InvalidOutputMax) {\n \n   AddInputFromArray<qint32>(TensorShape({value_count}),\n                             {-(1 << 23), 0, (1 << 23)});\n-  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {256.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-10.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {-11.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {256.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-10.0f});\n+  AddInputFromArray<float>(TensorShape({}), {-11.0f});\n   EXPECT_EQ(\n       \"requested_output_max must be >= requested_output_min, but got -11 and \"\n       \"-10\",\n",
            "@@ -0,0 +1,24 @@\n+# Tests of TensorFlow quantization ops written using the Python API.\n+\n+# buildifier: disable=same-origin-load\n+load(\"//tensorflow:tensorflow.bzl\", \"tf_py_test\")\n+\n+package(\n+    default_visibility = [\"//tensorflow:internal\"],\n+    licenses = [\"notice\"],\n+)\n+\n+tf_py_test(\n+    name = \"quantization_ops_test\",\n+    size = \"small\",\n+    srcs = [\"quantization_ops_test.py\"],\n+    deps = [\n+        \"//tensorflow/python:array_ops\",\n+        \"//tensorflow/python:client\",\n+        \"//tensorflow/python:client_testlib\",\n+        \"//tensorflow/python:framework\",\n+        \"//tensorflow/python:framework_for_generated_wrappers\",\n+        \"//tensorflow/python:math_ops\",\n+        \"//third_party/py/numpy\",\n+    ],\n+)\n",
            "@@ -0,0 +1,210 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for tf.quantize ops.\"\"\"\n+import numpy as np\n+\n+from tensorflow.python.framework import constant_op\n+from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n+from tensorflow.python.framework import test_util\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import nn_ops\n+from tensorflow.python.platform import googletest\n+\n+\n+class FakeQuantWithMinMaxVarsOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars(\n+              inputs=inputs, min=0.0, max=[[1.0], [2.0], [4.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars(\n+              inputs=inputs, min=[[1.0], [2.0], [4.0]], max=1.0))\n+\n+\n+class FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 1\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[[0.0]], max=[1.0]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Dimensions must be equal|incorrect size\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[0.0, 0.1], max=[1.0]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 1\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[1.0], max=[[1.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Dimensions must be equal|incorrect size\"):\n+      self.evaluate(\n+          array_ops.fake_quant_with_min_max_vars_per_channel(\n+              inputs=inputs, min=[0.0], max=[1.0, 1.1]))\n+\n+\n+class QuantizedBiasedAddTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.qint8)\n+    bias = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.qint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=[],\n+              max_input=1.0,\n+              min_bias=0.0,\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=[],\n+              min_bias=0.0,\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=1.0,\n+              min_bias=[],\n+              max_bias=1.0,\n+              out_type=dtypes.qint32))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_bias_add(\n+              input=inputs,\n+              bias=bias,\n+              min_input=0.0,\n+              max_input=1.0,\n+              min_bias=0.0,\n+              max_bias=[],\n+              out_type=dtypes.qint32))\n+\n+\n+class QuantizedInstanceNormOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.quantized_instance_norm(\n+              x=inputs, x_min=0.0, x_max=[[1.0], [2.0], [4.0]]))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          array_ops.quantized_instance_norm(\n+              x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))\n+\n+\n+class RequantizeOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=[],\n+              input_max=1.0,\n+              requested_output_min=0.0,\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=[],\n+              requested_output_min=0.0,\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=1.0,\n+              requested_output_min=[],\n+              requested_output_max=1.0,\n+              out_type=dtypes.qint8))\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.requantize(\n+              input=inputs,\n+              input_min=0.0,\n+              input_max=1.0,\n+              requested_output_min=0.0,\n+              requested_output_max=[],\n+              out_type=dtypes.qint8))\n+\n+\n+if __name__ == \"__main__\":\n+  googletest.main()\n"
        ],
        "Title": "\n          Segfault in `QuantizedInstanceNorm`\n        "
    }
]
[
    {
        "Bug description": "The implementation of  Conv2DBackpropInput  requires  input_sizes  to be 4-dimensional. Otherwise, it gives a  CHECK  failure which can be used to trigger a denial of service attack:",
        "Sample Code": "strides = [1, 1, 1, 1]\npadding = \"SAME\"\nuse_cudnn_on_gpu = True\nexplicit_paddings = []\ndata_format = \"NHWC\"\ndilations = [1, 1, 1, 1]\ninput_sizes = tf.constant([65534,65534], shape=[2], dtype=tf.int32)\nfilter = tf.constant(0.159749106, shape=[3,3,2,2], dtype=tf.float32)\nout_backprop = tf.constant(0, shape=[], dtype=tf.float32)\n)\ntf.raw_ops.Conv2DBackpropInput(input_sizes=input_sizes, filter=filter, out_backprop=out_backprop, strides=strides, padding=padding, use_cudnn_on_gpu=use_cudnn_on_gpu, explicit_paddings=explicit_paddings, data_format=data_format, dilations=dilations)",
        "Bug fix": [
            "@@ -422,6 +422,11 @@ class Conv2DBackpropInputOp : public OpKernel {\n     const Tensor& filter = context->input(1);\n     const Tensor& out_backprop = context->input(2);\n \n+    OP_REQUIRES(\n+        context, out_backprop.dims() == 4,\n+        errors::InvalidArgument(\"input_sizes must be 4-dimensional, got: \",\n+                                out_backprop.dims()));\n+\n     TensorShape input_shape;\n     OP_REQUIRES_OK(context,\n                    Conv2DBackpropComputeInputShape(input_sizes, filter.shape(),\n@@ -527,6 +532,10 @@ class Conv2DCustomBackpropInputOp : public OpKernel {\n     const Tensor& input_sizes = context->input(0);\n     const Tensor& filter = context->input(1);\n     const Tensor& out_backprop = context->input(2);\n+    OP_REQUIRES(\n+        context, out_backprop.dims() == 4,\n+        errors::InvalidArgument(\"input_sizes must be 4-dimensional, got: \",\n+                                out_backprop.dims()));\n \n     TensorShape input_shape;\n     OP_REQUIRES_OK(context,\n",
            "@@ -32,6 +32,7 @@ from tensorflow.python.framework import test_util\n from tensorflow.python.layers import convolutional\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import gen_nn_ops\n from tensorflow.python.ops import gradient_checker\n from tensorflow.python.ops import gradients_impl\n from tensorflow.python.ops import math_ops\n@@ -1319,7 +1320,7 @@ class Conv2DTest(test.TestCase):\n     x2 = self._CreateNumpyTensor(filter_sizes)\n     default_dilations = (dilations[0] == 1 and dilations[1] == 1)\n     if default_dilations or use_gpu:\n-      with self.cached_session(use_gpu=use_gpu) as sess:\n+      with self.cached_session(use_gpu=use_gpu):\n         if data_format == \"NCHW\":\n           input_sizes = test_util.NHWCToNCHW(input_sizes)\n         t1 = constant_op.constant(x1, shape=input_sizes)\n@@ -1365,7 +1366,7 @@ class Conv2DTest(test.TestCase):\n     x2 = self._CreateNumpyTensor(filter_sizes)\n     default_dilations = (dilations[0] == 1 and dilations[1] == 1)\n     if default_dilations or use_gpu:\n-      with self.cached_session(use_gpu=use_gpu) as sess:\n+      with self.cached_session(use_gpu=use_gpu):\n         if data_format == \"NCHW\":\n           input_sizes = test_util.NHWCToNCHW(input_sizes)\n         t1 = constant_op.constant(x1, shape=input_sizes)\n@@ -2628,6 +2629,27 @@ class Conv2DTest(test.TestCase):\n               strides=[1, 1, 1, 1],\n               padding=[[0, 0], [-1, 0], [0, 0], [0, 0]]))\n \n+  def testConv2DBackpropInputInvalidOutBackpropRaiseError(self):\n+    with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):\n+      with self.cached_session():\n+        input_sizes = constant_op.constant([65534, 65534],\n+                                           shape=[2],\n+                                           dtype=dtypes.int32)\n+        filters = constant_op.constant(\n+            0.159749106, shape=[3, 3, 2, 2], dtype=dtypes.float32)\n+        out_backprop = constant_op.constant(0, shape=[], dtype=dtypes.float32)\n+        t = gen_nn_ops.conv2d_backprop_input(\n+            input_sizes=input_sizes,\n+            filter=filters,\n+            out_backprop=out_backprop,\n+            strides=[1, 1, 1, 1],\n+            padding=\"SAME\",\n+            use_cudnn_on_gpu=True,\n+            explicit_paddings=[],\n+            data_format=\"NHWC\",\n+            dilations=[1, 1, 1, 1])\n+        self.evaluate(t)\n+\n \n @test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\n class DepthwiseConv2DTest(test.TestCase):\n@@ -2655,7 +2677,7 @@ class DepthwiseConv2DTest(test.TestCase):\n     # numbers from 1.\n     x1 = [f * 1.0 for f in range(1, total_size_1 + 1)]\n     x2 = [f * 1.0 for f in range(1, total_size_2 + 1)]\n-    with self.cached_session() as sess:\n+    with self.cached_session():\n       t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n       t1.set_shape(tensor_in_sizes)\n       t2 = constant_op.constant(x2, shape=filter_in_sizes)\n@@ -2926,7 +2948,7 @@ class DeepConv2DTest(test.TestCase):\n     x1 = np.random.rand(*tensor_in_sizes).astype(np.float32)\n     x2 = np.random.rand(*filter_in_sizes).astype(np.float32)\n \n-    with self.cached_session(use_gpu=False) as sess:\n+    with self.cached_session(use_gpu=False):\n       t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n       t2 = constant_op.constant(x2, shape=filter_in_sizes)\n       strides = [1] + conv_strides + [1]\n"
        ],
        "Title": "\n          `CHECK` fail in `Conv2DBackpropInput`\n        "
    },
    {
        "Bug description": "The implementation of  AvgPoolGrad  does not fully validate the input  orig_input_shape . This results in a  CHECK  failure which can be used to trigger a denial of service attack:",
        "Sample Code": "ksize = [1, 2, 2, 1]\nstrides = [1, 2, 2, 1]\npadding = \"VALID\"\ndata_format = \"NHWC\"\norig_input_shape = tf.constant(-536870912, shape=[4], dtype=tf.int32)\ngrad = tf.constant(.0890338004362538, shape=[1,5,7,1], dtype=tf.float64)\n)\ntf.raw_ops.AvgPoolGrad(orig_input_shape=orig_input_shape, grad=grad, ksize=ksize, strides=strides, padding=padding, data_format=data_format)",
        "Bug fix": [
            "@@ -298,7 +298,7 @@ class AvgPoolingGradOp : public OpKernel {\n     TensorShape output_shape;\n     auto shape_vec = tensor_in_shape.vec<int32>();\n     for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {\n-      output_shape.AddDim(shape_vec(i));\n+      OP_REQUIRES_OK(context, output_shape.AddDimWithStatus(shape_vec(i)));\n     }\n     const int64_t in_rows = output_shape.dim_size(1);\n     const int64_t in_cols = output_shape.dim_size(2);\n@@ -457,7 +457,7 @@ class AvgPoolingGradOp<GPUDevice, T> : public OpKernel {\n     TensorShape output_shape;\n     auto shape_vec = tensor_in_shape.vec<int32>();\n     for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {\n-      output_shape.AddDim(shape_vec(i));\n+      OP_REQUIRES_OK(context, output_shape.AddDimWithStatus(shape_vec(i)));\n     }\n \n     if (output_shape.num_elements() == 0) {\n@@ -543,7 +543,7 @@ class AvgPoolingGradOpCustomGPUKernel : public OpKernel {\n     TensorShape output_shape;\n     auto shape_vec = tensor_in_shape.vec<int32>();\n     for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {\n-      output_shape.AddDim(shape_vec(i));\n+      OP_REQUIRES_OK(context, output_shape.AddDimWithStatus(shape_vec(i)));\n     }\n     if (output_shape.num_elements() == 0) {\n       Tensor* output = nullptr;\n",
            "@@ -2470,6 +2470,22 @@ class PoolingTest(test.TestCase, parameterized.TestCase):\n               inp, grad, argmax, ksize=[1, 1, 1, 1], strides=[1, 1, 1, 1],\n               padding=\"VALID\")\n \n+  def testAvgPoolGradInvalidInputShapeRaiseError(self):\n+    with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):\n+      with self.cached_session():\n+        orig_input_shape = constant_op.constant(\n+            -536870912, shape=[4], dtype=dtypes.int32)\n+        grad = constant_op.constant(\n+            .0890338004362538, shape=[1, 5, 7, 1], dtype=dtypes.float64)\n+        t = gen_nn_ops.AvgPoolGrad(\n+            orig_input_shape=orig_input_shape,\n+            grad=grad,\n+            ksize=[1, 2, 2, 1],\n+            strides=[1, 2, 2, 1],\n+            padding=\"VALID\",\n+            data_format=\"NHWC\")\n+        self.evaluate(t)\n+\n \n def GetMaxPoolFwdTest(input_size, filter_size, strides, padding):\n \n"
        ],
        "Title": "\n          `CHECK` fail in `AvgPoolGrad`\n        "
    },
    {
        "Bug description": "If  QuantizedAdd  is given  min_input  or  max_input  tensors of a nonzero rank, it results in a segfault that can be used to trigger a denial of service attack.",
        "Sample Code": "Toutput = tf.qint32\nx = tf.constant(140, shape=[1], dtype=tf.quint8)\ny = tf.constant(26, shape=[10], dtype=tf.quint8)\nmin_x = tf.constant([], shape=[0], dtype=tf.float32)\nmax_x = tf.constant(0, shape=[], dtype=tf.float32)\nmin_y = tf.constant(0, shape=[], dtype=tf.float32)\nmax_y = tf.constant(0, shape=[], dtype=tf.float32)\n)\ntf.raw_ops.QuantizedAdd(x=x, y=y, min_x=min_x, max_x=max_x, min_y=min_y, max_y=max_y, Toutput=Toutput)",
        "Bug fix": [
            "@@ -32,8 +32,21 @@ class QuantizedReluOp : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n-    const float min_input = context->input(1).flat<float>()(0);\n-    const float max_input = context->input(2).flat<float>()(0);\n+    const Tensor& min_input_tensor = context->input(1);\n+    const Tensor& max_input_tensor = context->input(2);\n+\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),\n+        errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",\n+                                min_input_tensor.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),\n+        errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",\n+                                max_input_tensor.dims()));\n+\n+    const float min_input = min_input_tensor.scalar<float>()();\n+    const float max_input = max_input_tensor.scalar<float>()();\n+\n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(context,\n                    context->allocate_output(0, input.shape(), &output));\n@@ -65,8 +78,21 @@ class QuantizedRelu6Op : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n-    const float min_input = context->input(1).flat<float>()(0);\n-    const float max_input = context->input(2).flat<float>()(0);\n+    const Tensor& min_input_tensor = context->input(1);\n+    const Tensor& max_input_tensor = context->input(2);\n+\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),\n+        errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",\n+                                min_input_tensor.dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),\n+        errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",\n+                                max_input_tensor.dims()));\n+\n+    const float min_input = min_input_tensor.scalar<float>()();\n+    const float max_input = max_input_tensor.scalar<float>()();\n+\n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(context,\n                    context->allocate_output(0, input.shape(), &output));\n",
            "@@ -55,8 +55,8 @@ TEST_F(QuantizedActivationsTest, TestRelu) {\n \n   AddInputFromArray<quint8>(input_quantized.shape(),\n                             input_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n@@ -86,8 +86,8 @@ TEST_F(QuantizedActivationsTest, TestRelu6) {\n \n   AddInputFromArray<quint8>(input_quantized.shape(),\n                             input_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n",
            "@@ -25,6 +25,7 @@ limitations under the License.\n \n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/meta_support.h\"\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n@@ -457,10 +458,28 @@ class QuantizedAddOp : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& x = context->input(0);\n     const Tensor& y = context->input(1);\n-    const float min_x = context->input(2).flat<float>()(0);\n-    const float max_x = context->input(3).flat<float>()(0);\n-    const float min_y = context->input(4).flat<float>()(0);\n-    const float max_y = context->input(5).flat<float>()(0);\n+    const Tensor& min_x_tensor = context->input(2);\n+    const Tensor& max_x_tensor = context->input(3);\n+    const Tensor& min_y_tensor = context->input(4);\n+    const Tensor& max_y_tensor = context->input(5);\n+\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_x_tensor.shape()),\n+                errors::InvalidArgument(\"`min_x` must be rank 0 but is rank \",\n+                                        min_x_tensor.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_x_tensor.shape()),\n+                errors::InvalidArgument(\"`max_x` must be rank 0 but is rank \",\n+                                        max_x_tensor.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_y_tensor.shape()),\n+                errors::InvalidArgument(\"`min_y` must be rank 0 but is rank \",\n+                                        min_y_tensor.dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_y_tensor.shape()),\n+                errors::InvalidArgument(\"`max_y` must be rank 0 but is rank \",\n+                                        max_y_tensor.dims()));\n+\n+    const float min_x = min_x_tensor.scalar<float>()();\n+    const float max_x = max_x_tensor.scalar<float>()();\n+    const float min_y = min_y_tensor.scalar<float>()();\n+    const float max_y = max_y_tensor.scalar<float>()();\n \n     BCast bcast(BCast::FromShape(x.shape()), BCast::FromShape(y.shape()));\n     if (!bcast.IsValid()) {\n",
            "@@ -206,5 +206,60 @@ class RequantizeOpTest(test_util.TensorFlowTestCase):\n               out_type=dtypes.qint8))\n \n \n+class QuantizedAddOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    x = constant_op.constant(\n+        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n+    y = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.quint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          math_ops.quantized_add(\n+              x=x,\n+              y=y,\n+              min_x=[],\n+              max_x=1.0,\n+              min_y=0.0,\n+              max_y=1.0,\n+              Toutput=dtypes.qint32))\n+\n+\n+class QuantizedReluOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_relu(\n+              features=inputs,\n+              min_features=[],\n+              max_features=127.0,\n+              out_type=dtypes.quint8))\n+\n+\n+class QuantizedRelu6OpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_relu6(\n+              features=inputs,\n+              min_features=[],\n+              max_features=127.0,\n+              out_type=dtypes.quint8))\n+\n+\n if __name__ == \"__main__\":\n   googletest.main()\n"
        ],
        "Title": "\n          Segfault in `QuantizedAdd`\n        "
    },
    {
        "Bug description": "If  QuantizedAvgPool  is given  min_input  or  max_input  tensors of a nonzero rank, it results in a segfault that can be used to trigger a denial of service attack.",
        "Sample Code": "ksize = [1, 2, 2, 1]\nstrides = [1, 2, 2, 1]\npadding = \"SAME\"\ninput = tf.constant(1, shape=[1,4,4,2], dtype=tf.quint8)\nmin_input = tf.constant([], shape=[0], dtype=tf.float32)\nmax_input = tf.constant(0, shape=[1], dtype=tf.float32)\n)\ntf.raw_ops.QuantizedAvgPool(input=input, min_input=min_input, max_input=max_input, ksize=ksize, strides=strides, padding=padding)",
        "Bug fix": [
            "@@ -15,18 +15,18 @@ limitations under the License.\n \n // See docs in ../ops/nn_ops.cc.\n \n-#include \"tensorflow/core/framework/op_requires.h\"\n-#include \"tensorflow/core/platform/errors.h\"\n #define EIGEN_USE_THREADS\n \n #include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n #include \"tensorflow/core/framework/numeric_op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/op_requires.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/ops_util.h\"\n #include \"tensorflow/core/kernels/pooling_ops_common.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/platform/logging.h\"\n #include \"tensorflow/core/util/padding.h\"\n #include \"tensorflow/core/util/tensor_format.h\"\n@@ -67,8 +67,20 @@ class QuantizedAvgPoolingOp : public OpKernel {\n       return;\n     }\n \n-    const float min_input = context->input(1).flat<float>()(0);\n-    const float max_input = context->input(2).flat<float>()(0);\n+    const Tensor& min_input_tensor = context->input(1);\n+    const Tensor& max_input_tensor = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),\n+                errors::InvalidArgument(\n+                    \"min_input shape must be rank 0 but is rank \",\n+                    min_input_tensor.dims(),\n+                    \", received shape: \", min_input_tensor.shape()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),\n+                errors::InvalidArgument(\n+                    \"max_input shape must be rank 0 but is rank \",\n+                    max_input_tensor.dims(),\n+                    \", received shape: \", max_input_tensor.shape()));\n+    const float min_input = context->input(1).scalar<float>()();\n+    const float max_input = context->input(2).scalar<float>()();\n \n     OP_REQUIRES(context, params.depth_window == 1,\n                 errors::Unimplemented(\"Non-spatial pooling is not \"\n@@ -119,20 +131,20 @@ class QuantizedMaxPoolingOp : public MaxPoolingOp<Device, T> {\n       : MaxPoolingOp<Device, T>(context) {}\n \n   void Compute(OpKernelContext* context) override {\n-    auto min_input_tensor = context->input(1);\n-    auto max_input_tensor = context->input(2);\n-    OP_REQUIRES(\n-        context, min_input_tensor.NumElements() == 1,\n-        errors::InvalidArgument(\n-            \"min_input must be a scalar float value, got tensor with shape \",\n-            min_input_tensor.shape()));\n-    OP_REQUIRES(\n-        context, max_input_tensor.NumElements() == 1,\n-        errors::InvalidArgument(\n-            \"max_input must be a scalar float value, got tensor with shape \",\n-            max_input_tensor.shape()));\n-    const float min_input = context->input(1).flat<float>()(0);\n-    const float max_input = context->input(2).flat<float>()(0);\n+    const Tensor& min_input_tensor = context->input(1);\n+    const Tensor& max_input_tensor = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),\n+                errors::InvalidArgument(\n+                    \"min_input shape must be rank 0 but is rank \",\n+                    min_input_tensor.dims(),\n+                    \", received shape: \", min_input_tensor.shape()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),\n+                errors::InvalidArgument(\n+                    \"max_input shape must be rank 0 but is rank \",\n+                    max_input_tensor.dims(),\n+                    \", received shape: \", max_input_tensor.shape()));\n+    const float min_input = context->input(1).scalar<float>()();\n+    const float max_input = context->input(2).scalar<float>()();\n     MaxPoolingOp<Device, T>::Compute(context);\n     Tensor* output_min = nullptr;\n     OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));\n",
            "@@ -69,8 +69,8 @@ TEST_F(QuantizedPoolingTest, SmallAveragePooling) {\n \n   AddInputFromArray<quint8>(input_quantized.shape(),\n                             input_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n@@ -114,8 +114,8 @@ TEST_F(QuantizedPoolingTest, SmallMaxPooling) {\n \n   AddInputFromArray<quint8>(input_quantized.shape(),\n                             input_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {input_min});\n-  AddInputFromArray<float>(TensorShape({1}), {input_max});\n+  AddInputFromArray<float>(TensorShape({}), {input_min});\n+  AddInputFromArray<float>(TensorShape({}), {input_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const Tensor& output_quantized = *GetOutput(0);\n   const float output_min = GetOutput(1)->flat<float>()(0);\n",
            "@@ -154,6 +154,72 @@ class QuantizedInstanceNormOpTest(test_util.TensorFlowTestCase):\n               x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))\n \n \n+class QuantizedAvgPoolingOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n+    ksize = [1, 1, 1, 1]\n+    strides = [1, 1, 1, 1]\n+    padding = \"SAME\"\n+\n+    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n+                                \"must be.* rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_avg_pool(\n+              input=inputs,\n+              min_input=[],\n+              max_input=1.0,\n+              ksize=ksize,\n+              strides=strides,\n+              padding=padding))\n+\n+    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n+                                \"must be.* rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_avg_pool(\n+              input=inputs,\n+              min_input=0.0,\n+              max_input=[],\n+              ksize=ksize,\n+              strides=strides,\n+              padding=padding))\n+\n+\n+class QuantizedMaxPoolingOpTest(test_util.TensorFlowTestCase):\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def test_invalid_inputs(self):\n+    inputs = constant_op.constant(\n+        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n+    ksize = [1, 1, 1, 1]\n+    strides = [1, 1, 1, 1]\n+    padding = \"SAME\"\n+\n+    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n+                                \"must be.* rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_max_pool(\n+              input=inputs,\n+              min_input=[],\n+              max_input=1.0,\n+              ksize=ksize,\n+              strides=strides,\n+              padding=padding))\n+\n+    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n+                                \"must be.* rank 0\"):\n+      self.evaluate(\n+          nn_ops.quantized_max_pool(\n+              input=inputs,\n+              min_input=0.0,\n+              max_input=[],\n+              ksize=ksize,\n+              strides=strides,\n+              padding=padding))\n+\n+\n class RequantizeOpTest(test_util.TensorFlowTestCase):\n \n   @test_util.run_in_graph_and_eager_modes\n"
        ],
        "Title": "\n          Segfault in `QuantizedAvgPool`\n        "
    },
    {
        "Bug description": "If  LowerBound  or  UpperBound  is given an empty sorted_inputs  input, it results in a  nullptr  dereference, leading to a segfault that can be used to trigger a denial of service attack.",
        "Sample Code": "out_type = tf.int64\nsorted_inputs = tf.constant([], shape=[2,2,0,0,0,0,0,2], dtype=tf.float32)\nvalues = tf.constant(0.372660398, shape=[2,4], dtype=tf.float32)\n)\ntf.raw_ops.UpperBound(sorted_inputs=sorted_inputs, values=values, out_type=out_type)",
        "Bug fix": [
            "@@ -22,6 +22,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/kernels/fill_functor.h\"\n #include \"tensorflow/core/lib/core/bits.h\"\n #include \"tensorflow/core/platform/logging.h\"\n #include \"tensorflow/core/platform/threadpool.h\"\n@@ -129,6 +130,14 @@ class UpperBoundOp : public OpKernel {\n     auto output = output_t->template flat<OutType>();\n     const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n     const auto values = values_t.template flat<T>();\n+\n+    // For empty inputs, all values will be placed at the zeroth position.\n+    if (sorted_inputs.size() == 0) {\n+      functor::SetZeroFunctor<Device, OutType> set_zero;\n+      set_zero(ctx->eigen_device<Device>(), output);\n+      return;\n+    }\n+\n     OP_REQUIRES_OK(\n         ctx, functor::UpperBoundFunctor<Device, T, OutType>::Compute(\n                  ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n@@ -174,6 +183,14 @@ class LowerBoundOp : public OpKernel {\n     auto output = output_t->template flat<OutType>();\n     const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n     const auto values = values_t.template flat<T>();\n+\n+    // For empty inputs, all values will be placed at the zeroth position.\n+    if (sorted_inputs.size() == 0) {\n+      functor::SetZeroFunctor<Device, OutType> set_zero;\n+      set_zero(ctx->eigen_device<Device>(), output);\n+      return;\n+    }\n+\n     OP_REQUIRES_OK(\n         ctx, functor::LowerBoundFunctor<Device, T, OutType>::Compute(\n                  ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n",
            "@@ -2060,6 +2060,17 @@ class SortedSearchTest(test_util.TensorFlowTestCase):\n                 side=side,\n                 out_type=dtype), array_ops.zeros([2, 0], dtype))\n \n+  def testZeroInputSize(self):\n+    dtype = dtypes.int32\n+    for side in (\"left\", \"right\"):\n+      with self.subTest(side=side):\n+        self.assertAllEqual(\n+            array_ops.searchsorted(\n+                array_ops.ones([2, 0]),\n+                array_ops.ones([2, 3]),\n+                side=side,\n+                out_type=dtype), array_ops.zeros([2, 3], dtype))\n+\n   def testInt64(self):\n \n     @def_function.function\n"
        ],
        "Title": "\n          Segfault in `LowerBound` and `UpperBound`\n        "
    },
    {
        "Bug description": "The implementation of  BlockLSTMGradV2  does not fully validate its inputs.",
        "Sample Code": "use_peephole = False\nseq_len_max = tf.constant(1, shape=[], dtype=tf.int64)\nx = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\ncs_prev = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\nh_prev = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\nw = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\nwci = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\nwcf = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\nwco = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\nb = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\ni = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\ncs = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\nf = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\no = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\nci = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\nco = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\nh = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\ncs_grad = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\nh_grad = tf.constant(0.504355371, shape=[1,1,1], dtype=tf.float32)\n)\ntf.raw_ops.BlockLSTMGradV2(seq_len_max=seq_len_max, x=x, cs_prev=cs_prev, h_prev=h_prev, w=w, wci=wci, wcf=wcf, wco=wco, b=b, i=i, cs=cs, f=f, o=o, ci=ci, co=co, h=h, cs_grad=cs_grad, h_grad=h_grad, use_peephole=use_peephole)",
        "Bug fix": [
            "@@ -1138,19 +1138,30 @@ class BlockLSTMGradOp : public OpKernel {\n \n     const Tensor* x;\n     OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x));\n-    OP_REQUIRES(ctx, x->dims() == 3, errors::InvalidArgument(\"x must be 3D\"));\n+    OP_REQUIRES(\n+        ctx, x->dims() == 3,\n+        errors::InvalidArgument(\"x must be rank 3 but is rank \", x->dims()));\n     const int64_t timelen = x->dim_size(0);\n     const int64_t batch_size = x->dim_size(1);\n     const int64_t input_size = x->dim_size(2);\n \n     const Tensor* cs_prev_tensor = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->input(\"cs_prev\", &cs_prev_tensor));\n+    OP_REQUIRES(ctx, cs_prev_tensor->dims() == 2,\n+                errors::InvalidArgument(\"cs_prev must be rank 2 but is rank \",\n+                                        cs_prev_tensor->dims()));\n \n     const Tensor* h_prev_tensor = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->input(\"h_prev\", &h_prev_tensor));\n+    OP_REQUIRES(ctx, h_prev_tensor->dims() == 2,\n+                errors::InvalidArgument(\"h_prev must be rank 2 but is rank \",\n+                                        h_prev_tensor->dims()));\n \n     const Tensor* w_tensor = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n+    OP_REQUIRES(ctx, w_tensor->dims() == 2,\n+                errors::InvalidArgument(\"w must be rank 2 but is rank \",\n+                                        w_tensor->dims()));\n     const int64_t cell_size = w_tensor->dim_size(1) / 4;\n     OP_REQUIRES(ctx, input_size + cell_size == w_tensor->dim_size(0),\n                 errors::InvalidArgument(\n@@ -1159,15 +1170,27 @@ class BlockLSTMGradOp : public OpKernel {\n \n     const Tensor* wci_tensor = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->input(\"wci\", &wci_tensor));\n+    OP_REQUIRES(ctx, wci_tensor->dims() == 1,\n+                errors::InvalidArgument(\"wci must be rank 1 but is rank \",\n+                                        wci_tensor->dims()));\n \n     const Tensor* wcf_tensor = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->input(\"wcf\", &wcf_tensor));\n+    OP_REQUIRES(ctx, wcf_tensor->dims() == 1,\n+                errors::InvalidArgument(\"wcf must be rank 1 but is rank \",\n+                                        wcf_tensor->dims()));\n \n     const Tensor* wco_tensor = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->input(\"wco\", &wco_tensor));\n+    OP_REQUIRES(ctx, wco_tensor->dims() == 1,\n+                errors::InvalidArgument(\"wco must be rank 1 but is rank \",\n+                                        wco_tensor->dims()));\n \n     const Tensor* b_tensor = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n+    OP_REQUIRES(ctx, b_tensor->dims() == 1,\n+                errors::InvalidArgument(\"b must be rank 1 but is rank \",\n+                                        b_tensor->dims()));\n     OP_REQUIRES(\n         ctx, cell_size == b_tensor->dim_size(0) / 4,\n         errors::InvalidArgument(\"w and b cell_size don't match: \", cell_size,\n",
            "@@ -1354,6 +1354,58 @@ class LSTMTest(test.TestCase):\n               cell_clip=cell_clip,\n               use_peephole=use_peephole))\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def testLSTMBlockCellGradErrorHandling(self):\n+    use_peephole = False\n+    seq_len_max = constant_op.constant(1, shape=[], dtype=dtypes.int64)\n+    x = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)\n+    cs_prev = constant_op.constant(\n+        0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)\n+    h_prev = constant_op.constant(\n+        0.504355371, shape=[1, 1], dtype=dtypes.float32)\n+    w = constant_op.constant(0.504355371, shape=[1, 1], dtype=dtypes.float32)\n+    wci = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)\n+    wcf = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)\n+    wco = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)\n+    b = constant_op.constant(0.504355371, shape=[1], dtype=dtypes.float32)\n+    i = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)\n+    cs = constant_op.constant(\n+        0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)\n+    f = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)\n+    o = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)\n+    ci = constant_op.constant(\n+        0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)\n+    co = constant_op.constant(\n+        0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)\n+    h = constant_op.constant(0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)\n+    cs_grad = constant_op.constant(\n+        0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)\n+    h_grad = constant_op.constant(\n+        0.504355371, shape=[1, 1, 1], dtype=dtypes.float32)\n+    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n+                                \"must be rank\"):\n+      self.evaluate(\n+          gen_rnn_ops.block_lstm_grad_v2(\n+              seq_len_max=seq_len_max,\n+              x=x,\n+              cs_prev=cs_prev,\n+              h_prev=h_prev,\n+              w=w,\n+              wci=wci,\n+              wcf=wcf,\n+              wco=wco,\n+              b=b,\n+              i=i,\n+              cs=cs,\n+              f=f,\n+              o=o,\n+              ci=ci,\n+              co=co,\n+              h=h,\n+              cs_grad=cs_grad,\n+              h_grad=h_grad,\n+              use_peephole=use_peephole))\n+\n \n class BidirectionalRNNTest(test.TestCase):\n \n"
        ],
        "Title": "\n          Segfault in `BlockLSTMGradV2`\n        "
    },
    {
        "Bug description": "The implementation of  FractionalAvgPoolGrad  does not fully validate the input  orig_input_tensor_shape . This results in an overflow that results in a   CHECK  failure which can be used to trigger a denial of service attack.",
        "Sample Code": "overlapping = True\norig_input_tensor_shape = tf.constant(-1879048192, shape=[4], dtype=tf.int64)\nout_backprop = tf.constant([], shape=[0,0,0,0], dtype=tf.float64)\nrow_pooling_sequence = tf.constant(1, shape=[4], dtype=tf.int64)\ncol_pooling_sequence = tf.constant(1, shape=[4], dtype=tf.int64)\n)\ntf.raw_ops.FractionalAvgPoolGrad(orig_input_tensor_shape=orig_input_tensor_shape, out_backprop=out_backprop, row_pooling_sequence=row_pooling_sequence, col_pooling_sequence=col_pooling_sequence, overlapping=overlapping)",
        "Bug fix": [
            "@@ -12,6 +12,7 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n+\n #define EIGEN_USE_THREADS\n \n #include <algorithm>\n@@ -19,15 +20,15 @@ limitations under the License.\n #include <random>\n #include <vector>\n \n-#include \"tensorflow/core/kernels/fractional_pool_common.h\"\n-\n #include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n #include \"tensorflow/core/framework/numeric_op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/kernels/fractional_pool_common.h\"\n #include \"tensorflow/core/lib/random/random.h\"\n #include \"tensorflow/core/platform/logging.h\"\n #include \"tensorflow/core/platform/mutex.h\"\n #include \"tensorflow/core/util/guarded_philox_random.h\"\n+#include \"tensorflow/core/util/overflow.h\"\n \n namespace tensorflow {\n typedef Eigen::ThreadPoolDevice CPUDevice;\n@@ -241,7 +242,32 @@ class FractionalAvgPoolGradOp : public OpKernel {\n                     orig_input_tensor_shape.NumElements() == 4,\n                 errors::InvalidArgument(\"original input tensor shape must be\"\n                                         \"1-dimensional and 4 elements\"));\n+    int64_t num_elements = 1;\n+    for (int i = 0; i < orig_input_tensor_shape.dims(); i++) {\n+      OP_REQUIRES(context, orig_input_tensor_shape.dim_size(i) > 0,\n+                  errors::InvalidArgument(\n+                      \"orig_input_tensor_shape must be positive, got: \",\n+                      orig_input_tensor_shape.dim_size(i)));\n+      num_elements = MultiplyWithoutOverflow(\n+          num_elements, orig_input_tensor_shape.dim_size(i));\n+      OP_REQUIRES(\n+          context, num_elements > 0,\n+          errors::InvalidArgument(\n+              \"The total elements specified by orig_input_tensor_shape\",\n+              \" is too large. Encountered overflow after multiplying \",\n+              orig_input_tensor_shape.dim_size(i), \", result: \", num_elements));\n+    }\n+\n     const Tensor& out_backprop = context->input(1);\n+    OP_REQUIRES(context, out_backprop.dims() == 4,\n+                errors::InvalidArgument(\"out_backprop must be 4-dimensional\"));\n+    for (int i = 0; i < out_backprop.dims(); i++) {\n+      OP_REQUIRES(context, out_backprop.dim_size(i) > 0,\n+                  errors::InvalidArgument(\n+                      \"out_backprop must be positive for all dimension, got:\",\n+                      out_backprop.dim_size(i)));\n+    }\n+\n     const Tensor& row_seq_tensor = context->input(2);\n     const Tensor& col_seq_tensor = context->input(3);\n \n",
            "@@ -541,6 +541,27 @@ class FractionalAvgPoolGradTest(test.TestCase):\n           delta=1e-2)\n       self.assertLess(gradient_error, error_margin)\n \n+  def testInvalidSeqRaiseErrorForFractionalAvgPoolGrad(self):\n+    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n+      with self.cached_session() as _:\n+        overlapping = True\n+        orig_input_tensor_shape = constant_op.constant(\n+            -1879048192, shape=[4], dtype=dtypes.int64)\n+        out_backprop = constant_op.constant([],\n+                                            shape=[0, 0, 0, 0],\n+                                            dtype=dtypes.float64)\n+        row_pooling_sequence = constant_op.constant(\n+            1, shape=[4], dtype=dtypes.int64)\n+        col_pooling_sequence = constant_op.constant(\n+            1, shape=[4], dtype=dtypes.int64)\n+        t = gen_nn_ops.fractional_avg_pool_grad(\n+            orig_input_tensor_shape=orig_input_tensor_shape,\n+            out_backprop=out_backprop,\n+            row_pooling_sequence=row_pooling_sequence,\n+            col_pooling_sequence=col_pooling_sequence,\n+            overlapping=overlapping)\n+        self.evaluate(t)\n+\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          `CHECK` failures in `FractionalAvgPoolGrad`\n        "
    },
    {
        "Bug description": "The implementation of  AvgPool3DGradOp  does not fully validate the input  orig_input_shape . This results in an overflow that results in a   CHECK  failure which can be used to trigger a denial of service attack:",
        "Sample Code": "ksize = [1, 1, 1, 1, 1]\nstrides = [1, 1, 1, 1, 1]\npadding = \"SAME\"\ndata_format = \"NDHWC\"\norig_input_shape = tf.constant(1879048192, shape=[5], dtype=tf.int32)\ngrad = tf.constant(1, shape=[1,3,2,4,2], dtype=tf.float32)\n)\ntf.raw_ops.AvgPool3DGrad(orig_input_shape=orig_input_shape, grad=grad, ksize=ksize, strides=strides, padding=padding, data_format=data_format)",
        "Bug fix": [
            "@@ -403,6 +403,7 @@ cc_library(\n         \"//tensorflow/compiler/xla/client:xla_builder\",\n         \"//tensorflow/compiler/xla/client:xla_computation\",\n         \"//tensorflow/compiler/xla/service:hlo\",\n+        \"//tensorflow/core/util:overflow\",\n         \"//tensorflow/core:core_cpu\",\n         \"//tensorflow/core:core_cpu_internal\",\n         \"//tensorflow/core:framework\",\n",
            "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"tensorflow/compiler/xla/status_macros.h\"\n #include \"tensorflow/core/common_runtime/dma_helper.h\"\n #include \"tensorflow/core/platform/errors.h\"\n+#include \"tensorflow/core/util/overflow.h\"\n \n namespace tensorflow {\n \n@@ -443,6 +444,16 @@ Status XlaOpKernelContext::ConstantInputAsShape(int index, TensorShape* shape,\n   TF_RETURN_IF_ERROR(ConstantInput(index, &literal, mode));\n   std::vector<int64_t> dims;\n   TF_RETURN_IF_ERROR(LiteralToInt64Vector(literal, &dims));\n+\n+  int64_t num_elements = 1;\n+  for (auto i = dims.begin(); i != dims.end(); ++i) {\n+    num_elements = MultiplyWithoutOverflow(num_elements, *i);\n+    if (num_elements < 0)\n+      return errors::InvalidArgument(\n+          \"The total elements specified by orig_input_shape is too large.\",\n+          \"Encountered overflow after multiplying\", *i,\n+          \", result: \", num_elements);\n+  }\n   *shape = TensorShape(dims);\n   return OkStatus();\n }\n",
            "@@ -523,7 +523,7 @@ class AvgPooling3dGradOp : public OpKernel {\n     TensorShape output_shape;\n     auto shape_vec = tensor_in_shape.vec<int32>();\n     for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {\n-      output_shape.AddDim(shape_vec(i));\n+      OP_REQUIRES_OK(context, output_shape.AddDimWithStatus(shape_vec(i)));\n     }\n \n     Tensor* output;\n",
            "@@ -500,6 +500,7 @@ cuda_py_test(\n     srcs = [\"pooling_ops_3d_test.py\"],\n     deps = [\n         \"//tensorflow/python:client_testlib\",\n+        \"//tensorflow/python:dtypes\",\n         \"//tensorflow/python:framework_for_generated_wrappers\",\n         \"//tensorflow/python:nn_grad\",\n         \"//tensorflow/python:nn_ops\",\n",
            "@@ -18,6 +18,7 @@ import numpy as np\n \n from tensorflow.python.eager import context\n from tensorflow.python.framework import constant_op\n+from tensorflow.python.framework import dtypes\n from tensorflow.python.framework import errors\n from tensorflow.python.framework import errors_impl\n from tensorflow.python.framework import test_util\n@@ -67,7 +68,7 @@ class PoolingTest(test.TestCase):\n     # Initializes the input tensor with array containing incrementing\n     # numbers from 1.\n     x = [f * 1.0 for f in range(1, total_size + 1)]\n-    with self.cached_session(use_gpu=use_gpu) as sess:\n+    with self.cached_session(use_gpu=use_gpu):\n       t = constant_op.constant(x, shape=input_sizes)\n       window = [1] + list(window) + [1]\n       strides = [1] + list(strides) + [1]\n@@ -124,6 +125,23 @@ class PoolingTest(test.TestCase):\n         padding=\"SAME\",\n         expected=expected_output)\n \n+  def testMaxPool3dGrad(self):\n+    with self.assertRaises(\n+        (errors.ResourceExhaustedError, errors.InvalidArgumentError)):\n+      with self.cached_session():\n+        orig_input_shape = constant_op.constant(\n+            1879048192, shape=[5], dtype=dtypes.int32)\n+        grad = constant_op.constant(\n+            1, shape=[1, 3, 2, 4, 2], dtype=dtypes.float32)\n+        t = gen_nn_ops.AvgPool3DGrad(\n+            orig_input_shape=orig_input_shape,\n+            grad=grad,\n+            ksize=[1, 1, 1, 1, 1],\n+            strides=[1, 1, 1, 1, 1],\n+            padding=\"SAME\",\n+            data_format=\"NDHWC\")\n+        self.evaluate(t)\n+\n   def testMaxPool3dValidPadding(self):\n     expected_output = [40.0, 41.0, 42.0]\n     self._VerifyValues(\n"
        ],
        "Title": "\n          `CHECK` failures in `AvgPool3DGrad`\n        "
    },
    {
        "Bug description": "The  UnbatchGradOp  function takes an argument  id  that is assumed to be a scalar. A nonscalar  id  can trigger a  CHECK  failure and crash the program.",
        "Sample Code": "import tensorflow as tf\n\n# batch_index's size is not 3\n\ntf.raw_ops.UnbatchGrad(original_input= tf.constant([1]),batch_index=tf.constant([[0,0], ], dtype=tf.int64),grad=tf.constant([1,]),id=tf.constant([1,], dtype=tf.int64))",
        "Bug fix": [
            "@@ -885,8 +885,13 @@ class UnbatchGradResource : public ResourceBase {\n     const Tensor& data_t = context->input(0);\n     const Tensor& batch_index_t = context->input(1);\n     const Tensor& grad_t = context->input(2);\n+    const Tensor& batch_key_t = context->input(3);\n \n     mutex_lock ml(mu_);\n+    if (batch_key_t.NumElements() != 1) {\n+      return errors::InvalidArgument(\"Expected `id` to be scalar. Received \",\n+                                     batch_key_t.DebugString());\n+    }\n \n     const int64_t batch_key = context->input(3).scalar<int64_t>()();\n     // Mark our tensor as available.\n@@ -902,6 +907,11 @@ class UnbatchGradResource : public ResourceBase {\n             \"batch_index is empty while the tensor isn't.\");\n       }\n       std::unordered_set<int64_t> missing_tensors;\n+      if (batch_index_t.NumElements() != batch_index_t.dim_size(0) * 3) {\n+        return errors::InvalidArgument(\n+            \"batch_index should contain \", batch_index_t.dim_size(0) * 3,\n+            \" elements. Received \", batch_index_t.NumElements());\n+      }\n       const auto batch_index =\n           batch_index_t.shaped<int64_t, 2>({batch_index_t.dim_size(0), 3});\n       for (int i = 0; i < batch_index_t.dim_size(0); ++i) {\n",
            "@@ -20,7 +20,9 @@ import numpy as np\n \n from tensorflow.core.protobuf import config_pb2\n from tensorflow.python.eager import context\n+from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import function\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import test_util\n@@ -30,6 +32,7 @@ from tensorflow.python.ops import batch_ops\n from tensorflow.python.ops import gen_batch_ops\n from tensorflow.python.ops import gen_functional_ops\n from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import random_ops\n from tensorflow.python.ops import resource_variable_ops\n from tensorflow.python.ops import script_ops\n from tensorflow.python.ops import variables\n@@ -557,6 +560,56 @@ class BatchOpsTest(test.TestCase):\n       # The thread's call should hit the timeout, and thus get 0 results.\n       self.assertEqual(len(thread_results), 0)\n \n+  def testUnbatchGradInvalidId(self):\n+    with self.assertRaises(errors.InvalidArgumentError):\n+      self.evaluate(\n+          gen_batch_ops.unbatch_grad(\n+              original_input=constant_op.constant([1]),\n+              batch_index=constant_op.constant([\n+                  [0, 0, 0],\n+              ], dtype=dtypes.int64),\n+              grad=constant_op.constant([\n+                  1,\n+              ]),\n+              id=constant_op.constant([\n+                  1,\n+                  1,\n+              ], dtype=dtypes.int64)))\n+\n+  def testUnbatchGradInvalidBatchId(self):\n+    with self.assertRaises(errors.InvalidArgumentError):\n+      self.evaluate(\n+          gen_batch_ops.unbatch_grad(\n+              original_input=constant_op.constant([1]),\n+              batch_index=constant_op.constant([\n+                  [0, 0],\n+              ], dtype=dtypes.int64),\n+              grad=constant_op.constant([\n+                  1,\n+              ]),\n+              id=constant_op.constant([\n+                  1,\n+              ], dtype=dtypes.int64)))\n+\n+  def testUnbatchGradInvalidArgs(self):\n+    original_input = random_ops.random_uniform(\n+        shape=(3, 1), dtype=dtypes.float64, maxval=None)\n+    batch_index = random_ops.random_uniform(\n+        shape=(3, 1), dtype=dtypes.int64, maxval=65536)\n+    grad = random_ops.random_uniform(\n+        shape=(3, 1), dtype=dtypes.float64, maxval=None)\n+    batch_id = random_ops.random_uniform(\n+        shape=(3, 1), dtype=dtypes.int64, maxval=65536)\n+    with self.assertRaises(errors.InvalidArgumentError):\n+      self.evaluate(\n+          gen_batch_ops.unbatch_grad(\n+              original_input=original_input,\n+              batch_index=batch_index,\n+              grad=grad,\n+              id=batch_id,\n+              container=\"\",\n+              shared_name=\"\",\n+              name=\"\"))\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          `CHECK` failures in `UnbatchGradOp`\n        "
    },
    {
        "Bug description": "The  AvgPoolOp  function takes an argument  ksize  that must be positive but is not checked. A negative  ksize  can trigger a  CHECK  failure and crash the program.",
        "Sample Code": "import numpy as np\n\nvalue = np.ones([1, 1, 1, 1])\nksize = [1, 1e20, 1, 1]\nstrides = [1, 1, 1, 1]\npadding = 'SAME'\ndata_format = 'NHWC'\n\n\n\ntf.raw_ops.AvgPool(value=value, ksize=ksize, strides=strides, padding=padding, data_format=data_format)",
        "Bug fix": [
            "@@ -298,7 +298,7 @@ class AvgPoolingGradOp : public OpKernel {\n     TensorShape output_shape;\n     auto shape_vec = tensor_in_shape.vec<int32>();\n     for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {\n-      output_shape.AddDim(shape_vec(i));\n+      OP_REQUIRES_OK(context, output_shape.AddDimWithStatus(shape_vec(i)));\n     }\n     const int64_t in_rows = output_shape.dim_size(1);\n     const int64_t in_cols = output_shape.dim_size(2);\n@@ -457,7 +457,7 @@ class AvgPoolingGradOp<GPUDevice, T> : public OpKernel {\n     TensorShape output_shape;\n     auto shape_vec = tensor_in_shape.vec<int32>();\n     for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {\n-      output_shape.AddDim(shape_vec(i));\n+      OP_REQUIRES_OK(context, output_shape.AddDimWithStatus(shape_vec(i)));\n     }\n \n     if (output_shape.num_elements() == 0) {\n@@ -543,7 +543,7 @@ class AvgPoolingGradOpCustomGPUKernel : public OpKernel {\n     TensorShape output_shape;\n     auto shape_vec = tensor_in_shape.vec<int32>();\n     for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {\n-      output_shape.AddDim(shape_vec(i));\n+      OP_REQUIRES_OK(context, output_shape.AddDimWithStatus(shape_vec(i)));\n     }\n     if (output_shape.num_elements() == 0) {\n       Tensor* output = nullptr;\n",
            "@@ -2470,6 +2470,22 @@ class PoolingTest(test.TestCase, parameterized.TestCase):\n               inp, grad, argmax, ksize=[1, 1, 1, 1], strides=[1, 1, 1, 1],\n               padding=\"VALID\")\n \n+  def testAvgPoolGradInvalidInputShapeRaiseError(self):\n+    with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):\n+      with self.cached_session():\n+        orig_input_shape = constant_op.constant(\n+            -536870912, shape=[4], dtype=dtypes.int32)\n+        grad = constant_op.constant(\n+            .0890338004362538, shape=[1, 5, 7, 1], dtype=dtypes.float64)\n+        t = gen_nn_ops.AvgPoolGrad(\n+            orig_input_shape=orig_input_shape,\n+            grad=grad,\n+            ksize=[1, 2, 2, 1],\n+            strides=[1, 2, 2, 1],\n+            padding=\"VALID\",\n+            data_format=\"NHWC\")\n+        self.evaluate(t)\n+\n \n def GetMaxPoolFwdTest(input_size, filter_size, strides, padding):\n \n"
        ],
        "Title": "\n          `CHECK` failure in `AvgPoolOp`\n        "
    }
]
[
    {
        "Bug description": "The  RaggedRangOp  function takes an argument  limits  that is eventually used to construct a  TensorShape  as an  int64 . If  limits  is a very large float, it can overflow when converted to an  int64 . This triggers an  InvalidArgument  but also throws an abort signal that crashes the program.",
        "Sample Code": " tensorflow as tf\ntf.raw_ops.RaggedRange(starts=[1.1,0.1],limits=[10.0,1e20],deltas=[1,1])",
        "Bug fix": [
            "@@ -12,6 +12,7 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n+#include <cstdint>\n #include <limits>\n #include <memory>\n #include <string>\n@@ -78,8 +79,25 @@ class RaggedRangeOp : public OpKernel {\n       T limit = broadcast_limits ? limits(0) : limits(row);\n       T delta = broadcast_deltas ? deltas(0) : deltas(row);\n       OP_REQUIRES(context, delta != 0, InvalidArgument(\"Requires delta != 0\"));\n-      rt_nested_splits(row + 1) =\n-          rt_nested_splits(row) + RangeSize(start, limit, delta);\n+      int64_t size;  // The number of elements in the specified range.\n+      if (((delta > 0) && (limit < start)) ||\n+          ((delta < 0) && (limit > start))) {\n+        size = 0;\n+      } else if (std::is_integral<T>::value) {\n+        // The following is copied from tensorflow::RangeOp::Compute().\n+        size = Eigen::divup(Eigen::numext::abs(limit - start),\n+                            Eigen::numext::abs(delta));\n+      } else {\n+        // The following is copied from tensorflow::RangeOp::Compute().\n+        auto size_auto =\n+            Eigen::numext::ceil(Eigen::numext::abs((limit - start) / delta));\n+        OP_REQUIRES(\n+            context, size_auto <= std::numeric_limits<int64_t>::max(),\n+            errors::InvalidArgument(\"Requires ((limit - start) / delta) <= \",\n+                                    std::numeric_limits<int64_t>::max()));\n+        size = static_cast<int64_t>(size_auto);\n+      }\n+      rt_nested_splits(row + 1) = rt_nested_splits(row) + size;\n     }\n     SPLITS_TYPE nvals = rt_nested_splits(nrows);\n \n@@ -99,19 +117,6 @@ class RaggedRangeOp : public OpKernel {\n       }\n     }\n   }\n-\n- private:\n-  // Returns the number of elements in the specified range.\n-  SPLITS_TYPE RangeSize(T start, T limit, T delta) {\n-    if (((delta > 0) && (limit < start)) || ((delta < 0) && (limit > start))) {\n-      return 0;\n-    }\n-    // The following is copied from tensorflow::RangeOp::Compute().\n-    return (std::is_integral<T>::value\n-                ? ((std::abs(limit - start) + std::abs(delta) - 1) /\n-                   std::abs(delta))\n-                : std::ceil(std::abs((limit - start) / delta)));\n-  }\n };\n \n #define REGISTER_CPU_KERNEL(TYPE)                                  \\\n",
            "@@ -13,6 +13,7 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include <gtest/gtest.h>\n #include \"tensorflow/core/framework/fake_input.h\"\n #include \"tensorflow/core/framework/node_def_builder.h\"\n #include \"tensorflow/core/framework/shape_inference.h\"\n@@ -77,6 +78,17 @@ TEST_F(RaggedRangeOpTest, FloatValues) {\n       test::AsTensor<float>({0, 2, 4, 6, 5, 6, 5, 4, 3, 2}), 0.1);\n }\n \n+TEST_F(RaggedRangeOpTest, RangeSizeOverflow) {\n+  BuildRaggedRangeGraph<float>();\n+  AddInputFromArray<float>(TensorShape({2}), {1.1, 0.1});    // starts\n+  AddInputFromArray<float>(TensorShape({2}), {10.0, 1e10});  // limits\n+  AddInputFromArray<float>(TensorShape({2}), {1, 1e-10});    // deltas\n+\n+  EXPECT_EQ(absl::StrCat(\"Requires ((limit - start) / delta) <= \",\n+                         std::numeric_limits<int64_t>::max()),\n+            RunOpKernel().error_message());\n+}\n+\n TEST_F(RaggedRangeOpTest, BroadcastDeltas) {\n   BuildRaggedRangeGraph<int>();\n   AddInputFromArray<int>(TensorShape({3}), {0, 5, 8});  // starts\n",
            "@@ -84,8 +84,7 @@ class RaggedRangeOpTest(test_util.TensorFlowTestCase):\n          list(range(5, 15, 3))])\n \n     # Broadcast all arguments.\n-    self.assertAllEqual(\n-        ragged_math_ops.range(0, 5, 1), [list(range(0, 5, 1))])\n+    self.assertAllEqual(ragged_math_ops.range(0, 5, 1), [list(range(0, 5, 1))])\n \n   def testEmptyRanges(self):\n     rt1 = ragged_math_ops.range([0, 5, 3], [0, 3, 5])\n@@ -108,6 +107,10 @@ class RaggedRangeOpTest(test_util.TensorFlowTestCase):\n                                 r'Requires delta != 0'):\n       self.evaluate(ragged_math_ops.range(0, 0, 0))\n \n+    with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                r'Requires \\(\\(limit - start\\) / delta\\) <='):\n+      self.evaluate(ragged_math_ops.range(0.1, 1e10, 1e-10))\n+\n   def testShape(self):\n     self.assertAllEqual(\n         ragged_math_ops.range(0, 0, 1).shape.as_list(), [1, None])\n"
        ],
        "Title": "\n          Int overflow in `RaggedRangeOp`\n        "
    },
    {
        "Bug description": "The  ScatterNd  function takes an input argument that determines the indices of of the output tensor. An input index greater than the output tensor or less than zero will either write content at the wrong index or trigger a crash.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -656,11 +656,12 @@ inline TfLiteStatus GatherNdString(const RuntimeShape& params_shape,\n #endif\n \n template <typename IndicesT, typename UpdatesT>\n-inline void ScatterNd(const RuntimeShape& indices_shape,\n-                      const IndicesT* indices_data,\n-                      const RuntimeShape& updates_shape,\n-                      const UpdatesT* updates_data,\n-                      const RuntimeShape& output_shape, UpdatesT* output_data) {\n+inline TfLiteStatus ScatterNd(const RuntimeShape& indices_shape,\n+                              const IndicesT* indices_data,\n+                              const RuntimeShape& updates_shape,\n+                              const UpdatesT* updates_data,\n+                              const RuntimeShape& output_shape,\n+                              UpdatesT* output_data) {\n   ruy::profiler::ScopeLabel label(\"ScatterNd\");\n \n   int n_slices = 1;\n@@ -683,18 +684,24 @@ inline void ScatterNd(const RuntimeShape& indices_shape,\n     remain_flat_size = dims_to_count[i];\n   }\n \n+  if (n_slices * slice_size > updates_shape.FlatSize()) {\n+    return kTfLiteError;\n+  }\n   memset(output_data, 0, sizeof(UpdatesT) * output_flat_size);\n   for (int i = 0; i < n_slices; ++i) {\n     int to_pos = 0;\n     for (int j = 0; j < indices_nd; ++j) {\n       IndicesT idx = indices_data[i * indices_nd + j];\n-      TFLITE_DCHECK(0 <= idx && idx < output_shape.Dims(j));\n       to_pos += idx * dims_to_count[j];\n     }\n+    if (to_pos < 0 || to_pos + slice_size > output_flat_size) {\n+      return kTfLiteError;\n+    }\n     for (int j = 0; j < slice_size; j++) {\n       output_data[to_pos + j] += updates_data[i * slice_size + j];\n     }\n   }\n+  return kTfLiteOk;\n }\n \n template <typename T>\n",
            "@@ -129,11 +129,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n template <typename IndicesT, typename UpdatesT>\n TfLiteStatus ScatterNd(const TfLiteTensor* indices, const TfLiteTensor* updates,\n                        TfLiteTensor* output) {\n-  reference_ops::ScatterNd(\n+  return reference_ops::ScatterNd(\n       GetTensorShape(indices), GetTensorData<IndicesT>(indices),\n       GetTensorShape(updates), GetTensorData<UpdatesT>(updates),\n       GetTensorShape(output), GetTensorData<UpdatesT>(output));\n-  return kTfLiteOk;\n }\n \n template <typename IndicesT>\n@@ -149,25 +148,36 @@ TfLiteStatus EvalScatterNd(TfLiteContext* context, const TfLiteTensor* indices,\n                       ResizeOutputTensor<IndicesT>(context, shape, output));\n   }\n \n+  TfLiteStatus status = kTfLiteError;\n   switch (updates->type) {\n     case kTfLiteFloat32:\n-      return ScatterNd<IndicesT, float>(indices, updates, output);\n+      status = ScatterNd<IndicesT, float>(indices, updates, output);\n+      break;\n     case kTfLiteUInt8:\n-      return ScatterNd<IndicesT, uint8_t>(indices, updates, output);\n+      status = ScatterNd<IndicesT, uint8_t>(indices, updates, output);\n+      break;\n     case kTfLiteBool:\n-      return ScatterNd<IndicesT, bool>(indices, updates, output);\n+      status = ScatterNd<IndicesT, bool>(indices, updates, output);\n+      break;\n     case kTfLiteInt8:\n-      return ScatterNd<IndicesT, int8_t>(indices, updates, output);\n+      status = ScatterNd<IndicesT, int8_t>(indices, updates, output);\n+      break;\n     case kTfLiteInt32:\n-      return ScatterNd<IndicesT, int32_t>(indices, updates, output);\n+      status = ScatterNd<IndicesT, int32_t>(indices, updates, output);\n+      break;\n     case kTfLiteInt64:\n-      return ScatterNd<IndicesT, int64_t>(indices, updates, output);\n+      status = ScatterNd<IndicesT, int64_t>(indices, updates, output);\n+      break;\n     default:\n       TF_LITE_KERNEL_LOG(\n           context, \"Updates of type '%s' are not supported by scatter_nd.\",\n           TfLiteTypeGetName(updates->type));\n       return kTfLiteError;\n   }\n+  if (status != kTfLiteOk) {\n+    TF_LITE_KERNEL_LOG(context, \"scatter_nd index out of bounds\");\n+  }\n+  return status;\n }\n \n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n",
            "@@ -361,5 +361,34 @@ TEST(ScatterNdOpTest, DynamicShape) {\n                                 /*2, 3*/ 1,  2,  3,  4,  5}));\n }\n \n+TEST(ScatterNdOpTest, ReadAndWriteArrayLimits) {\n+  ScatterNdOpModel m({TensorType_INT32, {5, 1}}, {TensorType_INT32, {5}},\n+                     {TensorType_INT32, {1}});\n+  m.SetIndices<int32_t>({4, 3, 1, 0, 2});\n+  m.SetUpdates<int32_t>({1, 2, 3, 7, 9});\n+  m.SetShape<int32_t>({5});\n+  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n+  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({5}));\n+  EXPECT_THAT(m.GetOutput<int32_t>(), ElementsAreArray({7, 3, 9, 2, 1}));\n+}\n+\n+TEST(ScatterNdOpTest, OOBRead) {\n+  ScatterNdOpModel m({TensorType_INT32, {1, 1}}, {TensorType_INT32, {1}},\n+                     {TensorType_INT32, {1}});\n+  m.SetIndices<int32_t>({4});\n+  m.SetUpdates<int32_t>({1});\n+  m.SetShape<int32_t>({1});\n+  ASSERT_EQ(m.Invoke(), kTfLiteError);\n+}\n+\n+TEST(ScatterNdOpTest, OOBWrites) {\n+  ScatterNdOpModel m({TensorType_INT32, {5, 1}}, {TensorType_INT32, {5}},\n+                     {TensorType_INT32, {1}});\n+  m.SetIndices<int32_t>({4, 3, 1, -0x38, 0x38});\n+  m.SetUpdates<int32_t>({1, 2, 3, 0x44444444, 0x55555555});\n+  m.SetShape<int32_t>({1});\n+  ASSERT_EQ(m.Invoke(), kTfLiteError);\n+}\n+\n }  // namespace\n }  // namespace tflite\n"
        ],
        "Title": "\n          OOB write in `scatter_nd` op in TF Lite\n        "
    },
    {
        "Bug description": "The  GatherNd  function takes arguments that determine the sizes of inputs and outputs. If the inputs given are greater than or equal to the sizes of the outputs, an out-of-bounds memory read or a crash is triggered.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          OOB read in `Gather_nd` op in TF Lite Micro\n        "
    },
    {
        "Bug description": "In  core/kernels/list_kernels.cc's TensorListReserve ,  num_elements  is assumed to be a tensor of size 1. When a  num_elements  of more than 1 element is provided, then  tf.raw_ops.TensorListReserve  fails the  CHECK_EQ  in  CheckIsAlignedAndSingleElement .",
        "Sample Code": " tensorflow as tf\n\ntf.raw_ops.TensorListReserve(element_shape=(1,1), num_elements=tf.constant([1,1], dtype=tf.int32), element_dtype=tf.int8)",
        "Bug fix": [
            "@@ -31,9 +31,11 @@ limitations under the License.\n #include \"tensorflow/core/framework/allocator.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/tensor_types.h\"\n #include \"tensorflow/core/framework/variant.h\"\n #include \"tensorflow/core/framework/variant_op_registry.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n \n namespace tensorflow {\n \n@@ -322,6 +324,11 @@ class TensorListReserve : public OpKernel {\n   void Compute(OpKernelContext* c) override {\n     PartialTensorShape element_shape;\n     OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(0), &element_shape));\n+    OP_REQUIRES(\n+        c, TensorShapeUtils::IsScalar(c->input(1).shape()),\n+        errors::InvalidArgument(\n+            \"The num_elements to reserve must be a tensor size 1, but got \",\n+            c->input(1).shape()));\n     int32_t num_elements = c->input(1).scalar<int32>()();\n     OP_REQUIRES(c, num_elements >= 0,\n                 errors::InvalidArgument(\"The num_elements to reserve must be a \"\n",
            "@@ -94,6 +94,16 @@ class ListOpsTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n       l = list_ops.tensor_list_pop_back(l, element_dtype=dtypes.float32)\n       self.evaluate(l)\n \n+  def testTensorListReserveWithNonScalarNumElements(self):\n+    # list_kernels.cc in tf/core/kernels raises InvalidArgumentError, and\n+    # tf_ops_n_z.cc in tf/compiler/mlir/tf/ir raises UnknownError.\n+    with self.assertRaises((errors.InvalidArgumentError, errors.UnknownError)):\n+      l = list_ops.tensor_list_reserve(\n+          element_dtype=dtypes.float32,\n+          element_shape=[2, 3],\n+          num_elements=constant_op.constant([1, 1]))\n+      self.evaluate(l)\n+\n   def testPopUninitializedTensorUseListElementShape(self):\n     l = list_ops.tensor_list_reserve(\n         element_dtype=dtypes.float32, element_shape=[2, 3], num_elements=3)\n"
        ],
        "Title": "\n          `CHECK` failure in `TensorListReserve` via missing validation\n        "
    },
    {
        "Bug description": "The  GatherNd  function takes arguments that determine the sizes of inputs and outputs. If the inputs given are greater than or equal to the sizes of the outputs, an out-of-bounds memory read is triggered.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -14,6 +14,7 @@ limitations under the License.\n ==============================================================================*/\n #include <stdint.h>\n \n+#include \"tensorflow/lite/c/c_api_types.h\"\n #include \"tensorflow/lite/c/common.h\"\n #include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n #include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n@@ -102,13 +103,16 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n template <typename ParamsT, typename IndicesT>\n-TfLiteStatus GatherNd(const TfLiteTensor* params, const TfLiteTensor* indices,\n-                      TfLiteTensor* output) {\n-  reference_ops::GatherNd(\n+TfLiteStatus GatherNd(TfLiteContext* context, const TfLiteTensor* params,\n+                      const TfLiteTensor* indices, TfLiteTensor* output) {\n+  const TfLiteStatus status = reference_ops::GatherNd(\n       GetTensorShape(params), GetTensorData<ParamsT>(params),\n       GetTensorShape(indices), GetTensorData<IndicesT>(indices),\n       GetTensorShape(output), GetTensorData<ParamsT>(output));\n-  return kTfLiteOk;\n+  if (status != kTfLiteOk) {\n+    TF_LITE_KERNEL_LOG(context, \"gather_nd index out of bounds\");\n+  }\n+  return status;\n }\n \n template <typename IndicesT>\n@@ -136,17 +140,17 @@ TfLiteStatus EvalGatherNd(TfLiteContext* context, const TfLiteTensor* params,\n \n   switch (params->type) {\n     case kTfLiteFloat32:\n-      return GatherNd<float, IndicesT>(params, indices, output);\n+      return GatherNd<float, IndicesT>(context, params, indices, output);\n     case kTfLiteUInt8:\n-      return GatherNd<uint8_t, IndicesT>(params, indices, output);\n+      return GatherNd<uint8_t, IndicesT>(context, params, indices, output);\n     case kTfLiteInt8:\n-      return GatherNd<int8_t, IndicesT>(params, indices, output);\n+      return GatherNd<int8_t, IndicesT>(context, params, indices, output);\n     case kTfLiteInt16:\n-      return GatherNd<int16_t, IndicesT>(params, indices, output);\n+      return GatherNd<int16_t, IndicesT>(context, params, indices, output);\n     case kTfLiteInt32:\n-      return GatherNd<int32_t, IndicesT>(params, indices, output);\n+      return GatherNd<int32_t, IndicesT>(context, params, indices, output);\n     case kTfLiteInt64:\n-      return GatherNd<int64_t, IndicesT>(params, indices, output);\n+      return GatherNd<int64_t, IndicesT>(context, params, indices, output);\n     case kTfLiteString:\n       return GatherNdString<IndicesT>(params, indices, output);\n     default:\n",
            "@@ -73,6 +73,22 @@ TEST(GatherNdOpTest, ElementIndexingIntoMatrix) {\n   EXPECT_THAT(m.GetOutput<float>(), ElementsAreArray({1.1, 2.2}));\n }\n \n+TEST(GatherNdOpTest, ErrorOnOutOfBoundsTooLarge) {\n+  GatherNdOpModel m({TensorType_FLOAT32, {2, 2}}, {TensorType_INT32, {2, 2}});\n+  m.SetInput<float>({1.1, 1.2, 2.1, 2.2});\n+  m.SetPositions<int32_t>({0, 0, 2, 0});\n+  EXPECT_EQ(m.Invoke(), kTfLiteError);\n+  m.SetPositions<int32_t>({0, 0, 1, 2});\n+  EXPECT_EQ(m.Invoke(), kTfLiteError);\n+}\n+\n+TEST(GatherNdOpTest, ErrorOnOutOfBoundsNegative) {\n+  GatherNdOpModel m({TensorType_FLOAT32, {2, 2}}, {TensorType_INT32, {2, 2}});\n+  m.SetInput<float>({1.1, 1.2, 2.1, 2.2});\n+  m.SetPositions<int32_t>({1, -1, 1, 1});\n+  EXPECT_EQ(m.Invoke(), kTfLiteError);\n+}\n+\n TEST(GatherNdOpTest, SliceIndexingIntoMatrix) {\n   GatherNdOpModel m({TensorType_FLOAT32, {2, 2}}, {TensorType_INT32, {2, 1}});\n   m.SetInput<float>({1.1, 1.2, 2.1, 2.2});\n",
            "@@ -29,6 +29,7 @@ limitations under the License.\n #include \"third_party/eigen3/Eigen/Core\"\n #include \"fixedpoint/fixedpoint.h\"\n #include \"ruy/profiler/instrumentation.h\"  // from @ruy\n+#include \"tensorflow/lite/c/c_api_types.h\"\n #include \"tensorflow/lite/c/common.h\"\n #include \"tensorflow/lite/kernels/internal/common.h\"\n #include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n@@ -595,23 +596,31 @@ inline GatherNdHelperResult GatherNdHelper(const RuntimeShape& params_shape,\n   return ret;\n }\n \n+// Implements GatherNd.\n+// Returns an error if any of the indices_data would cause an out of bounds\n+// memory read.\n template <typename ParamsT, typename IndicesT = int32>\n-inline void GatherNd(const RuntimeShape& params_shape,\n-                     const ParamsT* params_data,\n-                     const RuntimeShape& indices_shape,\n-                     const IndicesT* indices_data,\n-                     const RuntimeShape& output_shape, ParamsT* output_data) {\n+inline TfLiteStatus GatherNd(const RuntimeShape& params_shape,\n+                             const ParamsT* params_data,\n+                             const RuntimeShape& indices_shape,\n+                             const IndicesT* indices_data,\n+                             const RuntimeShape& output_shape,\n+                             ParamsT* output_data) {\n   ruy::profiler::ScopeLabel label(\"GatherNd\");\n \n   const GatherNdHelperResult res = GatherNdHelper(params_shape, indices_shape);\n   for (int i = 0; i < res.n_slices; ++i) {\n-    int from_pos = 0;\n+    int64_t from_pos = 0;\n     for (int j = 0; j < res.indices_nd; ++j) {\n       from_pos += indices_data[i * res.indices_nd + j] * res.dims_to_count[j];\n     }\n+    if (from_pos < 0 || from_pos + res.slice_size > params_shape.FlatSize()) {\n+      return kTfLiteError;\n+    }\n     std::memcpy(output_data + i * res.slice_size, params_data + from_pos,\n                 sizeof(ParamsT) * res.slice_size);\n   }\n+  return kTfLiteOk;\n }\n \n #ifndef TF_LITE_STATIC_MEMORY\n"
        ],
        "Title": "\n          OOB read in `Gather_nd` op in TF Lite\n        "
    },
    {
        "Bug description": "The implementation of SobolSampleOp is vulnerable to a denial of service via CHECK-failure (assertion failure) caused by assuming  input(0) ,  input(1) , and  input(2)  to be scalar.",
        "Sample Code": " tensorflow as tf\ntf.raw_ops.SobolSample(dim=tf.constant([1,0]), num_results=tf.constant([1]), skip=tf.constant([1]))",
        "Bug fix": [
            "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"sobol_data.h\"  // from @sobol_data\n #include \"tensorflow/core/framework/device_base.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/lib/core/threadpool.h\"\n #include \"tensorflow/core/platform/platform_strings.h\"\n \n@@ -134,8 +135,14 @@ class SobolSampleOp : public OpKernel {\n       : OpKernel(context) {}\n \n   void Compute(OpKernelContext* context) override {\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(0).shape()),\n+                errors::InvalidArgument(\"dim must be a scalar\"));\n     int32_t dim = context->input(0).scalar<int32_t>()();\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(0).shape()),\n+                errors::InvalidArgument(\"num_results must be a scalar\"));\n     int32_t num_results = context->input(1).scalar<int32_t>()();\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(0).shape()),\n+                errors::InvalidArgument(\"skip must be a scalar\"));\n     int32_t skip = context->input(2).scalar<int32_t>()();\n \n     OP_REQUIRES(context, dim >= 1,\n"
        ],
        "Title": "\n          `CHECK` failure in `SobolSample` via missing validation\n        "
    },
    {
        "Bug description": "The implementation of tf.reshape op in TensorFlow is vulnerable to a denial of service via CHECK-failure (assertion failure) caused by overflowing the number of elements in a tensor:",
        "Sample Code": " tensorflow as tf\n\ntf.reshape(tensor=[[1]],shape=tf.constant([0 for i in range(255)], dtype=tf.int64))",
        "Bug fix": [
            "@@ -45,6 +45,11 @@ class ReshapeOp : public OpKernel {\n          TensorShapeUtils::IsScalar(sizes.shape())),\n         errors::InvalidArgument(\"sizes input must be 1-D, not \",\n                                 sizes.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, sizes.NumElements() < TensorShape::MaxDimensions(),\n+        errors::InvalidArgument(\"too many dimensions: must be < \",\n+                                TensorShape::MaxDimensions(), \", but received \",\n+                                sizes.NumElements()));\n \n     // Compute the output shape.  Determine product of specified\n     // dimensions, and find the index of the unspecified one.\n",
            "@@ -351,6 +351,15 @@ class OperatorShapeTest(test_util.TensorFlowTestCase):\n                                 \"must be a tensor with a single value\"):\n       array_ops.expand_dims(1, axis=[0, 1])\n \n+  def testReshapeWithManyDims(self):\n+    with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                \"too many dimensions\"):\n+      self.evaluate(\n+          array_ops.reshape(\n+              tensor=[[1]],\n+              shape=constant_op.constant([1 for i in range(254)],\n+                                         dtype=dtypes.int64)))\n+\n \n @test_util.with_eager_op_as_function\n class ReverseV2Test(test_util.TensorFlowTestCase):\n"
        ],
        "Title": "\n          `CHECK` failure in tf.reshape via overflows\n        "
    },
    {
        "Bug description": "The  TensorKey  used total estimated  AllocatedBytes() , which (a) is an estimate per tensor, and (b) is a very poor hash function for constants (e.g.  int32_t ).  It also tried to access individual tensor bytes through  tensor.data()  of size  AllocatedBytes() .  This led to ASAN failures because the  AllocatedBytes()  is an estimate of total bytes allocated by a tensor, including any pointed-to constructs (e.g. strings), and does not refer to contiguous bytes in the  .data()  buffer.  We couldn't use this byte vector anyways, since types like  tstring  include pointers, whereas we need to hash the string values themselves.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -16,6 +16,7 @@ limitations under the License.\n #define TENSORFLOW_CORE_FRAMEWORK_TENSOR_KEY_H_\n \n #include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/types.h\"\n \n namespace tensorflow {\n \n@@ -32,8 +33,7 @@ class TensorKey : public Tensor {\n     }\n     if (DataTypeCanUseMemcpy(t1.dtype())) {\n       return t1.tensor_data() == t2.tensor_data();\n-    }\n-    if (t1.dtype() == DT_STRING) {\n+    } else if (t1.dtype() == DT_STRING) {\n       const auto s1 = t1.unaligned_flat<tstring>();\n       const auto s2 = t2.unaligned_flat<tstring>();\n       for (int64_t i = 0, n = t1.NumElements(); i < n; ++i) {\n@@ -42,6 +42,9 @@ class TensorKey : public Tensor {\n         }\n       }\n       return true;\n+    } else {\n+      DCHECK(false) << \"Unimplemented dtype \" << DataTypeString(t1.dtype())\n+                    << std::endl;\n     }\n     return false;\n   }\n@@ -53,14 +56,19 @@ class TensorKey : public Tensor {\n   // Needed for absl hash function.\n   template <typename H>\n   friend H AbslHashValue(H h, const TensorKey& k) {\n-    const uint8* d = static_cast<uint8*>(k.data());\n-    size_t s = k.AllocatedBytes();\n-    std::vector<uint8> vec;\n-    vec.reserve(s);\n-    for (int i = 0; i < s; i++) {\n-      vec.push_back(d[i]);\n+    if (DataTypeCanUseMemcpy(k.dtype())) {\n+      return H::combine(std::move(h), k.tensor_data());\n+    } else if (k.dtype() == DT_STRING) {\n+      const auto strs = k.unaligned_flat<tstring>();\n+      for (int64_t i = 0, n = k.NumElements(); i < n; ++i) {\n+        h = H::combine(std::move(h), strs(i));\n+      }\n+      return h;\n+    } else {\n+      DCHECK(false) << \"Unimplemented dtype \" << DataTypeString(k.dtype())\n+                    << std::endl;\n     }\n-    return H::combine(std::move(h), s);\n+    return h;\n   }\n };\n \n",
            "@@ -165,8 +165,6 @@ tf_py_test(\n     grpc_enabled = True,\n     tags = [\n         \"no_windows\",  # TODO(b/192259628)\n-        \"noasan\",  # TODO(b/164696004)\n-        \"notsan\",  # TODO(b/164696004)\n     ],\n     deps = [\n         \"//tensorflow/python:array_ops\",\n"
        ],
        "Title": "\n          Heap buffer overflow due to incorrect hash function\n        "
    },
    {
        "Bug description": "The  macros that TensorFlow uses for writing assertions (e.g.,   have an incorrect logic when comparing  size_t  and  int  values. Due to type conversion rules, several of the macros would trigger incorrectly.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Type confusion leading to `CHECK`-failure based denial of service\n        "
    },
    {
        "Bug description": "The  tf.compat.v1.signal.rfft2d  and  tf.compat.v1.signal.rfft3d  lack input validation and under certain condition can result in crashes (due to  CHECK -failures).",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Incomplete validation in signal ops leads to crashes\n        "
    }
]
[
    {
        "Bug description": "Certain TFLite models that were created using TFLite model converter would crash when loaded in the TFLite interpreter. The culprit is that during quantization the scale of values could be greater than 1 but code was always assuming sub-unit scaling.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -81,6 +81,17 @@ TfLiteStatus ComparisonPrepareStringAllowed(TfLiteContext* context,\n   return ComparisonPrepareCommon(context, node, true);\n }\n \n+void QuantizeMultiplier(double double_multiplier, int32_t* quantized_multiplier,\n+                        int* left_shift) {\n+  if (double_multiplier < 1.0) {\n+    QuantizeMultiplierSmallerThanOneExp(double_multiplier, quantized_multiplier,\n+                                        left_shift);\n+  } else {\n+    QuantizeMultiplierGreaterThanOne(double_multiplier, quantized_multiplier,\n+                                     left_shift);\n+  }\n+}\n+\n template <typename input_dtype, reference_ops::ComparisonFn<int32> opname>\n void ComparisonQuantized(const TfLiteTensor* input1, const TfLiteTensor* input2,\n                          TfLiteTensor* output, bool requires_broadcast) {\n@@ -90,13 +101,11 @@ void ComparisonQuantized(const TfLiteTensor* input1, const TfLiteTensor* input2,\n     const int left_shift = 8;\n \n     int32 input1_multiplier;\n-    int input1_shift;\n-    QuantizeMultiplierSmallerThanOneExp(input1->params.scale,\n-                                        &input1_multiplier, &input1_shift);\n     int32 input2_multiplier;\n+    int input1_shift;\n     int input2_shift;\n-    QuantizeMultiplierSmallerThanOneExp(input2->params.scale,\n-                                        &input2_multiplier, &input2_shift);\n+    QuantizeMultiplier(input1->params.scale, &input1_multiplier, &input1_shift);\n+    QuantizeMultiplier(input2->params.scale, &input2_multiplier, &input2_shift);\n \n     ComparisonParams op_params;\n     op_params.left_shift = left_shift;\n",
            "@@ -653,6 +653,26 @@ TEST(ComparisonsTest, QuantizedInt8GreaterWithBroadcast) {\n   }\n }\n \n+TEST(ComparisonsTest,\n+     QuantizedInt8GreaterWithBroadcastMultiplierGreaterThanOne) {\n+  const float kMin = -127.f;\n+  const float kMax = 127.f;\n+  std::vector<std::vector<int>> test_shapes = {\n+      {6}, {2, 3}, {2, 1, 3}, {1, 3, 1, 2}};\n+  for (int i = 0; i < test_shapes.size(); ++i) {\n+    ComparisonOpModel model({TensorType_INT8, test_shapes[i], kMin, kMax},\n+                            {TensorType_INT8, {}, kMin, kMax}, TensorType_INT8,\n+                            BuiltinOperator_GREATER);\n+    model.QuantizeAndPopulate<int8_t>(model.input1(),\n+                                      {572, -2, -71, 8, 11, 20});\n+    model.QuantizeAndPopulate<int8_t>(model.input2(), {8});\n+    model.Invoke();\n+    EXPECT_THAT(model.GetOutput(),\n+                ElementsAre(true, false, false, false, true, true))\n+        << \"With shape number \" << i;\n+  }\n+}\n+\n TEST(ComparisonsTest, QuantizedUInt8GreaterEqualWithBroadcast) {\n   const float kMin = -1.f;\n   const float kMax = 128.f;\n"
        ],
        "Title": "\n          Core dump when loading TFLite models with quantization\n        "
    },
    {
        "Bug description": "The implementation of  tf.histogram_fixed_width  is vulnerable to a crash when the values array contain  NaN  elements:",
        "Sample Code": "import numpy as np\n\n\n\ntf.histogram_fixed_width(values=np.nan, value_range=[1,2])",
        "Bug fix": [
            "@@ -50,6 +50,15 @@ struct HistogramFixedWidthFunctor<CPUDevice, T, Tout> {\n                         static_cast<double>(nbins);\n     const double nbins_minus_1 = static_cast<double>(nbins - 1);\n \n+    // We cannot handle NANs in the algorithm below (due to the case to int32)\n+    const Eigen::Tensor<int32, 1, 1> nans_tensor =\n+        values.isnan().template cast<int32>();\n+    const Eigen::Tensor<int32, 0, 1> reduced_tensor = nans_tensor.sum();\n+    const int num_nans = reduced_tensor(0);\n+    if (num_nans > 0) {\n+      return errors::InvalidArgument(\"Histogram values must not contain NaN\");\n+    }\n+\n     // The calculation is done by finding the slot of each value in `values`.\n     // With [a, b]:\n     //   step = (b - a) / nbins\n@@ -98,12 +107,12 @@ class HistogramFixedWidthOp : public OpKernel {\n     const auto nbins = nbins_tensor.scalar<int32>()();\n \n     OP_REQUIRES(\n-        ctx, (value_range(0) < value_range(1)),\n+        ctx, value_range(0) < value_range(1),\n         errors::InvalidArgument(\"value_range should satisfy value_range[0] < \"\n                                 \"value_range[1], but got '[\",\n                                 value_range(0), \", \", value_range(1), \"]'\"));\n     OP_REQUIRES(\n-        ctx, (nbins > 0),\n+        ctx, nbins > 0,\n         errors::InvalidArgument(\"nbins should be a positive number, but got '\",\n                                 nbins, \"'\"));\n \n"
        ],
        "Title": "\n          Segfault if `tf.histogram_fixed_width` is called with NaN values\n        "
    },
    {
        "Bug description": "The implementation of  tf.ragged.constant  does not fully validate the input arguments. This results in a denial of service by consuming all available memory:",
        "Sample Code": " tensorflow as tf\ntf.ragged.constant(pylist=[],ragged_rank=8968073515812833920)",
        "Bug fix": [
            "@@ -188,6 +188,9 @@ def _constant_value(ragged_factory, inner_factory, pylist, dtype, ragged_rank,\n     if max_depth > scalar_depth:\n       raise ValueError(\"Invalid pylist=%r: empty list nesting is greater \"\n                        \"than scalar value nesting\" % pylist)\n+    if ragged_rank is not None and max_depth < ragged_rank:\n+      raise ValueError(f\"Invalid pylist={pylist}, max depth smaller than \"\n+                       f\"ragged_rank={ragged_rank}\")\n \n   # If both inner_shape and ragged_rank were specified, then check that\n   # they are compatible with pylist.\n"
        ],
        "Title": "\n          Denial of service in `tf.ragged.constant` due to lack of validation\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.UnsortedSegmentJoin  does not fully validate the input arguments. This results in a  CHECK -failure which can be used to trigger a denial of service attack:",
        "Sample Code": "tf.strings.unsorted_segment_join(\n  inputs=['123'],\n  segment_ids=[0],\n  ],\n  num_segments=-1)",
        "Bug fix": [
            "@@ -94,6 +94,8 @@ class UnsortedSegmentJoinOp : public OpKernel {\n                 errors::InvalidArgument(\"Number of segments cannot be empty.\"));\n     auto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();\n \n+    OP_REQUIRES(context, num_segments > 0,\n+                errors::InvalidArgument(\"Number of segments must be positive\"));\n     OP_REQUIRES(context, segment_dims != 0,\n                 errors::InvalidArgument(\"Segment_id cannot have rank 0\"));\n \n"
        ],
        "Title": "\n          Missing validation causes denial of service via `Conv3DBackpropFilterV2`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.EditDistance  has incomplete validation. Users can pass negative values to cause a segmentation fault based denial of service:",
        "Sample Code": "hypothesis_indices = tf.constant(-1250999896764, shape=[3, 3], dtype=tf.int64) \nhypothesis_values = tf.constant(0, shape=[3], dtype=tf.int64)\nhypothesis_shape = tf.constant(0, shape=[3], dtype=tf.int64)\n\ntruth_indices = tf.constant(-1250999896764, shape=[3, 3], dtype=tf.int64)\ntruth_values = tf.constant(2, shape=[3], dtype=tf.int64)\ntruth_shape = tf.constant(2, shape=[3], dtype=tf.int64) \n\ntf.raw_ops.EditDistance(\n  hypothesis_indices=hypothesis_indices,\n  hypothesis_values=hypothesis_values,\n  hypothesis_shape=hypothesis_shape,\n  truth_indices=truth_indices,\n  truth_values=truth_values,\n  ,\n  truth_shape=truth_shape)",
        "Bug fix": [
            "@@ -203,9 +203,9 @@ class EditDistanceOp : public OpKernel {\n         auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                       output_strides.begin(), int64_t{0});\n         OP_REQUIRES(\n-            ctx, loc < output_elements,\n+            ctx, 0 <= loc && loc < output_elements,\n             errors::Internal(\"Got an inner product \", loc,\n-                             \" which would require in writing to outside of \"\n+                             \" which would require writing to outside of \"\n                              \"the buffer for the output tensor (max elements \",\n                              output_elements, \")\"));\n         output_t(loc) =\n@@ -218,9 +218,9 @@ class EditDistanceOp : public OpKernel {\n         auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                       output_strides.begin(), int64_t{0});\n         OP_REQUIRES(\n-            ctx, loc < output_elements,\n+            ctx, 0 <= loc && loc < output_elements,\n             errors::Internal(\"Got an inner product \", loc,\n-                             \" which would require in writing to outside of \"\n+                             \" which would require writing to outside of \"\n                              \"the buffer for the output tensor (max elements \",\n                              output_elements, \")\"));\n         output_t(loc) = hypothesis_seq.size();\n@@ -232,9 +232,9 @@ class EditDistanceOp : public OpKernel {\n         auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                       output_strides.begin(), int64_t{0});\n         OP_REQUIRES(\n-            ctx, loc < output_elements,\n+            ctx, 0 <= loc && loc < output_elements,\n             errors::Internal(\"Got an inner product \", loc,\n-                             \" which would require in writing to outside of \"\n+                             \" which would require writing to outside of \"\n                              \"the buffer for the output tensor (max elements \",\n                              output_elements, \")\"));\n         output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n@@ -248,9 +248,9 @@ class EditDistanceOp : public OpKernel {\n       auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                     output_strides.begin(), int64_t{0});\n       OP_REQUIRES(\n-          ctx, loc < output_elements,\n+          ctx, 0 <= loc && loc < output_elements,\n           errors::Internal(\"Got an inner product \", loc,\n-                           \" which would require in writing to outside of the \"\n+                           \" which would require writing to outside of the \"\n                            \"buffer for the output tensor (max elements \",\n                            output_elements, \")\"));\n       output_t(loc) = hypothesis_seq.size();\n@@ -266,9 +266,9 @@ class EditDistanceOp : public OpKernel {\n       auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                     output_strides.begin(), int64_t{0});\n       OP_REQUIRES(\n-          ctx, loc < output_elements,\n+          ctx, 0 <= loc && loc < output_elements,\n           errors::Internal(\"Got an inner product \", loc,\n-                           \" which would require in writing to outside of the \"\n+                           \" which would require writing to outside of the \"\n                            \"buffer for the output tensor (max elements \",\n                            output_elements, \")\"));\n       output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n",
            "@@ -207,6 +207,24 @@ class EditDistanceTest(test.TestCase):\n         normalize=True,\n         expected_output=expected_output)\n \n+  def testEditDistanceBadIndices(self):\n+    hypothesis_indices = np.full((3, 3), -1250999896764, dtype=np.int64)\n+    hypothesis_values = np.empty(3, dtype=np.int64)\n+    hypothesis_shape = np.empty(3, dtype=np.int64)\n+    truth_indices = np.full((3, 3), -1250999896764, dtype=np.int64)\n+    truth_values = np.full([3], 2, dtype=np.int64)\n+    truth_shape = np.full([3], 2, dtype=np.int64)\n+    expected_output = []  # dummy; ignored\n+\n+    self._testEditDistance(\n+        hypothesis=(hypothesis_indices, hypothesis_values, hypothesis_shape),\n+        truth=(truth_indices, truth_values, truth_shape),\n+        normalize=False,\n+        expected_output=expected_output,\n+        expected_err_re=(r\"inner product -\\d+ which would require writing \"\n+                         \"to outside of the buffer for the output tensor\")\n+    )\n+\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          Segfault and OOB write due to incomplete validation in `EditDistance`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.SpaceToBatchND  (in all backends such as XLA and handwritten kernels) is vulnerable to an integer overflow:",
        "Sample Code": "input = tf.constant(-3.5e+35, shape=[10,19,22], dtype=tf.float32)\nblock_shape = tf.constant(-1879048192, shape=[2], dtype=tf.int64)\npaddings = tf.constant(0, shape=[2,2], dtype=tf.int32)\n)\ntf.raw_ops.SpaceToBatchND(input=input, block_shape=block_shape, paddings=paddings)",
        "Bug fix": [
            "@@ -17,6 +17,7 @@\n import numpy as np\n \n from tensorflow.compiler.tests import xla_test\n+from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import gen_array_ops\n@@ -145,6 +146,29 @@ class SpaceToBatchTest(xla_test.XLATestCase):\n     self._testOne(x_np, block_size, x_out)\n \n \n+class SpaceToBatchNDErrorHandlingTest(xla_test.XLATestCase):\n+\n+  def testInvalidBlockShape(self):\n+    with self.assertRaisesRegex(ValueError, \"block_shape must be positive\"):\n+      with self.session() as sess, self.test_scope():\n+        tf_in = constant_op.constant(\n+            -3.5e+35, shape=[10, 20, 20], dtype=dtypes.float32)\n+        block_shape = constant_op.constant(-10, shape=[2], dtype=dtypes.int64)\n+        paddings = constant_op.constant(0, shape=[2, 2], dtype=dtypes.int32)\n+        sess.run(array_ops.space_to_batch_nd(tf_in, block_shape, paddings))\n+\n+  def testOutputSizeOutOfBounds(self):\n+    with self.assertRaisesRegex(ValueError,\n+                                \"Negative.* dimension size caused by overflow\"):\n+      with self.session() as sess, self.test_scope():\n+        tf_in = constant_op.constant(\n+            -3.5e+35, shape=[10, 19, 22], dtype=dtypes.float32)\n+        block_shape = constant_op.constant(\n+            1879048192, shape=[2], dtype=dtypes.int64)\n+        paddings = constant_op.constant(0, shape=[2, 2], dtype=dtypes.int32)\n+        sess.run(array_ops.space_to_batch_nd(tf_in, block_shape, paddings))\n+\n+\n class SpaceToBatchNDTest(xla_test.XLATestCase):\n   \"\"\"Tests input-output pairs for the SpaceToBatchND and BatchToSpaceND ops.\"\"\"\n \n",
            "@@ -211,6 +211,7 @@ tf_kernel_library(\n         \"//tensorflow/core/kernels:stateful_random_ops_header\",\n         \"//tensorflow/core/kernels:stateless_random_ops_v2_header\",\n         \"//tensorflow/core/tpu:tpu_defs\",\n+        \"//tensorflow/core/util:overflow\",\n         \"//tensorflow/stream_executor/lib\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/container:flat_hash_map\",\n",
            "@@ -17,6 +17,7 @@ limitations under the License.\n #include \"tensorflow/compiler/tf2xla/xla_op_kernel.h\"\n #include \"tensorflow/compiler/tf2xla/xla_op_registry.h\"\n #include \"tensorflow/compiler/xla/client/xla_builder.h\"\n+#include \"tensorflow/core/util/overflow.h\"\n \n namespace tensorflow {\n namespace {\n@@ -60,10 +61,14 @@ void SpaceToBatch(XlaOpKernelContext* ctx, const xla::XlaOp& input,\n     int64_t pad_end = paddings.Get<int64_t>({i, 1});\n     OP_REQUIRES(ctx, pad_start >= 0 && pad_end >= 0,\n                 errors::InvalidArgument(\"Paddings must be non-negative\"));\n+    OP_REQUIRES(ctx, block_shape[i] >= 1,\n+                errors::InvalidArgument(\n+                    \"All values in block_shape must be positive, got value, \",\n+                    block_shape[i], \" at index \", i, \".\"));\n     dim->set_edge_padding_low(pad_start);\n     dim->set_edge_padding_high(pad_end);\n     padded_shape[1 + i] += pad_start + pad_end;\n-    block_num_elems *= block_shape[i];\n+    block_num_elems = MultiplyWithoutOverflow(block_num_elems, block_shape[i]);\n   }\n   // Don't pad the remainder dimensions.\n   for (int i = 0; i < remainder_shape.size(); ++i) {\n@@ -72,6 +77,16 @@ void SpaceToBatch(XlaOpKernelContext* ctx, const xla::XlaOp& input,\n   OP_REQUIRES(ctx, block_num_elems > 0,\n               errors::InvalidArgument(\n                   \"The product of the block dimensions must be positive\"));\n+  const int64_t batch_size = input_shape[0];\n+  const int64_t output_dim =\n+      MultiplyWithoutOverflow(batch_size, block_num_elems);\n+  if (output_dim < 0) {\n+    OP_REQUIRES(\n+        ctx, output_dim >= 0,\n+        errors::InvalidArgument(\"Negative output dimension size caused by \"\n+                                \"overflow when multiplying \",\n+                                batch_size, \" and \", block_num_elems));\n+  }\n \n   xla::XlaOp padded =\n       xla::Pad(input, XlaHelpers::Zero(b, input_dtype), padding_config);\n@@ -85,7 +100,6 @@ void SpaceToBatch(XlaOpKernelContext* ctx, const xla::XlaOp& input,\n   //       padded_shape[M] / block_shape[M-1],\n   //       block_shape[M-1]] +\n   //      remaining_shape\n-  const int64_t batch_size = input_shape[0];\n   std::vector<int64_t> reshaped_padded_shape(input_rank + block_rank);\n   reshaped_padded_shape[0] = batch_size;\n   for (int i = 0; i < block_rank; ++i) {\n@@ -134,7 +148,7 @@ void SpaceToBatch(XlaOpKernelContext* ctx, const xla::XlaOp& input,\n   // Determine the length of the prefix of block dims that can be combined\n   // into the batch dimension due to having no padding and block_shape=1.\n   std::vector<int64_t> output_shape(input_rank);\n-  output_shape[0] = batch_size * block_num_elems;\n+  output_shape[0] = output_dim;\n   for (int i = 0; i < block_rank; ++i) {\n     output_shape[1 + i] = padded_shape[1 + i] / block_shape[i];\n   }\n",
            "@@ -891,6 +891,7 @@ cc_library(\n         \"//tensorflow/core/lib/strings:scanner\",\n         \"//tensorflow/core/lib/strings:str_util\",\n         \"//tensorflow/core/platform:macros\",\n+        \"//tensorflow/core/util:overflow\",\n         \"@com_google_absl//absl/memory\",\n     ],\n )\n",
            "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"tensorflow/core/lib/strings/numbers.h\"\n #include \"tensorflow/core/lib/strings/scanner.h\"\n #include \"tensorflow/core/lib/strings/str_util.h\"\n+#include \"tensorflow/core/util/overflow.h\"\n \n namespace tensorflow {\n namespace shape_inference {\n@@ -1111,7 +1112,7 @@ Status InferenceContext::Multiply(DimensionHandle first,\n     *out = UnknownDim();\n   } else {\n     // Invariant: Both values are known and greater than 1.\n-    const int64_t product = first_value * second_value;\n+    const int64_t product = MultiplyWithoutOverflow(first_value, second_value);\n     if (product < 0) {\n       return errors::InvalidArgument(\n           \"Negative dimension size caused by overflow when multiplying \",\n",
            "@@ -29,6 +29,7 @@ load(\n load(\n     \"//third_party/mkl:build_defs.bzl\",\n     \"if_mkl\",\n+    \"mkl_deps\",\n )\n \n # buildifier: disable=same-origin-load\n@@ -61,10 +62,6 @@ load(\n     \"//tensorflow/core/platform:build_config_root.bzl\",\n     \"tf_cuda_tests_tags\",\n )\n-load(\n-    \"//third_party/mkl:build_defs.bzl\",\n-    \"mkl_deps\",\n-)\n load(\"@local_config_cuda//cuda:build_defs.bzl\", \"if_cuda\")\n load(\n     \"@local_config_rocm//rocm:build_defs.bzl\",\n@@ -4569,6 +4566,7 @@ tf_kernel_library(\n         \"//tensorflow/core:framework\",\n         \"//tensorflow/core:lib\",\n         \"//tensorflow/core/framework:bounds_check\",\n+        \"//tensorflow/core/util:overflow\",\n         \"//third_party/eigen3\",\n     ],\n )\n",
            "@@ -21,8 +21,6 @@ limitations under the License.\n #include <string>\n #include <utility>\n \n-#include \"tensorflow/core/kernels/spacetobatch_functor.h\"\n-\n #include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n #include \"tensorflow/core/framework/op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n@@ -31,8 +29,10 @@ limitations under the License.\n #include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/tensor_types.h\"\n #include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/spacetobatch_functor.h\"\n #include \"tensorflow/core/platform/logging.h\"\n #include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/core/util/overflow.h\"\n \n namespace tensorflow {\n \n@@ -99,7 +99,13 @@ Status SpaceToBatchOpCompute(OpKernelContext* context,\n   // Compute the product of the block_shape values.\n   int64_t block_shape_product = 1;\n   for (int block_dim = 0; block_dim < block_dims; ++block_dim) {\n-    block_shape_product *= block_shape[block_dim];\n+    if (block_shape[block_dim] < 1) {\n+      return errors::InvalidArgument(\n+          \"All values in block_shape must be positive, got value, \",\n+          block_shape[block_dim], \" at index \", block_dim, \".\");\n+    }\n+    block_shape_product =\n+        MultiplyWithoutOverflow(block_shape_product, block_shape[block_dim]);\n   }\n   if (block_shape_product <= 0) {\n     return errors::InvalidArgument(\n@@ -131,8 +137,14 @@ Status SpaceToBatchOpCompute(OpKernelContext* context,\n   // The actual output shape exposed to callers.\n   TensorShape external_output_shape;\n \n-  external_output_shape.AddDim(orig_input_tensor.dim_size(0) *\n-                               block_shape_product);\n+  const int64_t output_shape = MultiplyWithoutOverflow(\n+      orig_input_tensor.dim_size(0), block_shape_product);\n+  if (output_shape < 0) {\n+    return errors::InvalidArgument(\n+        \"Negative output dimension size caused by overflow when multiplying \",\n+        orig_input_tensor.dim_size(0), \" and \", block_shape_product);\n+  }\n+  external_output_shape.AddDim(output_shape);\n \n   int64_t input_batch_size = orig_input_tensor.dim_size(0);\n   for (int block_dim = 0; block_dim < removed_prefix_block_dims; ++block_dim) {\n",
            "@@ -533,6 +533,9 @@ tf_cuda_library(\n cc_library(\n     name = \"overflow\",\n     hdrs = [\"overflow.h\"],\n+    visibility = [\n+        \"//tensorflow:internal\",\n+    ],\n     deps = [\n         \"//tensorflow/core/platform:logging\",\n         \"//tensorflow/core/platform:macros\",\n",
            "@@ -16,7 +16,9 @@\n \n import numpy as np\n \n+from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import tensor_util\n from tensorflow.python.framework import test_util\n@@ -516,6 +518,27 @@ class SpaceToBatchNDErrorHandlingTest(test.TestCase):\n             dtypes.float32, shape=(3, 2, 3, 2)), [2, 3], [[1, 1], [0, 0]])\n     self.assertEqual([3 * 2 * 3, 2, 1, 2], t.get_shape().as_list())\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def testInvalidBlockShape(self):\n+    tf_in = constant_op.constant(\n+        -3.5e+35, shape=[10, 20, 20], dtype=dtypes.float32)\n+    block_shape = constant_op.constant(-10, shape=[2], dtype=dtypes.int64)\n+    paddings = constant_op.constant(0, shape=[2, 2], dtype=dtypes.int32)\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"block_shape must be positive\"):\n+      array_ops.space_to_batch_nd(tf_in, block_shape, paddings)\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def testOutputSizeOutOfBounds(self):\n+    tf_in = constant_op.constant(\n+        -3.5e+35, shape=[10, 19, 22], dtype=dtypes.float32)\n+    block_shape = constant_op.constant(\n+        1879048192, shape=[2], dtype=dtypes.int64)\n+    paddings = constant_op.constant(0, shape=[2, 2], dtype=dtypes.int32)\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Negative.* dimension size caused by overflow\"):\n+      array_ops.space_to_batch_nd(tf_in, block_shape, paddings)\n+\n \n class SpaceToBatchGradientTest(test.TestCase, PythonOpImpl):\n \n"
        ],
        "Title": "\n          Integer overflow in `SpaceToBatchND`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.QuantizedConv2D  does not fully validate the input arguments:",
        "Sample Code": "input = tf.constant(1, shape=[1, 2, 3, 3], dtype=tf.quint8)\nfilter = tf.constant(1, shape=[1, 2, 3, 3], dtype=tf.quint8)\n\n# bad args\nmin_input = tf.constant([], shape=[0], dtype=tf.float32)\nmax_input = tf.constant(0, shape=[], dtype=tf.float32)\nmin_filter = tf.constant(0, shape=[], dtype=tf.float32)\nmax_filter = tf.constant(0, shape=[], dtype=tf.float32)\n\ntf.raw_ops.QuantizedConv2D(\n  input=input,\n  filter=filter,\n  min_input=min_input,\n  max_input=max_input,\n  min_filter=min_filter,\n  max_filter=max_filter, \n  strides=[1, 1, 1, 1],\n  ],\n  padding=\"SAME\")",
        "Bug fix": [
            "@@ -18,8 +18,6 @@ limitations under the License.\n #include <algorithm>\n #include <vector>\n \n-#include \"tensorflow/core/platform/errors.h\"\n-\n #define EIGEN_USE_THREADS\n \n #define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\n@@ -32,6 +30,7 @@ limitations under the License.\n #include \"tensorflow/core/kernels/quantization_utils.h\"\n #include \"tensorflow/core/kernels/reference_gemm.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/util/padding.h\"\n \n namespace tensorflow {\n@@ -499,11 +498,26 @@ class QuantizedConv2DOp : public OpKernel {\n \n     // For 2D convolution, there should be 4 dimensions.\n     OP_REQUIRES(context, input.dims() == 4,\n-                errors::InvalidArgument(\"input must be 4-dimensional\",\n-                                        input.shape().DebugString()));\n+                errors::InvalidArgument(\"input must be rank 4 but is rank \",\n+                                        input.shape().dims()));\n     OP_REQUIRES(context, filter.dims() == 4,\n-                errors::InvalidArgument(\"filter must be 4-dimensional: \",\n-                                        filter.shape().DebugString()));\n+                errors::InvalidArgument(\"filter must be rank 4 but is rank \",\n+                                        filter.shape().dims()));\n+\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(2).shape()),\n+                errors::InvalidArgument(\"min_input must be rank 0 but is rank \",\n+                                        context->input(2).shape().dims()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(3).shape()),\n+                errors::InvalidArgument(\"max_input must be rank 0 but is rank \",\n+                                        context->input(3).shape().dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(context->input(4).shape()),\n+        errors::InvalidArgument(\"min_filter must be rank 0 but is rank \",\n+                                context->input(4).shape().dims()));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsScalar(context->input(5).shape()),\n+        errors::InvalidArgument(\"max_filter must be rank 0 but is rank \",\n+                                context->input(5).shape().dims()));\n \n     const float min_input = context->input(2).flat<float>()(0);\n     const float max_input = context->input(3).flat<float>()(0);\n",
            "@@ -91,10 +91,10 @@ TEST_F(QuantizedConv2DTest, Small) {\n                             image_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(filter_quantized.shape(),\n                             filter_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {image_min});\n-  AddInputFromArray<float>(TensorShape({1}), {image_max});\n-  AddInputFromArray<float>(TensorShape({1}), {filter_min});\n-  AddInputFromArray<float>(TensorShape({1}), {filter_max});\n+  AddInputFromArray<float>(TensorShape({}), {image_min});\n+  AddInputFromArray<float>(TensorShape({}), {image_max});\n+  AddInputFromArray<float>(TensorShape({}), {filter_min});\n+  AddInputFromArray<float>(TensorShape({}), {filter_max});\n   TF_ASSERT_OK(RunOpKernel());\n \n   // We're sliding the 3x3 filter across the 3x4 image, with accesses outside\n@@ -158,10 +158,10 @@ TEST_F(QuantizedConv2DTest, Small32Bit) {\n   AddInputFromArray<quint8>(\n       TensorShape({filter_size, filter_size, depth, filter_count}),\n       {10, 40, 70, 20, 50, 80, 30, 60, 90});\n-  AddInputFromArray<float>(TensorShape({1}), {0});\n-  AddInputFromArray<float>(TensorShape({1}), {255.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {0});\n-  AddInputFromArray<float>(TensorShape({1}), {255.0f});\n+  AddInputFromArray<float>(TensorShape({}), {0});\n+  AddInputFromArray<float>(TensorShape({}), {255.0f});\n+  AddInputFromArray<float>(TensorShape({}), {0});\n+  AddInputFromArray<float>(TensorShape({}), {255.0f});\n \n   TF_ASSERT_OK(RunOpKernel());\n   const int expected_width = image_width;\n@@ -201,10 +201,10 @@ TEST_F(QuantizedConv2DTest, OddPadding) {\n   AddInputFromArray<quint8>(\n       TensorShape({filter_size, filter_size, depth, filter_count}),\n       {1, 2, 3, 4, 5, 6, 7, 8, 9});\n-  AddInputFromArray<float>(TensorShape({1}), {0});\n-  AddInputFromArray<float>(TensorShape({1}), {255.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {0});\n-  AddInputFromArray<float>(TensorShape({1}), {255.0f});\n+  AddInputFromArray<float>(TensorShape({}), {0});\n+  AddInputFromArray<float>(TensorShape({}), {255.0f});\n+  AddInputFromArray<float>(TensorShape({}), {0});\n+  AddInputFromArray<float>(TensorShape({}), {255.0f});\n \n   TF_ASSERT_OK(RunOpKernel());\n   const int expected_width = image_width / stride;\n@@ -244,10 +244,10 @@ TEST_F(QuantizedConv2DTest, OddPaddingBatch) {\n   AddInputFromArray<quint8>(\n       TensorShape({filter_size, filter_size, depth, filter_count}),\n       {1, 2, 3, 4, 5, 6, 7, 8, 9});\n-  AddInputFromArray<float>(TensorShape({1}), {0});\n-  AddInputFromArray<float>(TensorShape({1}), {255.0f});\n-  AddInputFromArray<float>(TensorShape({1}), {0});\n-  AddInputFromArray<float>(TensorShape({1}), {255.0f});\n+  AddInputFromArray<float>(TensorShape({}), {0});\n+  AddInputFromArray<float>(TensorShape({}), {255.0f});\n+  AddInputFromArray<float>(TensorShape({}), {0});\n+  AddInputFromArray<float>(TensorShape({}), {255.0f});\n \n   TF_ASSERT_OK(RunOpKernel());\n   const int expected_width = image_width / stride;\n@@ -302,10 +302,10 @@ TEST_F(QuantizedConv2DTest, SmallWithNoZero) {\n                             image_quantized.flat<quint8>());\n   AddInputFromArray<quint8>(filter_quantized.shape(),\n                             filter_quantized.flat<quint8>());\n-  AddInputFromArray<float>(TensorShape({1}), {image_min});\n-  AddInputFromArray<float>(TensorShape({1}), {image_max});\n-  AddInputFromArray<float>(TensorShape({1}), {filter_min});\n-  AddInputFromArray<float>(TensorShape({1}), {filter_max});\n+  AddInputFromArray<float>(TensorShape({}), {image_min});\n+  AddInputFromArray<float>(TensorShape({}), {image_max});\n+  AddInputFromArray<float>(TensorShape({}), {filter_min});\n+  AddInputFromArray<float>(TensorShape({}), {filter_max});\n   TF_ASSERT_OK(RunOpKernel());\n   const int expected_width = image_width;\n   const int expected_height = image_height * filter_count;\n",
            "@@ -18,6 +18,8 @@ import numpy as np\n \n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n+from tensorflow.python.ops import math_ops\n from tensorflow.python.ops import nn_ops\n from tensorflow.python.platform import test\n \n@@ -196,6 +198,71 @@ class Conv2DTest(test.TestCase):\n         padding=\"SAME\",\n         expected=expected_output)\n \n+  def _testBadInputSize(self,\n+                        tin=None,\n+                        tfilter=None,\n+                        min_input=None,\n+                        max_input=None,\n+                        min_filter=None,\n+                        max_filter=None,\n+                        error_regex=\"\"):\n+    strides = [1, 1, 1, 1]\n+    padding = \"SAME\"\n+    if tin is None:\n+      tin = math_ops.cast(\n+          constant_op.constant(1, shape=[1, 2, 3, 3]), dtype=dtypes.quint8)\n+\n+    if tfilter is None:\n+      tfilter = math_ops.cast(\n+          constant_op.constant(1, shape=[1, 2, 3, 3]), dtype=dtypes.quint8)\n+\n+    if min_input is None:\n+      min_input = constant_op.constant(0, shape=[], dtype=dtypes.float32)\n+\n+    if max_input is None:\n+      max_input = constant_op.constant(0, shape=[], dtype=dtypes.float32)\n+\n+    if min_filter is None:\n+      min_filter = constant_op.constant(0, shape=[], dtype=dtypes.float32)\n+\n+    if max_filter is None:\n+      max_filter = constant_op.constant(0, shape=[], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                error_regex):\n+      self.evaluate(\n+          nn_ops.quantized_conv2d(\n+              tin,\n+              tfilter,\n+              out_type=dtypes.qint32,\n+              strides=strides,\n+              padding=padding,\n+              min_input=min_input,\n+              max_input=max_input,\n+              min_filter=min_filter,\n+              max_filter=max_filter))\n+\n+  def testBadInputSizes(self):\n+    self._testBadInputSize(\n+        tin=math_ops.cast(\n+            constant_op.constant(1, shape=[1, 2]), dtype=dtypes.quint8),\n+        error_regex=\"must be rank 4\")\n+    self._testBadInputSize(\n+        tfilter=math_ops.cast(\n+            constant_op.constant(1, shape=[1, 2]), dtype=dtypes.quint8),\n+        error_regex=\"must be rank 4\")\n+    self._testBadInputSize(\n+        min_input=constant_op.constant(0, shape=[1], dtype=dtypes.float32),\n+        error_regex=\"must be rank 0\")\n+    self._testBadInputSize(\n+        max_input=constant_op.constant(0, shape=[1], dtype=dtypes.float32),\n+        error_regex=\"must be rank 0\")\n+    self._testBadInputSize(\n+        min_filter=constant_op.constant(0, shape=[1], dtype=dtypes.float32),\n+        error_regex=\"must be rank 0\")\n+    self._testBadInputSize(\n+        max_filter=constant_op.constant(0, shape=[1], dtype=dtypes.float32),\n+        error_regex=\"must be rank 0\")\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          Missing validation results in undefined behavior in `QuantizedConv2D`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.SparseTensorDenseAdd  does not fully validate the input arguments:",
        "Sample Code": "a_indices = tf.constant(0, shape=[17, 2], dtype=tf.int64)\na_values = tf.constant([], shape=[0], dtype=tf.float32)\na_shape = tf.constant([6, 12], shape=[2], dtype=tf.int64)\n\nb = tf.constant(-0.223668531, shape=[6, 12], dtype=tf.float32)\n\ntf.raw_ops.SparseTensorDenseAdd(\n    (\n    a_indices=a_indices, a_values=a_values, a_shape=a_shape, b=b)",
        "Bug fix": [
            "@@ -18,6 +18,7 @@ limitations under the License.\n #include \"tensorflow/core/kernels/sparse_tensor_dense_add_op.h\"\n \n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/op_requires.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor_util.h\"\n@@ -47,6 +48,17 @@ Status ValidateInputs(const Tensor *a_indices, const Tensor *a_values,\n         a_values->shape().DebugString(), \" and \",\n         a_shape->shape().DebugString());\n   }\n+  int64_t nnz = a_indices->dim_size(0);\n+  int64_t ndims = a_indices->dim_size(1);\n+  if (a_values->dim_size(0) != nnz) {\n+    return errors::InvalidArgument(\"Dimensions \", nnz, \" and \",\n+                                   a_values->dim_size(0),\n+                                   \" are not compatible\");\n+  }\n+  if (a_shape->dim_size(0) != ndims) {\n+    return errors::InvalidArgument(\"Dimensions \", ndims, \" and \",\n+                                   a_shape->dim_size(0), \" are not compatible\");\n+  }\n   if (a_shape->NumElements() != b->dims()) {\n     return errors::InvalidArgument(\n         \"Two operands have different ranks; received: \", a_shape->NumElements(),\n@@ -61,6 +73,24 @@ Status ValidateInputs(const Tensor *a_indices, const Tensor *a_values,\n           a_shape_flat(i), \" vs dense side \", b->dim_size(i));\n     }\n   }\n+\n+  // Check for invalid indices.\n+  const auto a_indices_mat = a_indices->flat_inner_dims<Index>();\n+\n+  for (int64_t zidx = 0; zidx < nnz; ++zidx) {\n+    for (int64_t didx = 0; didx < ndims; ++didx) {\n+      const Index idx = a_indices_mat(zidx, didx);\n+      if (idx < 0 || idx >= a_shape_flat(didx)) {\n+        return errors::InvalidArgument(\n+            \"Sparse tensor has an invalid index on dimension \", didx,\n+            \": \"\n+            \"a_indices(\",\n+            zidx, \",\", didx, \") = \", idx,\n+            \", dense tensor shape: \", a_shape_flat);\n+      }\n+    }\n+  }\n+\n   return Status::OK();\n }\n \n",
            "@@ -189,7 +189,6 @@ class SparseAddTest(test.TestCase):\n                                                     [(nnz,), (n, m)], s, (n, m))\n       self.assertLess(err, 1e-3)\n \n-  @test_util.run_deprecated_v1\n   def testInvalidSparseTensor(self):\n     with test_util.force_cpu():\n       shape = [2, 2]\n@@ -201,12 +200,49 @@ class SparseAddTest(test.TestCase):\n           [[1, 3]],  # ...so is 3.\n       ]:\n         sparse = sparse_tensor.SparseTensorValue(bad_idx, val, shape)\n-        s = sparse_ops.sparse_add(sparse, dense)\n-\n-        with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n-                                    \"invalid index\"):\n+        with self.assertRaisesRegex(\n+            (ValueError, errors_impl.InvalidArgumentError), \"invalid index\"):\n+          s = sparse_ops.sparse_add(sparse, dense)\n           self.evaluate(s)\n \n+  def _testSparseDenseInvalidInputs(self,\n+                                    a_indices,\n+                                    a_values,\n+                                    a_shape,\n+                                    b,\n+                                    expected_error=\"\"):\n+    # Public API call to sparse-dense add.\n+    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n+                                expected_error):\n+      a = sparse_tensor.SparseTensor(a_indices, a_values, a_shape)\n+      self.evaluate(sparse_ops.sparse_add(a, b))\n+    # Directly call generated kernel, by-passing SparseTensor validation.\n+    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n+                                expected_error):\n+      self.evaluate(\n+          sparse_ops.gen_sparse_ops.sparse_tensor_dense_add(\n+              a_indices, a_values, a_shape, b))\n+\n+  def testSparseDenseInvalidInputs(self):\n+    self._testSparseDenseInvalidInputs(\n+        a_indices=constant_op.constant(0, shape=[17, 2], dtype=dtypes.int64),\n+        a_values=constant_op.constant(0, shape=[5], dtype=dtypes.float32),\n+        a_shape=constant_op.constant([3, 4], dtype=dtypes.int64),\n+        b=constant_op.constant(1, shape=[3, 4], dtype=dtypes.float32),\n+        expected_error=\"Dimensions 17 and 5 are not compatible\")\n+    self._testSparseDenseInvalidInputs(\n+        a_indices=constant_op.constant(0, shape=[17, 4], dtype=dtypes.int64),\n+        a_values=constant_op.constant(0, shape=[17], dtype=dtypes.float32),\n+        a_shape=constant_op.constant([3, 4], dtype=dtypes.int64),\n+        b=constant_op.constant(1, shape=[3, 4], dtype=dtypes.float32),\n+        expected_error=\"Dimensions 4 and 2 are not compatible\")\n+    self._testSparseDenseInvalidInputs(\n+        a_indices=constant_op.constant(7, shape=[17, 2], dtype=dtypes.int64),\n+        a_values=constant_op.constant(0, shape=[17], dtype=dtypes.float32),\n+        a_shape=constant_op.constant([3, 4], dtype=dtypes.int64),\n+        b=constant_op.constant(1, shape=[3, 4], dtype=dtypes.float32),\n+        expected_error=\"invalid index\")\n+\n ######################## Benchmarking code\n \n \n",
            "@@ -665,7 +665,7 @@ class SparseFillEmptyRowsTest(test_util.TensorFlowTestCase):\n class SparseAddTest(test_util.TensorFlowTestCase):\n \n   def testValuesInVariable(self):\n-    indices = constant_op.constant([[1]], dtype=dtypes.int64)\n+    indices = constant_op.constant([[0]], dtype=dtypes.int64)\n     values = variables.Variable([1], trainable=False, dtype=dtypes.float32)\n     shape = constant_op.constant([1], dtype=dtypes.int64)\n \n"
        ],
        "Title": "\n          Missing validation results in undefined behavior in `SparseTensorDenseAdd\n        "
    },
    {
        "Bug description": "There is a potential for segfault / denial of service in TensorFlow by calling  tf.compat.v1.*  ops which don't yet have support for quantized types (added after migration to TF 2.x):",
        "Sample Code": "import tensorflow as tf\n\n\n\ntf.compat.v1.placeholder_with_default(input=np.array([2]),shape=tf.constant(dtype=tf.qint8, value=np.array([1])))",
        "Bug fix": [
            "@@ -431,6 +431,7 @@ class StridedSliceAssignOp : public OpKernel {\n                           StridedSliceAssignOp<CPUDevice, type, true>)\n \n TF_CALL_ALL_TYPES(REGISTER_STRIDED_SLICE);\n+TF_CALL_QUANTIZED_TYPES(REGISTER_STRIDED_SLICE);\n \n #undef REGISTER_STRIDED_SLICE\n \n",
            "@@ -292,7 +292,7 @@ TF_CALL_GPU_ALL_TYPES(DECLARE_FOR_N_GPU);\n #endif  // END GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n \n TF_CALL_ALL_TYPES(DECLARE_FOR_N_CPU);\n-\n+TF_CALL_QUANTIZED_TYPES(DECLARE_FOR_N_CPU);\n \n #undef INSTANTIATE\n #undef DECLARE_FOR_N_CPU\n",
            "@@ -688,9 +688,12 @@ bool SetOpAttrScalar(TFE_Context* ctx, TFE_Op* op, const char* key,\n       for (int i = 0; i < num_dims; ++i) {\n         tensorflow::Safe_PyObjectPtr inner_py_value(\n             PySequence_ITEM(py_value, i));\n+        // If an error is generated when iterating through object, we can\n+        // sometimes get a nullptr.\n         if (inner_py_value.get() == Py_None) {\n           dims[i] = -1;\n-        } else if (!ParseDimensionValue(key, inner_py_value.get(), status,\n+        } else if (inner_py_value.get() == nullptr ||\n+                   !ParseDimensionValue(key, inner_py_value.get(), status,\n                                         &dims[i])) {\n           return false;\n         }\n"
        ],
        "Title": "\n          Segfault due to missing support for quantized types\n        "
    },
    {
        "Bug description": "Multiple TensorFlow operations misbehave in eager mode when the resource handle provided to them is invalid:",
        "Sample Code": " tensorflow as tf\n\ntf.summary.flush(writer=())",
        "Bug fix": [
            "@@ -304,6 +304,9 @@ Status GetDeviceForInput(const EagerOperation& op, const EagerContext& ctx,\n     const Tensor* tensor;\n     // TODO(fishx): Avoid blocking here.\n     TF_RETURN_IF_ERROR(tensor_handle->Tensor(&tensor));\n+    if (tensor->NumElements() == 0) {\n+      return errors::InvalidArgument(\"Empty resource handle\");\n+    }\n     const ResourceHandle& handle = tensor->flat<ResourceHandle>()(0);\n     device_name = handle.device();\n \n"
        ],
        "Title": "\n          Undefined behavior when users supply invalid resource handles\n        "
    }
]
[
    {
        "Bug description": "The implementation of depthwise ops in TensorFlow is vulnerable to a denial of service via  CHECK -failure (assertion failure) caused by overflowing the number of elements in a tensor:",
        "Sample Code": "input = tf.constant(1, shape=[1, 4, 4, 3], dtype=tf.float32)\nfilter_sizes = tf.constant(1879048192, shape=[13], dtype=tf.int32)\nout_backprop = tf.constant(1, shape=[1, 4, 4, 3], dtype=tf.float32)\ntf.raw_ops.DepthwiseConv2dNativeBackpropFilter(\n    (\n    input=input, filter_sizes=filter_sizes, out_backprop=out_backprop, strides=[1, 1, 1, 1], padding=\"SAME\")",
        "Bug fix": [
            "@@ -625,7 +625,7 @@ class DepthwiseConv2dNativeBackpropInputOp : public OpKernel {\n       OP_REQUIRES(context, in_sizes_data[i] >= 0,\n                   errors::InvalidArgument(\"Dimension \", i,\n                                           \" of input_sizes must be >= 0\"));\n-      input_shape.AddDim(in_sizes_data[i]);\n+      OP_REQUIRES_OK(context, input_shape.AddDimWithStatus(in_sizes_data[i]));\n     }\n     const TensorShape& filter_shape = filter.shape();\n     EXTRACT_AND_VERIFY_DIMENSIONS(\"DepthwiseConv2DBackpropInput\");\n@@ -1131,7 +1131,8 @@ class DepthwiseConv2dNativeBackpropFilterOp : public OpKernel {\n       OP_REQUIRES(context, filter_sizes_data[i] >= 0,\n                   errors::InvalidArgument(\"Dimension \", i,\n                                           \" of filter_sizes must be >= 0\"));\n-      filter_shape.AddDim(filter_sizes_data[i]);\n+      OP_REQUIRES_OK(context,\n+                     filter_shape.AddDimWithStatus(filter_sizes_data[i]));\n     }\n     const TensorShape& input_shape = input.shape();\n \n"
        ],
        "Title": "\n          `CHECK` failure in depthwise ops via overflows\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.Conv3DBackpropFilterV2  does not fully validate the input arguments. This results in a  CHECK -failure which can be used to trigger a denial of service attack:",
        "Sample Code": "tf.raw_ops.Conv3DBackpropFilterV2(\n  input=tf.constant(.5053710941, shape=[2,2,2,2,1], dtype=tf.float16),\n  filter_sizes=tf.constant(0, shape=[], dtype=tf.int32),\n  out_backprop=tf.constant(.5053710941, shape=[2,2,2,2,1], dtype=tf.float16),\n  strides=[1, 1, 1, 1, 1],\n  padding=\"VALID\",\n  data_format=\"NDHWC\",\n  ,\n  dilations=[1, 1, 1, 1, 1])",
        "Bug fix": [
            "@@ -741,6 +741,10 @@ class Conv3DBackpropFilterOp : public OpKernel {\n     TensorShape filter_shape;\n     if (takes_shape_) {\n       const Tensor& filter_sizes = context->input(1);\n+      OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),\n+                  errors::InvalidArgument(\n+                      \"filter_sizes shape must be rank 1 but is rank \",\n+                      filter_sizes.shape().dims()));\n       OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\n                                   filter_sizes.vec<int32>(), &filter_shape));\n     } else {\n@@ -875,6 +879,10 @@ class Conv3DCustomBackpropFilterOp : public OpKernel {\n     TensorShape filter_shape;\n     if (takes_shape_) {\n       const Tensor& filter_sizes = context->input(1);\n+      OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),\n+                  errors::InvalidArgument(\n+                      \"filter_sizes shape must be rank 1 but is rank \",\n+                      filter_sizes.shape().dims()));\n       OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\n                                   filter_sizes.vec<int32>(), &filter_shape));\n     } else {\n@@ -1638,6 +1646,10 @@ class Conv3DBackpropFilterOp<GPUDevice, T> : public OpKernel {\n     TensorShape filter_shape;\n     if (takes_shape_) {\n       const Tensor& filter_sizes = context->input(1);\n+      OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),\n+                  errors::InvalidArgument(\n+                      \"filter_sizes shape must be rank 1 but is rank \",\n+                      filter_sizes.shape().dims()));\n       OP_REQUIRES_OK(context, tensor::MakeShape(filter_sizes, &filter_shape));\n     } else {\n       filter_shape = context->input(1).shape();\n",
            "@@ -18,6 +18,7 @@ import numpy as np\n \n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import gradient_checker\n@@ -58,6 +59,23 @@ class Conv3DBackpropFilterV2GradTest(test.TestCase):\n           err_tolerance = 1e-3\n           self.assertLess(err, err_tolerance)\n \n+  def testBadFilterShape(self):\n+    strides = [1, 1, 1, 1, 1]\n+    padding = \"VALID\"\n+    tin = constant_op.constant(\n+        .5053710941, shape=[2, 2, 2, 2, 1], dtype=dtypes.float32)\n+    filter_sizes = constant_op.constant(0, shape=[], dtype=dtypes.int32)\n+    out_backprop = constant_op.constant(\n+        .5053710941, shape=[2, 2, 2, 2, 1], dtype=dtypes.float32)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 1\"):\n+      nn_ops.conv3d_backprop_filter_v2(\n+          input=tin,\n+          filter_sizes=filter_sizes,\n+          out_backprop=out_backprop,\n+          strides=strides,\n+          padding=padding)\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          Missing validation causes denial of service via `Conv3DBackpropFilterV2`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.LSTMBlockCell  does not fully validate the input arguments. This results in a  CHECK -failure which can be used to trigger a denial of service attack:",
        "Sample Code": "tf.raw_ops.LSTMBlockCell( \n  x=tf.constant(0.837607, shape=[28,29], dtype=tf.float32),\n  cs_prev=tf.constant(0, shape=[28,17], dtype=tf.float32),\n  h_prev=tf.constant(0.592631638, shape=[28,17], dtype=tf.float32),\n  w=tf.constant(0.887386262, shape=[46,68], dtype=tf.float32),\n  wci=tf.constant(0, shape=[], dtype=tf.float32),\n  wcf=tf.constant(0, shape=[17], dtype=tf.float32),\n  wco=tf.constant(0.592631638, shape=[28,17], dtype=tf.float32),\n  b=tf.constant(0.75259006, shape=[68], dtype=tf.float32),\n  ),\n  forget_bias=1, cell_clip=0, use_peephole=False)",
        "Bug fix": [
            "@@ -416,6 +416,65 @@ class LSTMBlockCellOp : public OpKernel {\n \n     const Device& device = ctx->eigen_device<Device>();\n \n+    // Sanity check that each of the tensors have the required NDIMS.\n+    OP_REQUIRES(ctx, x_tensor->dims() == 2,\n+                errors::InvalidArgument(\"x_tensor must be rank 2 but is rank \",\n+                                        x_tensor->dims(), \".\"));\n+    OP_REQUIRES(\n+        ctx, cs_prev_tensor->dims() == 2,\n+        errors::InvalidArgument(\"cs_prev_tensor must be rank 2 but is rank \",\n+                                cs_prev_tensor->dims(), \".\"));\n+    OP_REQUIRES(\n+        ctx, h_prev_tensor->dims() == 2,\n+        errors::InvalidArgument(\"h_prev_tensor must be rank 2 but is rank \",\n+                                h_prev_tensor->dims(), \".\"));\n+    OP_REQUIRES(ctx, w_tensor->dims() == 2,\n+                errors::InvalidArgument(\"w_tensor must be rank 2 but is rank \",\n+                                        w_tensor->dims(), \".\"));\n+    OP_REQUIRES(\n+        ctx, wci_tensor->dims() == 1,\n+        errors::InvalidArgument(\"wci_tensor must be rank 1 but is rank \",\n+                                wci_tensor->dims(), \".\"));\n+    OP_REQUIRES(\n+        ctx, wcf_tensor->dims() == 1,\n+        errors::InvalidArgument(\"wcf_tensor must be rank 1 but is rank \",\n+                                wci_tensor->dims(), \".\"));\n+    OP_REQUIRES(\n+        ctx, wco_tensor->dims() == 1,\n+        errors::InvalidArgument(\"wco_tensor must be rank 1 but is rank \",\n+                                wco_tensor->dims(), \".\"));\n+    OP_REQUIRES(ctx, b_tensor->dims() == 1,\n+                errors::InvalidArgument(\"b_tensor must be rank 1 but is rank \",\n+                                        b_tensor->dims(), \".\"));\n+    OP_REQUIRES(ctx, xh_tensor.dims() == 2,\n+                errors::InvalidArgument(\"xh_tensor must be rank 2 but is rank \",\n+                                        xh_tensor.dims(), \".\"));\n+    OP_REQUIRES(ctx, i_tensor->dims() == 2,\n+                errors::InvalidArgument(\"i_tensor must be rank 2 but is rank \",\n+                                        i_tensor->dims(), \".\"));\n+    OP_REQUIRES(ctx, cs_tensor->dims() == 2,\n+                errors::InvalidArgument(\"cs_tensor must be rank 2 but is rank \",\n+                                        cs_tensor->dims(), \".\"));\n+    OP_REQUIRES(ctx, f_tensor->dims() == 2,\n+                errors::InvalidArgument(\"f_tensor must be rank 2 but is rank \",\n+                                        f_tensor->dims(), \".\"));\n+    OP_REQUIRES(ctx, o_tensor->dims() == 2,\n+                errors::InvalidArgument(\"o_tensor must be rank 2 but is rank \",\n+                                        o_tensor->dims(), \".\"));\n+    OP_REQUIRES(ctx, ci_tensor->dims() == 2,\n+                errors::InvalidArgument(\"ci_tensor must be rank 2 but is rank \",\n+                                        ci_tensor->dims(), \".\"));\n+    OP_REQUIRES(ctx, co_tensor->dims() == 2,\n+                errors::InvalidArgument(\"co_tensor must be rank 2 but is rank \",\n+                                        co_tensor->dims(), \".\"));\n+    OP_REQUIRES(\n+        ctx, gates_tensor.dims() == 2,\n+        errors::InvalidArgument(\"gates_tensor must be rank 2 but is rank \",\n+                                gates_tensor.dims(), \".\"));\n+    OP_REQUIRES(ctx, h_tensor->dims() == 2,\n+                errors::InvalidArgument(\"h_tensor must be rank 2 but is rank \",\n+                                        h_tensor->dims(), \".\"));\n+\n     functor::LSTMBlockCellFprop<Device, T, USE_CUBLAS, gate_layout>(\n         batch_size, input_size, cell_size)(\n         ctx, device, forget_bias_, cell_clip_, use_peephole_,\n",
            "@@ -33,6 +33,7 @@ from tensorflow.python.framework import tensor_spec\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import   array_ops\n from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import gen_rnn_ops\n from tensorflow.python.ops import gradients_impl\n from tensorflow.python.ops import init_ops\n from tensorflow.python.ops import math_ops\n@@ -1323,6 +1324,36 @@ class LSTMTest(test.TestCase):\n   def testDynamicEquivalentToStaticRNNWithSequenceLength(self):\n     self._testDynamicEquivalentToStaticRNN(use_sequence_length=True)\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def testLSTMBlockCellErrorHandling(self):\n+    forget_bias = 1\n+    cell_clip = 0\n+    use_peephole = False\n+    x = constant_op.constant(0.837607, shape=[28, 29], dtype=dtypes.float32)\n+    cs_prev = constant_op.constant(0, shape=[28, 17], dtype=dtypes.float32)\n+    h_prev = constant_op.constant(\n+        0.592631638, shape=[28, 17], dtype=dtypes.float32)\n+    w = constant_op.constant(0.887386262, shape=[46, 68], dtype=dtypes.float32)\n+    wci = constant_op.constant(0, shape=[], dtype=dtypes.float32)\n+    wcf = constant_op.constant(0, shape=[17], dtype=dtypes.float32)\n+    wco = constant_op.constant(\n+        0.592631638, shape=[28, 17], dtype=dtypes.float32)\n+    b = constant_op.constant(0.75259006, shape=[68], dtype=dtypes.float32)\n+    with self.assertRaises(errors_impl.InvalidArgumentError):\n+      self.evaluate(\n+          gen_rnn_ops.lstm_block_cell(\n+              x=x,\n+              cs_prev=cs_prev,\n+              h_prev=h_prev,\n+              w=w,\n+              wci=wci,\n+              wcf=wcf,\n+              wco=wco,\n+              b=b,\n+              forget_bias=forget_bias,\n+              cell_clip=cell_clip,\n+              use_peephole=use_peephole))\n+\n \n class BidirectionalRNNTest(test.TestCase):\n \n"
        ],
        "Title": "\n          Missing validation causes denial of service via `LSTMBlockCell`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.SparseTensorToCSRSparseMatrix  does not fully validate the input arguments. This results in a  CHECK -failure which can be used to trigger a denial of service attack:",
        "Sample Code": "indices = tf.constant(53, shape=[3], dtype=tf.int64)\nvalues = tf.constant(0.554979503, shape=[218650], dtype=tf.float32)\ndense_shape = tf.constant(53, shape=[3], dtype=tf.int64)\n    \ntf.raw_ops.SparseTensorToCSRSparseMatrix(\n  indices=indices,\n  values=values,\n  ,\n  dense_shape=dense_shape)",
        "Bug fix": [
            "@@ -67,6 +67,13 @@ class SparseTensorToCSRSparseMatrixCPUOp : public OpKernel {\n     const Tensor& values = ctx->input(1);\n     const Tensor& dense_shape = ctx->input(2);\n     const int rank = dense_shape.NumElements();\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsVector(dense_shape.shape()),\n+        errors::InvalidArgument(\"dense_shape must be rank 1 but got rank\",\n+                                dense_shape.shape().dims()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(indices.shape()),\n+                errors::InvalidArgument(\"indices must be rank 2 but got rank\",\n+                                        indices.shape().dims()));\n     OP_REQUIRES(ctx, rank == 2 || rank == 3,\n                 errors::InvalidArgument(\"SparseTensor must have rank 2 or 3; \",\n                                         \"but indices has rank: \", rank));\n",
            "@@ -168,6 +168,25 @@ class CSRSparseMatrixOpsTest(test.TestCase):\n     self.assertAllClose(a_values, a_st_rt_value.values)\n     self.assertAllEqual(a_dense_shape, a_st_rt_value.dense_shape)\n \n+  def testSparseTensorConversionInvalidInputShapes(self):\n+    values = constant_op.constant(\n+        0.554979503, shape=[5], dtype=dtypes.float32)\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 1\"):\n+      indices = constant_op.constant(0, shape=[5, 2], dtype=dtypes.int64)\n+      dense_shape = constant_op.constant(53, shape=[], dtype=dtypes.int64)\n+      csr = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n+          indices=indices, values=values, dense_shape=dense_shape)\n+      self.evaluate(csr)\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"must be rank 2\"):\n+      indices = constant_op.constant(0, shape=[5], dtype=dtypes.int64)\n+      dense_shape = constant_op.constant(53, shape=[1], dtype=dtypes.int64)\n+      csr = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n+          indices=indices, values=values, dense_shape=dense_shape)\n+      self.evaluate(csr)\n+\n   # TODO(b/139491352): Add handle_data propagation to array_ops.identity.\n   @test_util.run_deprecated_v1\n   def testCSRSparseMatrixResourceVariable(self):\n"
        ],
        "Title": "\n          Missing validation causes denial of service via `SparseTensorToCSRSparseMatrix`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.LoadAndRemapMatrix  does not fully validate the input arguments. This results in a  CHECK -failure which can be used to trigger a denial of service attack:",
        "Sample Code": "ckpt_path = tf.constant(\n    \"/tmp/warm_starting_util_test5kl2a3pc/tmpph76tep2/model-0\", shape=[], dtype=tf.string)\nold_tensor_name = tf.constant(\n    \"/tmp/warm_starting_util_test5kl2a3pc/tmpph76tep2/model-0\", shape=[], dtype=tf.string)\n\nrow_remapping = tf.constant(0, shape=[], dtype=tf.int64)\ncol_remapping = tf.constant(3, shape=[3], dtype=tf.int64)\ninitializing_values = tf.constant([], shape=[0, 1], dtype=tf.float32)\n\ntf.raw_ops.LoadAndRemapMatrix(\n  ckpt_path=ckpt_path,\n  old_tensor_name=old_tensor_name,\n  row_remapping=row_remapping,\n  col_remapping=col_remapping,\n  initializing_values=initializing_values,\n  num_rows=1,\n  ,\n  num_cols=1)",
        "Bug fix": [
            "@@ -74,6 +74,11 @@ class LoadAndRemapMatrixOp : public OpKernel {\n     std::vector<bool> row_id_present;\n     const Tensor* row_remapping_t;\n     OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n+    OP_REQUIRES(\n+        context, row_remapping_t->dims() == 1,\n+        errors::InvalidArgument(\"The `row_remapping` tensor must be 1-D, got \"\n+                                \"a tensor of shape \",\n+                                row_remapping_t->shape().DebugString()));\n     const auto row_remapping = row_remapping_t->vec<int64_t>();\n     OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                 errors::InvalidArgument(strings::StrCat(\n",
            "@@ -227,6 +227,32 @@ class LoadAndRemapMatrixTest(test.TestCase):\n           np.reshape(initializing_values, (num_rows, num_cols)),\n           self.evaluate(remapped_matrix))\n \n+  def test_load_and_remap_invalid_dims(self):\n+    ckpt_path = constant_op.constant(\n+        '/tmp/warm_starting_util_test5kl2a3pc/tmpph76tep2/model-0',\n+        shape=[],\n+        dtype=dtypes.string)\n+    old_tensor_name = constant_op.constant(\n+        '/tmp/warm_starting_util_test5kl2a3pc/tmpph76tep2/model-0',\n+        shape=[],\n+        dtype=dtypes.string)\n+    row_remapping = constant_op.constant(0, shape=[], dtype=dtypes.int64)\n+    col_remapping = constant_op.constant(3, shape=[3], dtype=dtypes.int64)\n+    initializing_values = constant_op.constant([],\n+                                               shape=[0, 1],\n+                                               dtype=dtypes.float32)\n+    with self.cached_session(), self.assertRaisesRegex(\n+        (ValueError, errors.InvalidArgumentError), 'tensor must be 1-D'):\n+      self.evaluate(\n+          gen_checkpoint_ops.load_and_remap_matrix(\n+              ckpt_path=ckpt_path,\n+              old_tensor_name=old_tensor_name,\n+              row_remapping=row_remapping,\n+              col_remapping=col_remapping,\n+              initializing_values=initializing_values,\n+              num_rows=1,\n+              num_cols=1))\n+\n   @test_util.run_deprecated_v1\n   def test_load_and_remap_invalid_remapping(self):\n     \"\"\"Tests that errors are raised when an ID maps to multiple new IDs.\n"
        ],
        "Title": "\n          Missing validation causes denial of service via `LoadAndRemapMatrix`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.UnsortedSegmentJoin  does not fully validate the input arguments. This results in a  CHECK -failure which can be used to trigger a denial of service attack:",
        "Sample Code": "tf.raw_ops.UnsortedSegmentJoin(\n  inputs=tf.constant(\"this\", shape=[12], dtype=tf.string),\n  segment_ids=tf.constant(0, shape=[12], dtype=tf.int64),\n  ),\n  num_segments=tf.constant(0, shape=[12], dtype=tf.int64))",
        "Bug fix": [
            "@@ -92,6 +92,9 @@ class UnsortedSegmentJoinOp : public OpKernel {\n     const Tensor& num_segments_tensor = context->input(2);\n     OP_REQUIRES(context, num_segments_tensor.NumElements() != 0,\n                 errors::InvalidArgument(\"Number of segments cannot be empty.\"));\n+    OP_REQUIRES(context,\n+                TensorShapeUtils::IsScalar(num_segments_tensor.shape()),\n+                errors::InvalidArgument(\"Number of segments must be a scalar\"));\n     auto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();\n \n     OP_REQUIRES(\n"
        ],
        "Title": "\n          Missing validation causes denial of service via `UnsortedSegmentJoin`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.StagePeek  does not fully validate the input arguments. This results in a  CHECK -failure which can be used to trigger a denial of service attack:",
        "Sample Code": "index = tf.constant([], shape=[0], dtype=tf.int32)\n)\ntf.raw_ops.StagePeek(index=index, dtypes=[tf.int32])",
        "Bug fix": [
            "@@ -258,6 +258,8 @@ class StagePeekOp : public OpKernel {\n     core::ScopedUnref scope(buf);\n     Buffer::Tuple tuple;\n \n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(ctx->input(0).shape()),\n+                errors::InvalidArgument(\"index must be scalar\"));\n     std::size_t index = ctx->input(0).scalar<int>()();\n \n     OP_REQUIRES_OK(ctx, buf->Peek(index, &tuple));\n",
            "@@ -13,6 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n@@ -134,6 +135,16 @@ class StageTest(test.TestCase):\n       for i in range(10):\n         self.assertTrue(sess.run(peek, feed_dict={p: i}) == [i])\n \n+  def testPeekBadIndex(self):\n+    stager = data_flow_ops.StagingArea([\n+        dtypes.int32,\n+    ], shapes=[[10]])\n+    stager.put([array_ops.zeros([10], dtype=dtypes.int32)])\n+\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                'must be scalar'):\n+      self.evaluate(stager.peek([]))\n+\n   @test_util.run_deprecated_v1\n   def testSizeAndClear(self):\n     with ops.Graph().as_default() as G:\n",
            "@@ -1737,7 +1737,7 @@ class BaseStagingArea:\n \n     # Sanity check number of values\n     if not len(vals) <= len(self._dtypes):\n-      raise ValueError(f\"Unexpected number of inputs {len(vals)} vs\"\n+      raise ValueError(f\"Unexpected number of inputs {len(vals)} vs \"\n                        f\"{len(self._dtypes)}\")\n \n     tensors = []\n"
        ],
        "Title": "\n          Missing validation causes denial of service via `StagePeek`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.GetSessionTensor  does not fully validate the input arguments. This results in a  CHECK -failure which can be used to trigger a denial of service attack:",
        "Sample Code": "handle = tf.constant(\"[]\", shape=[0], dtype=tf.string)\n)\ntf.raw_ops.GetSessionTensor(handle=handle)",
        "Bug fix": [
            "@@ -98,6 +98,8 @@ class GetSessionTensorOp : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& handle = ctx->input(0);\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(handle.shape()),\n+                errors::InvalidArgument(\"handle must be scalar\"));\n     const string& name = handle.scalar<tstring>()();\n     Tensor val;\n     auto session_state = ctx->session_state();\n"
        ],
        "Title": "\n          Missing validation causes denial of service via `GetSessionTensor`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.DeleteSessionTensor  does not fully validate the input arguments. This results in a  CHECK -failure which can be used to trigger a denial of service attack:",
        "Sample Code": "handle = tf.constant(\"[]\", shape=[0], dtype=tf.string)\n)\ntf.raw_ops.DeleteSessionTensor(handle=handle)",
        "Bug fix": [
            "@@ -134,6 +134,8 @@ class DeleteSessionTensorOp : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& handle = ctx->input(0);\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(handle.shape()),\n+                errors::InvalidArgument(\"`handle` must be scalar\"));\n     const string& name = handle.scalar<tstring>()();\n     auto session_state = ctx->session_state();\n     OP_REQUIRES(ctx, session_state != nullptr,\n"
        ],
        "Title": "\n          Missing validation causes denial of service via `DeleteSessionTensor`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.QuantizeAndDequantizeV4Grad  does not fully validate the input arguments. This results in a  CHECK -failure which can be used to trigger a denial of service attack:",
        "Sample Code": "tf.raw_ops.QuantizeAndDequantizeV4Grad(\n  gradients=tf.constant(1, shape=[2,2], dtype=tf.float64),\n  input=tf.constant(1, shape=[2,2], dtype=tf.float64),\n  input_min=tf.constant([], shape=[0], dtype=tf.float64),\n  input_max=tf.constant(-10, shape=[], dtype=tf.float64),\n  ),\n  axis=-1)",
        "Bug fix": [
            "@@ -174,13 +174,13 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\n     OP_REQUIRES(ctx,\n                 input_min_tensor.dims() == 0 || input_min_tensor.dims() == 1,\n                 errors::InvalidArgument(\n-                    \"Input min tensor must have dimension 1. Recieved \",\n+                    \"Input min tensor must have dimension 0 or 1. Received \",\n                     input_min_tensor.dims(), \".\"));\n     const Tensor& input_max_tensor = ctx->input(3);\n     OP_REQUIRES(ctx,\n                 input_max_tensor.dims() == 0 || input_max_tensor.dims() == 1,\n                 errors::InvalidArgument(\n-                    \"Input max tensor must have dimension 1. Recieved \",\n+                    \"Input max tensor must have dimension 0 or 1. Received \",\n                     input_max_tensor.dims(), \".\"));\n     if (axis_ != -1) {\n       OP_REQUIRES(\n@@ -203,6 +203,12 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\n                    ctx->allocate_output(2, min_max_shape, &input_max_backprop));\n \n     if (axis_ == -1) {\n+      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_min_tensor.shape()),\n+                  errors::InvalidArgument(\n+                      \"input_min must be a scalar if axis is unspecified\"));\n+      OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(input_max_tensor.shape()),\n+                  errors::InvalidArgument(\n+                      \"input_max must be a scalar if axis is unspecified\"));\n       functor::QuantizeAndDequantizeOneScaleGradientFunctor<Device, T> f;\n       f(ctx->eigen_device<Device>(), gradient.template flat<T>(),\n         input.template flat<T>(), input_min_tensor.scalar<T>(),\n"
        ],
        "Title": "\n          Missing validation crashes `QuantizeAndDequantizeV4Grad`\n        "
    }
]
[
    {
        "Bug description": "The implementation of  tf.raw_ops.TensorSummaryV2  does not fully validate the input arguments. This results in a  CHECK -failure which can be used to trigger a denial of service attack:",
        "Sample Code": "import tensorflow as tf\n\ntf.raw_ops.TensorSummaryV2(\n  tag=np.array('test'),\n  tensor=np.array(3),\n  ),\n  serialized_summary_metadata=tf.io.encode_base64(np.empty((0))))",
        "Bug fix": [
            "@@ -36,6 +36,10 @@ class SummaryTensorOpV2 : public OpKernel {\n                 errors::InvalidArgument(\"tag must be scalar\"));\n     const Tensor& tensor = c->input(1);\n     const Tensor& serialized_summary_metadata_tensor = c->input(2);\n+    OP_REQUIRES(\n+        c,\n+        TensorShapeUtils::IsScalar(serialized_summary_metadata_tensor.shape()),\n+        errors::InvalidArgument(\"serialized_summary_metadata must be scalar\"));\n \n     Summary s;\n     Summary::Value* v = s.add_value();\n"
        ],
        "Title": "\n          Missing validation causes `TensorSummaryV2` to crash\n        "
    },
    {
        "Bug description": "TensorFlow's  saved_model_cli  tool is vulnerable to a code injection:",
        "Sample Code": "# ...\n\n  for input_raw in filter(bool, input_exprs_str.split(';')):\n    # ...\n    if safe:\n      # ...\n    else:\n      # ast.literal_eval does not work with numpy expressions\n      input_dict[input_key] = eval(expr)  # pylint: disable=eval-used\n  \n  return input_dict",
        "Bug fix": [
            "@@ -684,7 +684,7 @@ def load_inputs_from_input_arg_string(inputs_str, input_exprs_str,\n   tensor_key_feed_dict = {}\n \n   inputs = preprocess_inputs_arg_string(inputs_str)\n-  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str, safe=False)\n+  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str)\n   input_examples = preprocess_input_examples_arg_string(input_examples_str)\n \n   for input_tensor_key, (filename, variable_name) in inputs.items():\n",
            "@@ -486,43 +486,6 @@ Concrete Functions:\n     self.assertTrue(np.all(feed_dict['y'] == pkl1))\n     self.assertTrue(np.all(feed_dict['z'] == pkl2))\n \n-  def testInputParserPythonExpression(self):\n-    x1 = np.ones([2, 10])\n-    x2 = np.array([[1], [2], [3]])\n-    x3 = np.mgrid[0:5, 0:5]\n-    x4 = [[3], [4]]\n-    input_expr_str = ('x1=np.ones([2,10]);x2=np.array([[1],[2],[3]]);'\n-                      'x3=np.mgrid[0:5,0:5];x4=[[3],[4]]')\n-    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n-        '', input_expr_str, '')\n-    self.assertTrue(np.all(feed_dict['x1'] == x1))\n-    self.assertTrue(np.all(feed_dict['x2'] == x2))\n-    self.assertTrue(np.all(feed_dict['x3'] == x3))\n-    self.assertTrue(np.all(feed_dict['x4'] == x4))\n-\n-  def testInputParserBoth(self):\n-    x0 = np.array([[1], [2]])\n-    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n-    np.savez(input_path, a=x0)\n-    x1 = np.ones([2, 10])\n-    input_str = 'x0=' + input_path + '[a]'\n-    input_expr_str = 'x1=np.ones([2,10])'\n-    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n-        input_str, input_expr_str, '')\n-    self.assertTrue(np.all(feed_dict['x0'] == x0))\n-    self.assertTrue(np.all(feed_dict['x1'] == x1))\n-\n-  def testInputParserBothDuplicate(self):\n-    x0 = np.array([[1], [2]])\n-    input_path = os.path.join(test.get_temp_dir(), 'input.npz')\n-    np.savez(input_path, a=x0)\n-    x1 = np.ones([2, 10])\n-    input_str = 'x0=' + input_path + '[a]'\n-    input_expr_str = 'x0=np.ones([2,10])'\n-    feed_dict = saved_model_cli.load_inputs_from_input_arg_string(\n-        input_str, input_expr_str, '')\n-    self.assertTrue(np.all(feed_dict['x0'] == x1))\n-\n   def testInputParserErrorNoName(self):\n     x0 = np.array([[1], [2]])\n     x1 = np.array(range(5))\n@@ -629,7 +592,7 @@ Concrete Functions:\n     base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n     args = self.parser.parse_args([\n         'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n-        'regress_x2_to_y3', '--input_exprs', 'x2=np.ones((3,1))'\n+        'regress_x2_to_y3', '--input_exprs', 'x2=[1,2,3]'\n     ] + (['--use_tfrt'] if use_tfrt else []))\n     with self.assertRaises(ValueError):\n       saved_model_cli.run(args)\n@@ -640,7 +603,7 @@ Concrete Functions:\n     base_path = test.test_src_dir_path(SAVED_MODEL_PATH)\n     args = self.parser.parse_args([\n         'run', '--dir', base_path, '--tag_set', 'serve', '--signature_def',\n-        'INVALID_SIGNATURE', '--input_exprs', 'x2=np.ones((3,1))'\n+        'INVALID_SIGNATURE', '--input_exprs', 'x2=[1,2,3]'\n     ] + (['--use_tfrt'] if use_tfrt else []))\n     with self.assertRaisesRegex(ValueError,\n                                 'Could not find signature \"INVALID_SIGNATURE\"'):\n"
        ],
        "Title": "\n          Code injection in `saved_model_cli`\n        "
    },
    {
        "Bug description": "When  building an XLA compilation cache , if default settings are used, TensorFlow triggers a null pointer dereference:",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Null pointer dereference in `BuildXlaCompilationCache` (XLA) \n        "
    },
    {
        "Bug description": "The  simplifyBroadcast  is vulenrable to a segfault (hence, denial of service), if called with scalar shapes.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -157,6 +157,10 @@ llvm::Optional<Value> simplifyBroadcast(ShapeComponentAnalysis& analysis,\n     shapes_found.push_back(*found_shape);\n     maxRank = std::max(maxRank, found_shape->size());\n   }\n+  if (maxRank == 0) {\n+    return Value(builder->create<tensor::FromElementsOp>(\n+        loc, shapes[0].getType(), SmallVector<Value>()));\n+  }\n \n   SmallVector<const ShapeComponentAnalysis::SymbolicExpr*> joined_dimensions(\n       maxRank);\n",
            "@@ -0,0 +1,11 @@\n+builtin.func @test(%V__0 : tensor<i1> { python_test_attrs.static_type = tensor<i1> }, %V__1 : tensor<f32> { python_test_attrs.static_type = tensor<f32> }, %V__2 : tensor<f32> { python_test_attrs.static_type = tensor<f32> }) -> tensor<f32> {\n+  %0 = \"tf.Cast\"(%V__0) : (tensor<i1>) -> tensor<i1>\n+  %1 = \"tf.Selu\"(%V__2) : (tensor<f32>) -> tensor<f32>\n+  %2 = \"tf.NextAfter\"(%1, %V__2) : (tensor<f32>, tensor<f32>) -> tensor<f32>\n+  %3 = \"tf.Elu\"(%2) : (tensor<f32>) -> tensor<f32>\n+  %4 = \"tf.Cosh\"(%3) : (tensor<f32>) -> tensor<f32>\n+  %5 = \"tf.Elu\"(%4) : (tensor<f32>) -> tensor<f32>\n+  %6 = \"tf.Div\"(%V__1, %5) : (tensor<f32>, tensor<f32>) -> tensor<f32>\n+  %7 = \"tf.Select\"(%0, %6, %V__1) : (tensor<i1>, tensor<f32>, tensor<f32>) -> tensor<f32>\n+  return %7 : tensor<f32>\n+}\n\\ No newline at end of file\n"
        ],
        "Title": "\n          Segfault in `simplifyBroadcast` (MLIR) \n        "
    },
    {
        "Bug description": "The  TFG dialect of TensorFlow (MLIR)  makes several assumptions about the incoming  GraphDef  before converting it to the MLIR-based dialect.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Multiple crashes, heap OOB accesses in TFG dialect (MLIR)\n        "
    },
    {
        "Bug description": "A  GraphDef  from a TensorFlow  SavedModel  can be maliciously altered to cause a TensorFlow process to crash due to encountering  a  :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -561,6 +561,11 @@ Node* Graph::AddNode(NodeDef node_def, Status* status) {\n     VLOG(3) << \"AddNode: found type constructor for \" << node_def.name();\n     const auto ctor_type =\n         full_type::SpecializeType(AttrSlice(node_def), op_reg_data->op_def);\n+    if (!ctor_type.ok()) {\n+      *status = errors::InvalidArgument(\"type error: \",\n+                                        ctor_type.status().ToString());\n+      return nullptr;\n+    }\n     const FullTypeDef ctor_typedef = ctor_type.ValueOrDie();\n     if (ctor_typedef.type_id() != TFT_UNSET) {\n       *(node_def.mutable_experimental_type()) = ctor_typedef;\n"
        ],
        "Title": "\n          Crash due to erroneous `StatusOr`\n        "
    },
    {
        "Bug description": "TensorFlow's  type inference  can cause a heap OOB read as the bounds checking is done in a  DCHECK  (which is a no-op during production):",
        "Sample Code": "",
        "Bug fix": [
            "@@ -222,10 +222,16 @@ void Node::RunForwardTypeInference() {\n       const auto& node_t = node->def().experimental_type();\n       if (node_t.type_id() != TFT_UNSET) {\n         int ix = input_idx[i];\n-        DCHECK(ix < node_t.args_size())\n-            << \"input \" << i << \" should have an output \" << ix\n-            << \" but instead only has \" << node_t.args_size()\n-            << \" outputs: \" << node_t.DebugString();\n+        if (ix >= node_t.args_size()) {\n+          LOG(WARNING) << name() << \" has bad type information: input \" << i\n+                       << \" should have an output \" << ix\n+                       << \" but instead only has \" << node_t.args_size()\n+                       << \" outputs: \" << node_t.DebugString()\n+                       << \"\\nThis indicates either \"\n+                          \"a bug in op registration or a corrupted graph.\";\n+          ClearTypeInfo();\n+          return;\n+        }\n         input_types.emplace_back(node_t.args(ix));\n       } else {\n         input_types.emplace_back(*no_type);\n"
        ],
        "Title": "\n          Heap OOB access in `RunForwardTypeInference`\n        "
    },
    {
        "Bug description": "The  GraphDef  format in TensorFlow does not allow self recursive functions. The runtime assumes that this invariant is satisfied. However, a  GraphDef  containing a fragment such as the following can be consumed when loading a  SavedModel :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -25,6 +25,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/attr_value.pb.h\"\n #include \"tensorflow/core/framework/function.pb.h\"\n #include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/framework/op_def.pb.h\"\n #include \"tensorflow/core/framework/tensor.pb.h\"\n #include \"tensorflow/core/lib/io/path.h\"\n #include \"tensorflow/core/lib/monitoring/counter.h\"\n@@ -99,6 +100,19 @@ static Status ValidateNode(const NodeDef& node) {\n   return Status::OK();\n }\n \n+static Status ValidateFunctionNotRecursive(const FunctionDef& function) {\n+  const auto& function_name = function.signature().name();\n+  for (const auto& node : function.node_def()) {\n+    if (node.op() == function_name) {\n+      return errors::FailedPrecondition(\n+          \"Function \", function_name,\n+          \" is self recursive and TensorFlow does not support this scenario.\");\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n static Status ValidateSavedTensors(const GraphDef& graph_def) {\n   for (const auto& node : graph_def.node()) {\n     TF_RETURN_IF_ERROR(ValidateNode(node));\n@@ -110,6 +124,10 @@ static Status ValidateSavedTensors(const GraphDef& graph_def) {\n       for (const auto& node : function.node_def()) {\n         TF_RETURN_IF_ERROR(ValidateNode(node));\n       }\n+\n+      // Also check that there is no recursivity in the library\n+      // TODO(mihaimaruseac): Do more than self-recursivity\n+      TF_RETURN_IF_ERROR(ValidateFunctionNotRecursive(function));\n     }\n   }\n \n"
        ],
        "Title": "\n          Stack overflow due to self-recursive function in `GraphDef`\n        "
    },
    {
        "Bug description": "The Grappler component of TensorFlow is vulnerable to a denial of service via  CHECK -failure (assertion failure) in  constant folding :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -1017,7 +1017,12 @@ bool ConstantFolding::IsFoldableUncached(\n       }\n     }\n     for (const auto& output_prop : output_props) {\n-      const PartialTensorShape output_shape(output_prop.shape());\n+      PartialTensorShape output_shape;\n+      if (!PartialTensorShape::BuildPartialTensorShape(output_prop.shape(),\n+                                                       &output_shape)\n+               .ok()) {\n+        return false;\n+      }\n       if (output_shape.IsFullyDefined()) {\n         const int64_t num_bytes =\n             output_shape.num_elements() * DataTypeSize(output_prop.dtype());\n"
        ],
        "Title": "\n          `CHECK` failure in constant folding\n        "
    },
    {
        "Bug description": "Under certain scenarios, Grappler component of TensorFlow can trigger a null pointer dereference. There are 2 places where this can occur, for the same malicious alteration of a  SavedModel  file (fixing the first one would trigger the same dereference in the second place):",
        "Sample Code": "",
        "Bug fix": [
            "@@ -3505,6 +3505,9 @@ bool ConstantFolding::MulConvPushDown(GraphDef* optimized_graph, NodeDef* node,\n \n   NodeDef* mul_left_child = node_map_->GetNode(node->input(0));\n   NodeDef* mul_right_child = node_map_->GetNode(node->input(1));\n+  if (mul_left_child == nullptr || mul_right_child == nullptr) {\n+    return false;\n+  }\n   // One child must be constant, and the second must be Conv op.\n   const bool left_child_is_constant = IsReallyConstant(*mul_left_child);\n   const bool right_child_is_constant = IsReallyConstant(*mul_right_child);\n"
        ],
        "Title": "\n          Null pointer dereference in Grappler's `IsConstant`\n        "
    }
]
[
    {
        "Bug description": "Under certain scenarios, Grappler component of TensorFlow is vulnerable to an integer overflow during  cost estimation for crop and resize . Since the cropping parameters are user controlled, a malicious person can trigger undefined behavior.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -2681,27 +2681,42 @@ Status OpLevelCostEstimator::PredictCropAndResize(const OpContext& op_context,\n   // calculation differs from rough estimate in implementation, as it separates\n   // out cost per box from cost per pixel and cost per element.\n \n+  // Since crop arguments are user controlled, check for overflow.\n+  int64_t crop_area = MultiplyWithoutOverflow(crop_height, crop_width);\n+  if (crop_area < 0)\n+    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\n+                                   crop_height, \" with \", crop_width,\n+                                   \" would overflow\");\n+  int64_t crop_volume = MultiplyWithoutOverflow(crop_area, num_boxes);\n+  if (crop_volume < 0)\n+    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\n+                                   crop_area, \" with \", num_boxes,\n+                                   \" would overflow\");\n+  int64_t crop_depth = MultiplyWithoutOverflow(crop_height, num_boxes);\n+  if (crop_depth < 0)\n+    return errors::InvalidArgument(\"Cannot estimate cost, multiplying \",\n+                                   crop_height, \" with \", num_boxes,\n+                                   \" would overflow\");\n+\n   // Ops for variables height_scale and width_scale.\n   int64_t ops = (sub_cost * 6 + mul_cost * 2 + div_cost * 2) * num_boxes;\n   // Ops for variable in_y.\n-  ops += (mul_cost * 2 + sub_cost + add_cost) * crop_height * num_boxes;\n+  ops += (mul_cost * 2 + sub_cost + add_cost) * crop_depth;\n   // Ops for variable in_x (same computation across both branches).\n-  ops += (mul_cost * 2 + sub_cost + add_cost) * crop_height * crop_width *\n-         num_boxes;\n+  ops += (mul_cost * 2 + sub_cost + add_cost) * crop_volume;\n   // Specify op_cost based on the method.\n   if (use_bilinear_interp) {\n     // Ops for variables top_y_index, bottom_y_index, y_lerp.\n-    ops += (floor_cost + ceil_cost + sub_cost) * crop_height * num_boxes;\n+    ops += (floor_cost + ceil_cost + sub_cost) * crop_depth;\n     // Ops for variables left_x, right_x, x_lerp;\n-    ops += (floor_cost + ceil_cost + sub_cost) * crop_height * crop_width *\n-           num_boxes;\n+    ops += (floor_cost + ceil_cost + sub_cost) * crop_volume;\n     // Ops for innermost loop across depth.\n     ops +=\n         (cast_to_float_cost * 4 + add_cost * 3 + sub_cost * 3 + mul_cost * 3) *\n         output_elements;\n   } else /* method == \"nearest\" */ {\n     // Ops for variables closest_x_index and closest_y_index.\n-    ops += round_cost * 2 * crop_height * crop_width * num_boxes;\n+    ops += round_cost * 2 * crop_volume;\n     // Ops for innermost loop across depth.\n     ops += cast_to_float_cost * output_elements;\n   }\n"
        ],
        "Title": "\n          Integer overflow in Grappler cost estimation of crop and resize operation\n        "
    },
    {
        "Bug description": "A malicious user can cause a denial of service by altering a  SavedModel  such that  Grappler optimizer would attempt to build a tensor using a reference  . This would result in a crash due to a  CHECK -fail  in the   as reference types are not allowed.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          `CHECK`-fails due to attempting to build a reference tensor\n        "
    },
    {
        "Bug description": "A malicious user can cause a denial of service by altering a  SavedModel  such that  assertions in   would be falsified and crash the Python interpreter.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -191,7 +191,11 @@ class FunctionInstantiationHelper {\n     for (size_t i = 0; i < dtypes.size(); ++i) {\n       TF_RETURN_IF_ERROR(AddItem(strings::StrCat(arg_def.name(), \":\", i),\n                                  {true, arg_index, 0, false, {dtypes[i]}}));\n-      DCHECK_EQ(arg_index, result_.nodes.size());\n+      if (arg_index != result_.nodes.size()) {\n+        return errors::Internal(\n+            \"Expected arg_index to be equal to the number of nodes in result.\",\n+            \" Got \", arg_index, \" and \", result_.nodes.size());\n+      }\n       string name = arg_def.name();\n       if (dtypes.size() > 1) {\n         strings::StrAppend(&name, \"_\", i);\n"
        ],
        "Title": "\n          Multiple `CHECK`-fails in `function.cc`\n        "
    },
    {
        "Bug description": "When  decoding PNG images  TensorFlow can produce a memory leak if the image is invalid. \nAfter calling  png::CommonInitDecode(..., &decode) , the  decode  value contains allocated buffers which can only be freed by calling  png::CommonFreeDecode(&decode) . However, several error case in the function implementation invoke the  OP_REQUIRES  macro which immediately terminates the execution of the function, without allowing for the memory free to occur.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -18,6 +18,8 @@ limitations under the License.\n #include <cstdint>\n #include <memory>\n \n+#include \"tensorflow/core/lib/gtl/cleanup.h\"\n+\n #define EIGEN_USE_THREADS\n \n #include \"absl/strings/escaping.h\"\n@@ -326,6 +328,16 @@ class DecodeImageV2Op : public OpKernel {\n         context, png::CommonInitDecode(input, channels_, channel_bits, &decode),\n         errors::InvalidArgument(\"Invalid PNG. Failed to initialize decoder.\"));\n \n+    // If we reach this point, then there is data in `decode` which must be\n+    // freed by the time we end execution in this function. We cannot call\n+    // `png::CommonFreeDecode()` before an `OP_REQUIRES` because if\n+    // `OP_REQUIRES` constraint is satisfied then the data would be freed\n+    // prematurely. Instead, let's use a `Cleanup` object.\n+    auto cleanup = gtl::MakeCleanup([&decode]() {\n+      std::cerr << \"Cleanup called...\\n\";\n+      png::CommonFreeDecode(&decode);\n+    });\n+\n     // Verify that width and height are not too large:\n     // - verify width and height don't overflow int.\n     // - width can later be multiplied by channels_ and sizeof(uint16), so\n"
        ],
        "Title": "\n          Memory leak in decoding PNG images\n        "
    },
    {
        "Bug description": "A malicious user can cause a use after free behavior when  decoding PNG images :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -339,7 +339,6 @@ class DecodeImageV2Op : public OpKernel {\n     if (width != static_cast<int64_t>(decode.width) || width <= 0 ||\n         width >= (1LL << 27) || height != static_cast<int64_t>(decode.height) ||\n         height <= 0 || height >= (1LL << 27) || total_size >= (1LL << 29)) {\n-      png::CommonFreeDecode(&decode);\n       OP_REQUIRES(context, false,\n                   errors::InvalidArgument(\"PNG size too large for int: \",\n                                           decode.width, \" by \", decode.height));\n"
        ],
        "Title": "\n          Use after free in `DecodePng` kernel\n        "
    },
    {
        "Bug description": "A malicious user can cause a denial of service by altering a  SavedModel  such that  any binary op  would trigger  CHECK  failures. This occurs when the protobuf part corresponding to the tensor arguments is modified such that the  dtype  no longer matches the  dtype  expected by the op. In that case, calling the templated binary operator for the binary op would receive corrupted data, due to the type confusion involved:",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          `CHECK`-failures in binary ops due to type confusion\n        "
    },
    {
        "Bug description": "A malicious user can cause a denial of service by altering a  SavedModel  such that  TensorByteSize  would trigger  CHECK  failures.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -45,7 +45,7 @@ constexpr int kMaxTensorNestDepth = 100;\n // not fully defined return -1.\n int64_t TensorByteSize(const TensorProto& t) {\n   // num_elements returns -1 if shape is not fully defined.\n-  int64_t num_elems = TensorShape(t.tensor_shape()).num_elements();\n+  int64_t num_elems = PartialTensorShape(t.tensor_shape()).num_elements();\n   return num_elems < 0 ? -1 : num_elems * DataTypeSize(t.dtype());\n }\n \n"
        ],
        "Title": "\n          `CHECK`-failures in `TensorByteSize`\n        "
    },
    {
        "Bug description": "The Grappler optimizer in TensorFlow can be used to cause a denial of service by altering a  SavedModel  such that  SafeToRemoveIdentity  would trigger  CHECK  failures.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          `CHECK`-failures during Grappler's `SafeToRemoveIdentity`\n        "
    },
    {
        "Bug description": "The Grappler optimizer in TensorFlow can be used to cause a denial of service by altering a  SavedModel  such that  IsSimplifiableReshape  would trigger  CHECK  failures.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -1684,15 +1684,17 @@ Status ConstantFolding::FoldGraph(\n   return Status::OK();\n }\n \n-bool ConstantFolding::IsSimplifiableReshape(\n+Status ConstantFolding::IsSimplifiableReshape(\n     const NodeDef& node, const GraphProperties& properties) const {\n   if (!IsReshape(node)) {\n-    return false;\n+    return errors::Internal(\"Node \", node.name(), \" is not a Reshape node\");\n   }\n   CHECK_LE(2, node.input_size());\n   const NodeDef* new_shape = node_map_->GetNode(node.input(1));\n   if (!IsReallyConstant(*new_shape)) {\n-    return false;\n+    return errors::Internal(\"Node \", node.name(), \" has shape \",\n+                            new_shape->DebugString(),\n+                            \" which is not a constant\");\n   }\n   TensorVector outputs;\n   auto outputs_cleanup = gtl::MakeCleanup([&outputs] {\n@@ -1703,22 +1705,25 @@ bool ConstantFolding::IsSimplifiableReshape(\n \n   Status s = EvaluateNode(*new_shape, TensorVector(), &outputs);\n   if (!s.ok()) {\n-    return false;\n+    return errors::Internal(\"Could not evaluate node \", node.name());\n   }\n   CHECK_EQ(1, outputs.size());\n \n   const std::vector<OpInfo::TensorProperties>& props =\n       properties.GetInputProperties(node.name());\n   if (props.empty()) {\n-    return false;\n+    return errors::Internal(\"Node \", node.name(), \" has no properties\");\n   }\n   const OpInfo::TensorProperties& prop = props[0];\n   if (prop.dtype() == DT_INVALID) {\n-    return false;\n+    return errors::Internal(\"Node \", node.name(), \" has property \",\n+                            prop.DebugString(), \" with invalid dtype\");\n   }\n   const PartialTensorShape shape(prop.shape());\n   if (!shape.IsFullyDefined()) {\n-    return false;\n+    return errors::Internal(\"Node \", node.name(), \" has property \",\n+                            prop.DebugString(), \" with shape \",\n+                            shape.DebugString(), \" which is not fully defined\");\n   }\n \n   PartialTensorShape new_dims;\n@@ -1738,7 +1743,12 @@ bool ConstantFolding::IsSimplifiableReshape(\n     TF_CHECK_OK(TensorShapeUtils::MakeShape(shp, &new_dims));\n   }\n \n-  return shape.IsCompatibleWith(new_dims);\n+  if (!shape.IsCompatibleWith(new_dims)) {\n+    return errors::Internal(\"Expected shape \", shape.DebugString(),\n+                            \"to be compatible with \", new_dims.DebugString());\n+  }\n+\n+  return Status::OK();\n }\n \n #define IS_VALUE_CASE(DTYPE, VALUE)                   \\\n@@ -2925,7 +2935,7 @@ bool ConstantFolding::SimplifyReduction(GraphDef* optimized_graph,\n bool ConstantFolding::SimplifyReshape(const GraphProperties& properties,\n                                       bool use_shape_info, NodeDef* node) {\n   if (!use_shape_info || node->attr().count(\"T\") == 0 ||\n-      !IsSimplifiableReshape(*node, properties)) {\n+      !IsSimplifiableReshape(*node, properties).ok()) {\n     return false;\n   }\n   DataType output_type = node->attr().at(\"T\").type();\n",
            "@@ -129,8 +129,8 @@ class ConstantFolding : public GraphOptimizer {\n   Status FoldGraph(const GraphProperties& properties, GraphDef* output,\n                    absl::flat_hash_set<string>* nodes_to_not_simplify);\n \n-  bool IsSimplifiableReshape(const NodeDef& node,\n-                             const GraphProperties& properties) const;\n+  Status IsSimplifiableReshape(const NodeDef& node,\n+                               const GraphProperties& properties) const;\n   Status SimplifyGraph(GraphDef* optimized_graph, GraphProperties* properties,\n                        absl::flat_hash_set<string>* nodes_to_not_simplify);\n   Status SimplifyNode(NodeDef* node, GraphDef* optimized_graph,\n"
        ],
        "Title": "\n          `CHECK`-failures during Grappler's `IsSimplifiableReshape`\n        "
    },
    {
        "Bug description": "During shape inference, TensorFlow can  allocate a large vector  based on a value from a tensor controlled by the user:",
        "Sample Code": "",
        "Bug fix": [
            "@@ -14,6 +14,8 @@ limitations under the License.\n ==============================================================================*/\n #include \"tensorflow/core/framework/shape_inference.h\"\n \n+#include <cstdint>\n+\n #include \"tensorflow/core/framework/bounds_check.h\"\n #include \"tensorflow/core/framework/full_type_util.h\"\n #include \"tensorflow/core/framework/node_def.pb.h\"\n@@ -789,6 +791,19 @@ Status InferenceContext::InternalMakeShapeFromTensor(\n       return ReturnUnknownShape(out);\n     }\n     const auto num_dims = Value(shape_dim);\n+    // TODO(mihaimaruseac): Should be `TensorShape::MaxDimensions()` as we are\n+    // not able to materialize shapes with more than this number of dimensions\n+    // but then shape inference would fail for operations such as\n+    // `tf.range`/`tf.ones`, etc. where the shape is not really materialized,\n+    // only used during the inference. Hence, just prevent doing a `reserve`\n+    // with a very large argument.\n+    const int64_t max_dimensions = 1 << 20;\n+    if (num_dims >= max_dimensions) {\n+      return errors::Internal(\n+          \"Cannot create a tensor with \", num_dims,\n+          \" dimensions, as these would be more than maximum of \",\n+          max_dimensions);\n+    }\n     std::vector<DimensionHandle> dims;\n     dims.reserve(num_dims);\n     for (int i = 0; i < num_dims; i++) dims.push_back(UnknownDim());\n"
        ],
        "Title": "\n          Abort caused by allocating a vector that is too large\n        "
    }
]
[
    {
        "Bug description": "If a graph node is invalid, TensorFlow can leak memory in the  implementation of  :",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Memory leak when a graph node is invalid\n        "
    },
    {
        "Bug description": "The  implementation of   is vulnerable to a crash caused by dereferencing a null pointer:",
        "Sample Code": "",
        "Bug fix": [
            "@@ -34,9 +34,14 @@ Status GetInitOp(const string& export_dir, const MetaGraphDef& meta_graph_def,\n   const auto& init_op_sig_it =\n       meta_graph_def.signature_def().find(kSavedModelInitOpSignatureKey);\n   if (init_op_sig_it != sig_def_map.end()) {\n-    *init_op_name = init_op_sig_it->second.outputs()\n-                        .find(kSavedModelInitOpSignatureKey)\n-                        ->second.name();\n+    const auto& sig_def_outputs = init_op_sig_it->second.outputs();\n+    const auto& sig_def_outputs_it =\n+        sig_def_outputs.find(kSavedModelInitOpSignatureKey);\n+    if (sig_def_outputs_it == sig_def_outputs.end()) {\n+      return errors::FailedPrecondition(\"Could not find output \",\n+                                        kSavedModelInitOpSignatureKey);\n+    }\n+    *init_op_name = sig_def_outputs_it->second.name();\n     return Status::OK();\n   }\n \n"
        ],
        "Title": "\n          Null dereference in `GetInitOp`\n        "
    },
    {
        "Bug description": "The  implementation of   is vulnerable to an integer overflow if an attacker can create an operation which would involve tensors with large enough number of elements:",
        "Sample Code": "",
        "Bug fix": [
            "@@ -340,6 +340,7 @@ cc_library(\n         \"//tensorflow/core:lib\",\n         \"//tensorflow/core:protos_all_cc\",\n         \"//tensorflow/core/grappler/clusters:utils\",\n+        \"//tensorflow/core/util:overflow\",\n     ] + tf_protos_grappler(),\n )\n \n",
            "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"tensorflow/core/grappler/costs/op_context.h\"\n #include \"tensorflow/core/grappler/costs/utils.h\"\n #include \"tensorflow/core/platform/errors.h\"\n+#include \"tensorflow/core/util/overflow.h\"\n \n namespace tensorflow {\n namespace grappler {\n@@ -1607,7 +1608,14 @@ int64_t OpLevelCostEstimator::CalculateOutputSize(const OpInfo& op_info,\n     auto output_shape = MaybeGetMinimumShape(original_output_shape, num_dims,\n                                              found_unknown_shapes);\n     for (const auto& dim : output_shape.dim()) {\n-      output_size *= dim.size();\n+      int64_t new_output_size =\n+          MultiplyWithoutOverflow(output_size, dim.size());\n+      if (new_output_size < 0) {\n+        VLOG(1) << \"Overflow encountered when estimating cost, multiplying \"\n+                << output_size << \" with \" << dim.size();\n+        return -1;\n+      }\n+      output_size = new_output_size;\n     }\n     total_output_size += output_size;\n     VLOG(1) << \"Output Size: \" << output_size\n"
        ],
        "Title": "\n          Integer overflow in `OpLevelCostEstimator::CalculateOutputSize`\n        "
    },
    {
        "Bug description": "The  implementation of   is vulnerable to an integer overflow if an attacker can create an operation which would involve a tensor with large enough number of elements:",
        "Sample Code": "",
        "Bug fix": [
            "@@ -1555,7 +1555,13 @@ int64_t OpLevelCostEstimator::CalculateTensorSize(\n   int64_t count = CalculateTensorElementCount(tensor, found_unknown_shapes);\n   int size = DataTypeSize(BaseType(tensor.dtype()));\n   VLOG(2) << \"Count: \" << count << \" DataTypeSize: \" << size;\n-  return count * size;\n+  int64_t tensor_size = MultiplyWithoutOverflow(count, size);\n+  if (tensor_size < 0) {\n+    VLOG(1) << \"Overflow encountered when computing tensor size, multiplying \"\n+            << count << \" with \" << size;\n+    return -1;\n+  }\n+  return tensor_size;\n }\n \n int64_t OpLevelCostEstimator::CalculateInputSize(const OpInfo& op_info,\n"
        ],
        "Title": "\n          Integer overflow in `OpLevelCostEstimator::CalculateTensorSize`\n        "
    },
    {
        "Bug description": "The  implementation of   can result in copying unitialized data to a new tensor. This later results in undefined behavior.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -50,6 +50,12 @@ class AssignOp : public OpKernel {\n     // We always return the input ref.\n     context->forward_ref_input_to_ref_output(0, 0);\n \n+    // Prevent copying uninitialized data, to solve harder to debug undefined\n+    // behaviors that cannot be traced back to the original tensor.\n+    OP_REQUIRES(\n+        context, rhs.IsInitialized(),\n+        errors::Internal(\"Right hand side of AssignOp is not initialized\"));\n+\n     // We can't always know how this value will be used downstream, so make\n     // conservative assumptions in specifying constraints on the memory\n     // allocation attributes, unless the Grappler graph analysis determined that\n"
        ],
        "Title": "\n          Unitialized variable access in `AssignOp`\n        "
    },
    {
        "Bug description": "There is a typo in TensorFlow's  SpecializeType  which results in heap OOB read/write:",
        "Sample Code": "",
        "Bug fix": [
            "@@ -100,7 +100,7 @@ StatusOr<FullTypeDef> SpecializeType(const AttrSlice& attrs,\n     // verifications are needed, they should be done by separately, and in a\n     // way that can be reused for type inference.\n     for (int j = 0; j < t->args_size(); j++) {\n-      auto* arg = t->mutable_args(i);\n+      auto* arg = t->mutable_args(j);\n       if (arg->type_id() == TFT_VAR) {\n         const auto* attr = attrs.Find(arg->s());\n         if (attr == nullptr) {\n"
        ],
        "Title": "\n          Heap OOB read/write in `SpecializeType`\n        "
    },
    {
        "Bug description": "Under certain scenarios, TensorFlow can fail to specialize a type during  shape inference :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -170,7 +170,10 @@ void InferenceContext::PreInputInit(\n     const std::vector<ShapeHandle>& input_tensors_as_shapes) {\n   // TODO(mdan): This is also done at graph construction. Run only here instead?\n   const auto ret = full_type::SpecializeType(attrs_, op_def);\n-  DCHECK(ret.status().ok()) << \"while instantiating types: \" << ret.status();\n+  if (!ret.status().ok()) {\n+    construction_status_ = ret.status();\n+    return;\n+  }\n   ret_types_ = ret.ValueOrDie();\n \n   input_tensors_ = input_tensors;\n"
        ],
        "Title": "\n          Crash when type cannot be specialized\n        "
    },
    {
        "Bug description": "When decoding a tensor from protobuf, TensorFlow might do a null-dereference if attributes of some mutable arguments to some operations are missing from the proto. This is  guarded by a  :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -22,6 +22,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/op_def.pb.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/platform/statusor.h\"\n+#include \"tensorflow/core/protobuf/error_codes.pb.h\"\n \n namespace tensorflow {\n \n@@ -102,7 +103,11 @@ StatusOr<FullTypeDef> SpecializeType(const AttrSlice& attrs,\n       auto* arg = t->mutable_args(i);\n       if (arg->type_id() == TFT_VAR) {\n         const auto* attr = attrs.Find(arg->s());\n-        DCHECK(attr != nullptr);\n+        if (attr == nullptr) {\n+          return Status(\n+              error::INVALID_ARGUMENT,\n+              absl::StrCat(\"Could not find an attribute for key \", arg->s()));\n+        }\n         if (attr->value_case() == AttrValue::kList) {\n           const auto& attr_list = attr->list();\n           arg->set_type_id(TFT_PRODUCT);\n"
        ],
        "Title": "\n          Null-dereference when specializing tensor type\n        "
    },
    {
        "Bug description": "When decoding a tensor from protobuf, a TensorFlow process can encounter cases where a  CHECK  assertion is invalidated based on user controlled arguments, if the tensors have an invalid  dtype  and 0 elements or an invalid shape. This allows attackers to cause denial of services in TensorFlow processes.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -983,6 +983,15 @@ bool Tensor::FromProto(Allocator* a, const TensorProto& proto) {\n                          dtype_error = true, dtype_error = true);\n     }\n     if (dtype_error || p == nullptr) return false;\n+  } else {\n+    // Handle the case of empty tensors (N = 0) or tensors with incomplete shape\n+    // (N = -1). All other values of `shape.num_elements()` should be invalid by\n+    // construction.\n+    // Here, we just need to validate that the `proto.dtype()` value is valid.\n+    bool dtype_error = false;\n+    CASES_WITH_DEFAULT(proto.dtype(), break, dtype_error = true,\n+                       dtype_error = true);\n+    if (dtype_error) return false;\n   }\n   shape_ = shape;\n   set_dtype(proto.dtype());\n"
        ],
        "Title": "\n          `CHECK`-fail when decoding invalid tensors from proto\n        "
    },
    {
        "Bug description": "TensorFlow is vulnerable to a heap OOB write in  Grappler :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -1134,7 +1134,12 @@ class SymbolicShapeRefiner {\n         GetUnknownOutputShape(node, output_port);\n     InferenceContext* ctx = GetContext(node);\n     if (ctx == nullptr) {\n-      return errors::InvalidArgument(\"Missing context\");\n+      return errors::InvalidArgument(\"SetUnknownShape: Missing context\");\n+    }\n+    if (output_port < 0 || output_port >= ctx->num_outputs()) {\n+      return errors::InvalidArgument(\n+          \"SetUnknownShape: output_port must be in [0, \", ctx->num_outputs(),\n+          \") but was \", output_port);\n     }\n     ctx->set_output(output_port, shape);\n     return Status::OK();\n"
        ],
        "Title": "\n          Heap OOB write in Grappler\n        "
    }
]
[
    {
        "Bug description": "An attacker can trigger denial of service via assertion failure by altering a  SavedModel  on disk such that  AttrDef s of some operation are duplicated.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -821,9 +821,10 @@ bool RepeatedAttrDefEqual(\n     const protobuf::RepeatedPtrField<OpDef::AttrDef>& a2) {\n   std::unordered_map<string, const OpDef::AttrDef*> a1_set;\n   for (const OpDef::AttrDef& def : a1) {\n-    DCHECK(a1_set.find(def.name()) == a1_set.end())\n-        << \"AttrDef names must be unique, but '\" << def.name()\n-        << \"' appears more than once\";\n+    if (a1_set.find(def.name()) != a1_set.end()) {\n+      LOG(ERROR) << \"AttrDef names must be unique, but '\" << def.name()\n+                 << \"' appears more than once\";\n+    }\n     a1_set[def.name()] = &def;\n   }\n   for (const OpDef::AttrDef& def : a2) {\n"
        ],
        "Title": "\n          `CHECK`-fail with repeated `AttrDef`\n        "
    },
    {
        "Bug description": "When decoding a resource handle tensor from protobuf, a TensorFlow process can encounter cases where a  CHECK  assertion is invalidated based on user controlled arguments. This allows attackers to cause denial of services in TensorFlow processes.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -734,7 +734,9 @@ cc_library(\n         \"//tensorflow/core/lib/core:errors\",\n         \"//tensorflow/core/lib/strings:strcat\",\n         \"//tensorflow/core/platform:casts\",\n+        \"//tensorflow/core/platform:errors\",\n         \"//tensorflow/core/platform:intrusive_ptr\",\n+        \"//tensorflow/core/platform:macros\",\n         \"//tensorflow/core/platform:statusor\",\n         \"//tensorflow/core/platform:tensor_coding\",\n         \"//tensorflow/core/platform:types\",\n",
            "@@ -17,8 +17,11 @@ limitations under the License.\n \n #include \"absl/strings/str_format.h\"\n #include \"tensorflow/core/framework/resource_handle.pb.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n #include \"tensorflow/core/lib/strings/strcat.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n+#include \"tensorflow/core/platform/macros.h\"\n \n namespace tensorflow {\n \n@@ -28,7 +31,15 @@ namespace tensorflow {\n ResourceHandle::ResourceHandle() {}\n \n ResourceHandle::ResourceHandle(const ResourceHandleProto& proto) {\n-  FromProto(proto);\n+  TF_CHECK_OK(FromProto(proto));\n+}\n+\n+Status ResourceHandle::BuildResourceHandle(const ResourceHandleProto& proto,\n+                                           ResourceHandle* out) {\n+  if (out == nullptr)\n+    return errors::Internal(\n+        \"BuildResourceHandle() was called with nullptr for the output\");\n+  return out->FromProto(proto);\n }\n \n ResourceHandle::~ResourceHandle() {}\n@@ -46,7 +57,7 @@ void ResourceHandle::AsProto(ResourceHandleProto* proto) const {\n   }\n }\n \n-void ResourceHandle::FromProto(const ResourceHandleProto& proto) {\n+Status ResourceHandle::FromProto(const ResourceHandleProto& proto) {\n   set_device(proto.device());\n   set_container(proto.container());\n   set_name(proto.name());\n@@ -55,10 +66,16 @@ void ResourceHandle::FromProto(const ResourceHandleProto& proto) {\n   std::vector<DtypeAndPartialTensorShape> dtypes_and_shapes;\n   for (const auto& dtype_and_shape : proto.dtypes_and_shapes()) {\n     DataType dtype = dtype_and_shape.dtype();\n-    PartialTensorShape shape(dtype_and_shape.shape());\n+    PartialTensorShape shape;\n+    Status s = PartialTensorShape::BuildPartialTensorShape(\n+        dtype_and_shape.shape(), &shape);\n+    if (!s.ok()) {\n+      return s;\n+    }\n     dtypes_and_shapes.push_back(DtypeAndPartialTensorShape{dtype, shape});\n   }\n   dtypes_and_shapes_ = std::move(dtypes_and_shapes);\n+  return Status::OK();\n }\n \n string ResourceHandle::SerializeAsString() const {\n@@ -69,9 +86,7 @@ string ResourceHandle::SerializeAsString() const {\n \n bool ResourceHandle::ParseFromString(const string& s) {\n   ResourceHandleProto proto;\n-  const bool status = proto.ParseFromString(s);\n-  if (status) FromProto(proto);\n-  return status;\n+  return proto.ParseFromString(s) && FromProto(proto).ok();\n }\n \n string ResourceHandle::DebugString() const {\n@@ -140,7 +155,9 @@ bool DecodeResourceHandleList(std::unique_ptr<port::StringListDecoder> d,\n     if (!proto.ParseFromArray(d->Data(sizes[i]), sizes[i])) {\n       return false;\n     }\n-    ps[i].FromProto(proto);\n+    if (!ps[i].FromProto(proto).ok()) {\n+      return false;\n+    }\n   }\n   return true;\n }\n",
            "@@ -46,6 +46,11 @@ class ResourceHandle {\n   ResourceHandle(const ResourceHandleProto& proto);\n   ~ResourceHandle();\n \n+  // Use this factory method if the `proto` comes from user controlled input, to\n+  // prevent a denial of service.\n+  static Status BuildResourceHandle(const ResourceHandleProto& proto,\n+                                    ResourceHandle* out);\n+\n   // Unique name for the device containing the resource.\n   const std::string& device() const { return device_; }\n \n@@ -91,7 +96,7 @@ class ResourceHandle {\n \n   // Conversion to and from ResourceHandleProto\n   void AsProto(ResourceHandleProto* proto) const;\n-  void FromProto(const ResourceHandleProto& proto);\n+  Status FromProto(const ResourceHandleProto& proto);\n \n   // Serialization via ResourceHandleProto\n   std::string SerializeAsString() const;\n",
            "@@ -537,6 +537,46 @@ TensorBuffer* FromProtoField(Allocator* a, const TensorProto& in, int64_t n) {\n   return buf;\n }\n \n+// Separate implementation for `ResourceHandle` to handle the case when the\n+// proto for the resource is invalid. See `resource_handle.h` constructor and\n+// static factory builder.\n+template <>\n+TensorBuffer* FromProtoField<ResourceHandle>(Allocator* a,\n+                                             const TensorProto& in, int64_t n) {\n+  CHECK_GT(n, 0);\n+  Buffer<ResourceHandle>* buf = new Buffer<ResourceHandle>(a, n);\n+  ResourceHandle* data = buf->template base<ResourceHandle>();\n+  if (data == nullptr) {\n+    buf->Unref();\n+    return nullptr;\n+  }\n+  const int64_t in_n = ProtoHelper<ResourceHandle>::NumElements(in);\n+  if (in_n <= 0) {\n+    std::fill_n(data, n, ResourceHandle());\n+  } else {\n+    // If tensor shape says we have n < in_n elements in the output tensor\n+    // then make sure to only decode the first n out of the in_n elements in the\n+    // in tensors. In all other cases, we decode all in_n elements of in and set\n+    // the remaining elements up to n to be the default ResourceHandle() value.\n+    const int64_t real_n = n < in_n ? n : in_n;\n+    for (int64_t i = 0; i < real_n; ++i) {\n+      Status s = ResourceHandle::BuildResourceHandle(in.resource_handle_val(i),\n+                                                     &data[i]);\n+      if (!s.ok()) {\n+        LOG(ERROR) << \"Could not decode resource handle from proto \\\"\"\n+                   << in.resource_handle_val(i).ShortDebugString()\n+                   << \"\\\", returned status: \" << s.ToString();\n+        buf->Unref();\n+        return nullptr;\n+      }\n+    }\n+    for (int64_t i = in_n; i < n; ++i) {\n+      data[i] = ResourceHandle();\n+    }\n+  }\n+  return buf;\n+}\n+\n template <>\n TensorBuffer* FromProtoField<Variant>(Allocator* a, const TensorProto& in,\n                                       int64_t n) {\n"
        ],
        "Title": "\n          `CHECK`-fail when decoding resource handles from proto\n        "
    },
    {
        "Bug description": "The implementation of  tf.sparse.split  does not fully validate the input arguments. Hence, a malicious user can trigger a denial of service via a segfault or a heap OOB read:",
        "Sample Code": "data = tf.random.uniform([1, 32, 32], dtype=tf.float32)\naxis = [1, 2]\nx = tf.sparse.from_dense(data)\n)\nresult = tf.sparse.split(x,3, axis=axis)",
        "Bug fix": "",
        "Title": "\n          Missing validation causes `tf.sparse.split` to crash when `axis` is a tuple\n        "
    },
    {
        "Bug description": "The implementation of  Range  suffers from integer overflows. These can trigger undefined behavior or, in some scenarios, extremely large allocations.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Integer overflow in Range resulting in undefined behavior and OOM\n        "
    },
    {
        "Bug description": "In multiple places, TensorFlow uses  tempfile.mktemp  to create temporary files. While this is acceptable in testing, in utilities and libraries it is dangerous as a different process can create the file between the check for the filename in  mktemp  and the actual creation of the file by a subsequent operation (a TOC/TOU type of weakness).",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Insecure temporary file\n        "
    },
    {
        "Bug description": "An attacker can craft a TFLite model that would allow limited reads and writes outside of arrays in TFLite. This exploits missing validation in  the conversion from sparse tensors to dense tensors .",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Read and Write outside of bounds in TFLite\n        "
    },
    {
        "Bug description": "An attacker can craft a TFLite model that would cause a write outside of bounds of an array in TFLite. In fact, the attacker can override the linked list used by the memory allocator. This can be leveraged for an arbitrary write primitive under certain conditions.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -928,6 +928,36 @@ TfLiteStatus EvalShuffledQuantized(TfLiteContext* context, TfLiteNode* node,\n   return kTfLiteOk;\n }\n \n+// Verifies that sparsity values are valid given input/weight/output.\n+bool VerifySparsity(const RuntimeShape& weights_shape,\n+                    const RuntimeShape& input_shape,\n+                    const RuntimeShape& output_shape,\n+                    const TfLiteSparsity* sparsity) {\n+  const int weights_dims_count = weights_shape.DimensionsCount();\n+  const int output_dims_count = output_shape.DimensionsCount();\n+  const int w0_size = sparsity->dim_metadata[0].dense_size;\n+  const int accum_depth = weights_shape.Dims(weights_dims_count - 1);\n+  const int output_elements = output_shape.FlatSize();\n+  const int input_elements = input_shape.FlatSize();\n+  const int batches = FlatSizeSkipDim(output_shape, output_dims_count - 1);\n+  const int output_depth = MatchingDim(weights_shape, weights_dims_count - 2,\n+                                       output_shape, output_dims_count - 1);\n+  const int max_batch_index = batches - 1;\n+  const int max_output = max_batch_index * output_depth + w0_size;\n+  const int max_batch_depth = accum_depth * max_batch_index;\n+\n+  // Verify output size is enough.\n+  if (output_elements < max_output) return false;\n+\n+  // Verify index from sparse in input is valid.\n+  for (int i = 0; i < sparsity->dim_metadata[1].array_indices->size; ++i) {\n+    if (input_elements <=\n+        max_batch_depth + sparsity->dim_metadata[1].array_indices->data[i])\n+      return false;\n+  }\n+  return true;\n+}\n+\n template <KernelType kernel_type>\n TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\n                        TfLiteFullyConnectedParams* params, OpData* data,\n@@ -968,24 +998,32 @@ TfLiteStatus EvalFloat(TfLiteContext* context, TfLiteNode* node,\n                            \"Unsupported sparse fully-connected weight format.\");\n         return kTfLiteError;\n       }\n+      const auto& input_shape = GetTensorShape(input);\n+      const auto& filter_shape = GetTensorShape(filter);\n+      const auto& output_shape = GetTensorShape(output);\n+      const auto& bias_shape = GetTensorShape(bias);\n+      if (!VerifySparsity(filter_shape, input_shape, output_shape, &sparsity)) {\n+        TF_LITE_KERNEL_LOG(context, \"Invalid sparse fully-connected format.\");\n+        return kTfLiteError;\n+      }\n \n       if (sparsity.dim_metadata_size == kDimMetadataSizeRandomSparse) {\n         // Random sparse.\n         optimized_ops::FullyConnectedSparseWeight(\n-            sparsity, op_params, GetTensorShape(input),\n-            GetTensorData<float>(input), GetTensorShape(filter),\n-            GetTensorData<float>(filter), GetTensorShape(bias),\n-            GetTensorData<float>(bias), GetTensorShape(output),\n-            GetTensorData<float>(output));\n+            sparsity, op_params,                         // Disable formatting\n+            input_shape, GetTensorData<float>(input),    // Disable formatting\n+            filter_shape, GetTensorData<float>(filter),  // Disable formatting\n+            bias_shape, GetTensorData<float>(bias),      // Disable formatting\n+            output_shape, GetTensorData<float>(output));\n       } else if (sparsity.dim_metadata_size == kDimMetadataSizeBlockSparse &&\n                  sparsity.dim_metadata[2].dense_size == 4) {\n         // Block sparse with block size of 1x4.\n         optimized_ops::FullyConnectedSparseWeight1x4(\n-            sparsity, op_params, GetTensorShape(input),\n-            GetTensorData<float>(input), GetTensorShape(filter),\n-            GetTensorData<float>(filter), GetTensorShape(bias),\n-            GetTensorData<float>(bias), GetTensorShape(output),\n-            GetTensorData<float>(output),\n+            sparsity, op_params,                         // Disable formatting\n+            input_shape, GetTensorData<float>(input),    // Disable formatting\n+            filter_shape, GetTensorData<float>(filter),  // Disable formatting\n+            bias_shape, GetTensorData<float>(bias),      // Disable formatting\n+            output_shape, GetTensorData<float>(output),\n             CpuBackendContext::GetFromContext(context));\n       } else {\n         TF_LITE_KERNEL_LOG(context,\n"
        ],
        "Title": "\n          Dangerous OOB write in TFLite\n        "
    },
    {
        "Bug description": "An attacker can craft a TFLite model that would cause an integer overflow  in embedding lookup operations :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -1030,6 +1030,7 @@ cc_library(\n     copts = tflite_copts_warnings() + tflite_copts(),\n     deps = [\n         \":kernel_api\",\n+        \":macros\",\n         \"//tensorflow/lite/c:common\",\n         \"//tensorflow/lite/schema:schema_fbs\",\n     ],\n@@ -1083,6 +1084,7 @@ cc_test(\n     features = [\"-dynamic_link_test_srcs\"],  # see go/dynamic_link_test_srcs\n     deps = [\n         \":util\",\n+        \"//tensorflow/lite/c:c_api_types\",\n         \"//tensorflow/lite/c:common\",\n         \"//tensorflow/lite/schema:schema_fbs\",\n         \"@com_google_googletest//:gtest_main\",\n",
            "@@ -690,27 +690,6 @@ TfLiteStatus Subgraph::CheckInputAndOutputForOverlap(const int* input_indices,\n   return kTfLiteOk;\n }\n \n-namespace {\n-// Multiply two sizes and return true if overflow occurred;\n-// This is based off tensorflow/overflow.h but is simpler as we already\n-// have unsigned numbers. It is also generalized to work where sizeof(size_t)\n-// is not 8.\n-TfLiteStatus MultiplyAndCheckOverflow(size_t a, size_t b, size_t* product) {\n-  // Multiplying a * b where a and b are size_t cannot result in overflow in a\n-  // size_t accumulator if both numbers have no non-zero bits in their upper\n-  // half.\n-  constexpr size_t size_t_bits = 8 * sizeof(size_t);\n-  constexpr size_t overflow_upper_half_bit_position = size_t_bits / 2;\n-  *product = a * b;\n-  // If neither integers have non-zero bits past 32 bits can't overflow.\n-  // Otherwise check using slow devision.\n-  if (TFLITE_EXPECT_FALSE((a | b) >> overflow_upper_half_bit_position != 0)) {\n-    if (a != 0 && *product / a != b) return kTfLiteError;\n-  }\n-  return kTfLiteOk;\n-}\n-}  // namespace\n-\n TfLiteStatus Subgraph::BytesRequired(TfLiteType type, const int* dims,\n                                      size_t dims_size, size_t* bytes) {\n   TF_LITE_ENSURE(&context_, bytes != nullptr);\n",
            "@@ -27,6 +27,7 @@ limitations under the License.\n \n #include \"tensorflow/lite/builtin_ops.h\"\n #include \"tensorflow/lite/c/common.h\"\n+#include \"tensorflow/lite/core/macros.h\"\n #include \"tensorflow/lite/schema/schema_generated.h\"\n \n namespace tflite {\n@@ -176,4 +177,19 @@ bool IsValidationSubgraph(const char* name) {\n   // NOLINTNEXTLINE: can't use absl::StartsWith as absl is not allowed.\n   return name && std::string(name).find(kValidationSubgraphNamePrefix) == 0;\n }\n+\n+TfLiteStatus MultiplyAndCheckOverflow(size_t a, size_t b, size_t* product) {\n+  // Multiplying a * b where a and b are size_t cannot result in overflow in a\n+  // size_t accumulator if both numbers have no non-zero bits in their upper\n+  // half.\n+  constexpr size_t size_t_bits = 8 * sizeof(size_t);\n+  constexpr size_t overflow_upper_half_bit_position = size_t_bits / 2;\n+  *product = a * b;\n+  // If neither integers have non-zero bits past 32 bits can't overflow.\n+  // Otherwise check using slow devision.\n+  if (TFLITE_EXPECT_FALSE((a | b) >> overflow_upper_half_bit_position != 0)) {\n+    if (a != 0 && *product / a != b) return kTfLiteError;\n+  }\n+  return kTfLiteOk;\n+}\n }  // namespace tflite\n",
            "@@ -99,6 +99,12 @@ constexpr char kValidationSubgraphNamePrefix[] = \"VALIDATION:\";\n // Checks whether the prefix of the subgraph name indicates the subgraph is a\n // validation subgraph.\n bool IsValidationSubgraph(const char* name);\n+\n+// Multiply two sizes and return true if overflow occurred;\n+// This is based off tensorflow/overflow.h but is simpler as we already\n+// have unsigned numbers. It is also generalized to work where sizeof(size_t)\n+// is not 8.\n+TfLiteStatus MultiplyAndCheckOverflow(size_t a, size_t b, size_t* product);\n }  // namespace tflite\n \n #endif  // TENSORFLOW_LITE_UTIL_H_\n",
            "@@ -22,6 +22,7 @@ limitations under the License.\n #include <vector>\n \n #include <gtest/gtest.h>\n+#include \"tensorflow/lite/c/c_api_types.h\"\n #include \"tensorflow/lite/c/common.h\"\n #include \"tensorflow/lite/schema/schema_generated.h\"\n \n@@ -130,5 +131,12 @@ TEST(ValidationSubgraph, NameIsDetected) {\n   EXPECT_TRUE(IsValidationSubgraph(\"VALIDATION:main\"));\n }\n \n+TEST(MultiplyAndCheckOverflow, Validate) {\n+  size_t res = 0;\n+  EXPECT_TRUE(MultiplyAndCheckOverflow(1, 2, &res) == kTfLiteOk);\n+  EXPECT_FALSE(MultiplyAndCheckOverflow(static_cast<size_t>(123456789023),\n+                                        1223423425, &res) == kTfLiteOk);\n+}\n+\n }  // namespace\n }  // namespace tflite\n"
        ],
        "Title": "\n          Integer overflow in TFLite\n        "
    },
    {
        "Bug description": "An attacker can craft a TFLite model that would cause an integer overflow  in  :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -21,10 +21,10 @@ limitations under the License.\n #include <string.h>\n #endif  // TF_LITE_STATIC_MEMORY\n \n-int TfLiteIntArrayGetSizeInBytes(int size) {\n+size_t TfLiteIntArrayGetSizeInBytes(int size) {\n   static TfLiteIntArray dummy;\n \n-  int computed_size = sizeof(dummy) + sizeof(dummy.data[0]) * size;\n+  size_t computed_size = sizeof(dummy) + sizeof(dummy.data[0]) * size;\n #if defined(_MSC_VER)\n   // Context for why this is needed is in http://b/189926408#comment21\n   computed_size -= sizeof(dummy.data[0]);\n@@ -51,7 +51,7 @@ int TfLiteIntArrayEqualsArray(const TfLiteIntArray* a, int b_size,\n #ifndef TF_LITE_STATIC_MEMORY\n \n TfLiteIntArray* TfLiteIntArrayCreate(int size) {\n-  int alloc_size = TfLiteIntArrayGetSizeInBytes(size);\n+  size_t alloc_size = TfLiteIntArrayGetSizeInBytes(size);\n   if (alloc_size <= 0) return NULL;\n   TfLiteIntArray* ret = (TfLiteIntArray*)malloc(alloc_size);\n   if (!ret) return ret;\n",
            "@@ -98,7 +98,7 @@ typedef struct TfLiteIntArray {\n \n // Given the size (number of elements) in a TfLiteIntArray, calculate its size\n // in bytes.\n-int TfLiteIntArrayGetSizeInBytes(int size);\n+size_t TfLiteIntArrayGetSizeInBytes(int size);\n \n #ifndef TF_LITE_STATIC_MEMORY\n // Create a array of a given `size` (uninitialized entries).\n"
        ],
        "Title": "\n          Integer overflow in TFLite array creation\n        "
    },
    {
        "Bug description": "An attacker can craft a TFLite model that would trigger a division by zero in  the implementation of depthwise convolutions .",
        "Sample Code": "",
        "Bug fix": [
            "@@ -115,6 +115,8 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n \n   TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n   TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);\n+  TF_LITE_ENSURE(context, params->dilation_height_factor > 0);\n+  TF_LITE_ENSURE(context, params->dilation_width_factor > 0);\n \n   const TfLiteType data_type = input->type;\n \n"
        ],
        "Title": "\n          FPE in depthwise convolutions in TFLite\n        "
    }
]
[
    {
        "Bug description": "An attacker can craft a TFLite model that would trigger a division by zero in  BiasAndClamp :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -75,6 +75,7 @@ float ActivationFunction(float x) {\n inline void BiasAndClamp(float clamp_min, float clamp_max, int bias_size,\n                          const float* bias_data, int array_size,\n                          float* array_data) {\n+  if (bias_size == 0) return;\n   // Note: see b/132215220: in May 2019 we thought it would be OK to replace\n   // this with the Eigen one-liner:\n   //   return (array.colwise() + bias).cwiseMin(clamp_max).cwiseMin(clamp_max).\n"
        ],
        "Title": "\n          FPE in `BiasAndClamp` in TFLite\n        "
    },
    {
        "Bug description": "The  implementation of   is vulnerable to a heap overflow:",
        "Sample Code": "import numpy as np\n\ntf.raw_ops.SparseCountSparseOutput(\n  indices=[[-1,-1]],\n  values=[2],\n  dense_shape=[1, 1],\n  weights=[1],\n  binary_output=True,\n  minlength=-1,\n  maxlength=-1,\n  ,\n  name=None)",
        "Bug fix": [
            "@@ -185,6 +185,27 @@ class SparseCount : public OpKernel {\n                 errors::InvalidArgument(\n                     \"Input indices must be a 2-dimensional tensor. Got: \",\n                     indices.shape().DebugString()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(values.shape()),\n+                errors::InvalidArgument(\"Input values must be a vector. Got: \",\n+                                        values.shape().DebugString()));\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(shape.shape()),\n+                errors::InvalidArgument(\"Input shape must be a vector. Got: \",\n+                                        shape.shape().DebugString()));\n+    OP_REQUIRES(context,\n+                values.shape().dim_size(0) == indices.shape().dim_size(0),\n+                errors::InvalidArgument(\n+                    \"Number of values must match first dimension of indices.\",\n+                    \"Got \", values.shape().dim_size(0),\n+                    \" values, indices shape: \", indices.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, shape.shape().dim_size(0) == indices.shape().dim_size(1),\n+        errors::InvalidArgument(\n+            \"Number of dimensions must match second dimension of indices.\",\n+            \"Got \", shape.shape().dim_size(0),\n+            \" dimensions, indices shape: \", indices.shape().DebugString()));\n+    OP_REQUIRES(context, shape.NumElements() > 0,\n+                errors::InvalidArgument(\n+                    \"The shape argument requires at least one element.\"));\n \n     if (use_weights) {\n       OP_REQUIRES(\n@@ -195,28 +216,11 @@ class SparseCount : public OpKernel {\n               \"; values shape: \", values.shape().DebugString()));\n     }\n \n-    OP_REQUIRES(context, shape.NumElements() != 0,\n-                errors::InvalidArgument(\n-                    \"The shape argument requires at least one element.\"));\n-\n     bool is_1d = shape.NumElements() == 1;\n     auto shape_vector = shape.flat<int64_t>();\n     int num_batches = is_1d ? 1 : shape_vector(0);\n     int num_values = values.NumElements();\n \n-    for (int b = 0; b < shape_vector.size(); b++) {\n-      OP_REQUIRES(context, shape_vector(b) >= 0,\n-                  errors::InvalidArgument(\n-                      \"Elements in dense_shape must be >= 0. Instead got:\",\n-                      shape.DebugString()));\n-    }\n-\n-    OP_REQUIRES(context, num_values == indices.shape().dim_size(0),\n-                errors::InvalidArgument(\n-                    \"Number of values must match first dimension of indices.\",\n-                    \"Got \", num_values,\n-                    \" values, indices shape: \", indices.shape().DebugString()));\n-\n     const auto indices_values = indices.matrix<int64_t>();\n     const auto values_values = values.flat<T>();\n     const auto weight_values = weights.flat<W>();\n@@ -225,16 +229,6 @@ class SparseCount : public OpKernel {\n \n     T max_value = 0;\n \n-    OP_REQUIRES(context, num_values <= indices.shape().dim_size(0),\n-                errors::InvalidArgument(\n-                    \"The first dimension of indices must be equal to or \"\n-                    \"greather than number of values. ( \",\n-                    indices.shape().dim_size(0), \" vs. \", num_values, \" )\"));\n-    OP_REQUIRES(context, indices.shape().dim_size(1) > 0,\n-                errors::InvalidArgument(\"The second dimension of indices must \"\n-                                        \"be greater than 0. Received: \",\n-                                        indices.shape().dim_size(1)));\n-\n     for (int idx = 0; idx < num_values; ++idx) {\n       int batch = is_1d ? 0 : indices_values(idx, 0);\n       if (batch >= num_batches) {\n"
        ],
        "Title": "\n          Heap overflow in `SparseCountSparseOutput`\n        "
    },
    {
        "Bug description": "The  implementation of   can be made to crash a TensorFlow process by an integer overflow whose result is then used in a memory allocation:",
        "Sample Code": "import numpy as np\n    \ntf.raw_ops.SparseCountSparseOutput(\n  indices=[[1,1]],\n  values=[2],\n  dense_shape=[2 ** 31, 2 ** 32],\n  weights=[1],\n  binary_output=True,\n  minlength=-1,\n  maxlength=-1,\n  ,\n  name=None)",
        "Bug fix": [
            "@@ -13,6 +13,8 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include <limits>\n+\n #include \"absl/container/flat_hash_map.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/op_requires.h\"\n@@ -23,6 +25,9 @@ limitations under the License.\n \n namespace tensorflow {\n \n+// Don't allocate too large `BatchedMap<T>` objects\n+static int kMaxBatches = std::numeric_limits<int>::max();\n+\n template <class T>\n using BatchedMap = std::vector<absl::flat_hash_map<int64_t, T>>;\n \n@@ -235,6 +240,10 @@ class SparseCount : public OpKernel {\n \n     bool is_1d = shape.NumElements() == 1;\n     int num_batches = is_1d ? 1 : shape_vector(0);\n+    OP_REQUIRES(\n+        context, 0 < num_batches && num_batches < kMaxBatches,\n+        errors::InvalidArgument(\"Cannot allocate \", num_batches,\n+                                \" batches, is the dense shape too wide?\"));\n \n     const auto values_values = values.flat<T>();\n     const auto weight_values = weights.flat<W>();\n"
        ],
        "Title": "\n          Integer overflow leading to crash in `SparseCountSparseOutput`\n        "
    },
    {
        "Bug description": "The  implementation of   has an undefined behavior where user controlled inputs can trigger a reference binding to null pointer.",
        "Sample Code": "tf.raw_ops.QuantizedMaxPool(\n    input = tf.constant([[[[4]]]], dtype=tf.quint8),\n    min_input = [],\n    max_input = [1],\n    ksize = [1, 1, 1, 1],\n    strides = [1, 1, 1, 1],\n    padding = \"SAME\", name=None\n)\n)",
        "Bug fix": [
            "@@ -15,6 +15,8 @@ limitations under the License.\n \n // See docs in ../ops/nn_ops.cc.\n \n+#include \"tensorflow/core/framework/op_requires.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n #define EIGEN_USE_THREADS\n \n #include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n@@ -117,6 +119,18 @@ class QuantizedMaxPoolingOp : public MaxPoolingOp<Device, T> {\n       : MaxPoolingOp<Device, T>(context) {}\n \n   void Compute(OpKernelContext* context) override {\n+    auto min_input_tensor = context->input(1);\n+    auto max_input_tensor = context->input(2);\n+    OP_REQUIRES(\n+        context, min_input_tensor.NumElements() == 1,\n+        errors::InvalidArgument(\n+            \"min_input must be a scalar float value, got tensor with shape \",\n+            min_input_tensor.shape()));\n+    OP_REQUIRES(\n+        context, max_input_tensor.NumElements() == 1,\n+        errors::InvalidArgument(\n+            \"max_input must be a scalar float value, got tensor with shape \",\n+            max_input_tensor.shape()));\n     const float min_input = context->input(1).flat<float>()(0);\n     const float max_input = context->input(2).flat<float>()(0);\n     MaxPoolingOp<Device, T>::Compute(context);\n"
        ],
        "Title": "\n          Reference binding to null pointer in `QuantizedMaxPool`\n        "
    },
    {
        "Bug description": "The  implementation of   allows malicious users to cause denial of service by passing in arguments which would trigger a  CHECK -fail:",
        "Sample Code": "tf.raw_ops.DenseBincount(\n  input=[[0], [1], [2]],\n  size=[1],\n  weights=[3,2,1],\n  ],\n  binary_output=False)",
        "Bug fix": [
            "@@ -276,6 +276,9 @@ class DenseBincountOp : public OpKernel {\n     const Tensor& size_t = ctx->input(1);\n     const Tensor& weights = ctx->input(2);\n \n+    OP_REQUIRES(ctx, size_t.dims() == 0,\n+                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n+                                        size_t.dims()));\n     Tidx size = size_t.scalar<Tidx>()();\n     OP_REQUIRES(\n         ctx, size >= 0,\n@@ -372,6 +375,9 @@ class SparseBincountOp : public OpKernel {\n     const auto weights = ctx->input(4).flat<T>();\n     const int64_t weights_size = weights.size();\n \n+    OP_REQUIRES(ctx, size_t.dims() == 0,\n+                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n+                                        size_t.dims()));\n     Tidx size = size_t.scalar<Tidx>()();\n     OP_REQUIRES(\n         ctx, size >= 0,\n@@ -462,6 +468,9 @@ class RaggedBincountOp : public OpKernel {\n     const auto weights = ctx->input(3).flat<T>();\n     const int64_t weights_size = weights.size();\n \n+    OP_REQUIRES(ctx, size_t.dims() == 0,\n+                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n+                                        size_t.dims()));\n     Tidx size = size_t.scalar<Tidx>()();\n     OP_REQUIRES(\n         ctx, size >= 0,\n",
            "@@ -1699,6 +1699,11 @@ REGISTER_OP(\"Bincount\")\n         return Status::OK();\n       }\n \n+      if (size_tensor->dims() != 0) {\n+        return errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n+                                       size_tensor->dims());\n+      }\n+\n       // Return `[size]` shape if size is known.\n       int32_t size_val = size_tensor->scalar<int32>()();\n       if (size_val < 0) {\n@@ -1730,6 +1735,10 @@ REGISTER_OP(\"DenseBincount\")\n         c->set_output(0, c->UnknownShape());\n         return Status::OK();\n       }\n+      if (size_tensor->dims() != 0) {\n+        return errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n+                                       size_tensor->dims());\n+      }\n \n       int64_t size_val;\n       DataType dtype;\n@@ -1771,6 +1780,10 @@ REGISTER_OP(\"SparseBincount\")\n         c->set_output(0, c->UnknownShape());\n         return Status::OK();\n       }\n+      if (size_tensor->dims() != 0) {\n+        return errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n+                                       size_tensor->dims());\n+      }\n \n       int64_t size_val;\n       DataType dtype;\n",
            "@@ -344,6 +344,14 @@ class BincountOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n             gen_math_ops.dense_bincount(\n                 input=[[[1, 2, 3], [0, 3, 2]]], weights=[], size=10))\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def test_size_is_not_scalar(self):  # b/206619828\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Shape must be rank 0 but is rank 1\"):\n+      self.evaluate(\n+          gen_math_ops.dense_bincount(\n+              input=[0], size=[1, 1], weights=[3], binary_output=False))\n+\n \n class SparseBincountOpTest(test_util.TensorFlowTestCase,\n                            parameterized.TestCase):\n@@ -511,6 +519,19 @@ class SparseBincountOpTest(test_util.TensorFlowTestCase,\n                 weights=[],\n                 binary_output=True)))\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def test_size_is_not_scalar(self):  # b/206619828\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Shape must be rank 0 but is rank 1\"):\n+      self.evaluate(\n+          gen_math_ops.sparse_bincount(\n+              indices=[[0], [1]],\n+              values=[0, 0],\n+              dense_shape=[1, 1],\n+              size=[1, 1],\n+              weights=[0, 0],\n+              binary_output=False))\n+\n \n class RaggedBincountOpTest(test_util.TensorFlowTestCase,\n                            parameterized.TestCase):\n@@ -650,6 +671,19 @@ class RaggedBincountOpTest(test_util.TensorFlowTestCase,\n                 size=size,\n                 binary_output=True)))\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def test_size_is_not_scalar(self):  # b/206619828\n+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n+                                \"Shape must be rank 0 but is rank 1\"):\n+      self.evaluate(\n+          gen_math_ops.ragged_bincount(\n+              splits=[0, 0, 1],\n+              values=[1],\n+              size=[1, 1],\n+              weights=[0, 0, 0],\n+              binary_output=False,\n+              name=None))\n+\n \n if __name__ == \"__main__\":\n   googletest.main()\n"
        ],
        "Title": "\n          Assertion failure based denial of service via faulty bin count operations\n        "
    },
    {
        "Bug description": "The  implementation of   has an undefined behavior: under certain condition it can be made to dereference a  nullptr  value:",
        "Sample Code": "import numpy as np\n\ntf.raw_ops.SparseTensorSliceDataset(\n  indices=[[]],\n  values=[],\n  [],\n  dense_shape=[1,1])",
        "Bug fix": [
            "@@ -240,28 +240,29 @@ class SparseTensorSliceDatasetOp : public DatasetOpKernel {\n     OP_REQUIRES_OK(ctx, ctx->input(\"dense_shape\", &dense_shape));\n \n     OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(indices->shape()),\n-                errors::InvalidArgument(\n-                    \"Input indices should be a matrix but received shape \",\n-                    indices->shape().DebugString()));\n-\n-    const auto num_indices = indices->NumElements();\n-    const auto num_values = values->NumElements();\n-    if (num_indices == 0 || num_values == 0) {\n-      OP_REQUIRES(ctx, num_indices == num_values,\n-                  errors::InvalidArgument(\n-                      \"If indices or values are empty, the other one must also \"\n-                      \"be. Got indices of shape \",\n-                      indices->shape().DebugString(), \" and values of shape \",\n-                      values->shape().DebugString()));\n-    }\n+                errors::InvalidArgument(\"Input indices must be a matrix. Got: \",\n+                                        indices->shape().DebugString()));\n     OP_REQUIRES(ctx, TensorShapeUtils::IsVector(values->shape()),\n-                errors::InvalidArgument(\n-                    \"Input values should be a vector but received shape \",\n-                    indices->shape().DebugString()));\n+                errors::InvalidArgument(\"Input values must be a vector. Got: \",\n+                                        values->shape().DebugString()));\n     OP_REQUIRES(ctx, TensorShapeUtils::IsVector(dense_shape->shape()),\n+                errors::InvalidArgument(\"Input shape must be a vector. Got: \",\n+                                        dense_shape->shape().DebugString()));\n+    OP_REQUIRES(\n+        ctx, values->shape().dim_size(0) == indices->shape().dim_size(0),\n+        errors::InvalidArgument(\n+            \"Number of values must match first dimension of indices. \", \"Got \",\n+            values->shape().dim_size(0),\n+            \" values, indices shape: \", indices->shape().DebugString()));\n+    OP_REQUIRES(\n+        ctx, dense_shape->shape().dim_size(0) == indices->shape().dim_size(1),\n+        errors::InvalidArgument(\n+            \"Number of dimensions must match second dimension of indices. \",\n+            \"Got \", dense_shape->shape().dim_size(0),\n+            \" dimensions, indices shape: \", indices->shape().DebugString()));\n+    OP_REQUIRES(ctx, dense_shape->NumElements() > 0,\n                 errors::InvalidArgument(\n-                    \"Input shape should be a vector but received shape \",\n-                    dense_shape->shape().DebugString()));\n+                    \"The shape argument requires at least one element.\"));\n \n     // We currently ensure that `sparse_tensor` is ordered in the\n     // batch dimension.\n",
            "@@ -134,6 +134,25 @@ class FromSparseTensorSlicesTest(test_base.DatasetTestBase,\n       with self.assertRaises(errors.InvalidArgumentError):\n         sess.run(init_op, feed_dict={st: sparse_feed})\n \n+  @combinations.generate(combinations.combine(tf_api_version=1, mode=[\"graph\"]))\n+  def testEmptySparseTensorSlicesInvalid2(self):\n+    \"\"\"Test a dataset based on invalid `tf.sparse.SparseTensor`.\"\"\"\n+    st = array_ops.sparse_placeholder(dtypes.float64)\n+    iterator = dataset_ops.make_initializable_iterator(\n+        dataset_ops.Dataset.from_sparse_tensor_slices(st))\n+    init_op = iterator.initializer\n+\n+    with self.cached_session() as sess:\n+      # Test with an empty sparse tensor but with non empty values.\n+      empty_indices = [[]]\n+      empty_values = []\n+      dense_shape = [1, 1]\n+      sparse_feed = sparse_tensor.SparseTensorValue(empty_indices, empty_values,\n+                                                    dense_shape)\n+      # Here, we expect the test to fail when running the feed.\n+      with self.assertRaises(errors.InvalidArgumentError):\n+        sess.run(init_op, feed_dict={st: sparse_feed})\n+\n   @combinations.generate(combinations.combine(tf_api_version=2, mode=[\"eager\"]))\n   def testFromSparseTensorSlicesError(self):\n     with self.assertRaises(AttributeError):\n"
        ],
        "Title": "\n          Undefined behavior in `SparseTensorSliceDataset`\n        "
    },
    {
        "Bug description": "Multiple operations in TensorFlow can be used to trigger a denial of service via  CHECK -fails (i.e., assertion failures). This is similar to  TFSA-2021-198  ( CVE-2021-41197 ) and has similar fixes.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          `CHECK`-fails when building invalid/overflowing tensor shapes\n        "
    },
    {
        "Bug description": "The  implementation of   can be made to crash a TensorFlow process via a division by 0:",
        "Sample Code": "import numpy as np\n\ntf.raw_ops.FractionalMaxPool(\n  value=tf.constant(value=[[[[1, 4, 2, 3]]]], dtype=tf.int64),\n  pooling_ratio=[1.0, 1.44, 1.73, 1.0],\n  pseudo_random=False,\n  overlapping=False,\n  deterministic=False,\n  seed=0,\n  seed2=0,\n  ,\n  name=None)",
        "Bug fix": [
            "@@ -83,6 +83,13 @@ class FractionalMaxPoolOp : public OpKernel {\n     std::vector<int> output_size(tensor_in_and_out_dims);\n     for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n       input_size[i] = tensor_in.dim_size(i);\n+\n+      OP_REQUIRES(\n+          context, input_size[i] >= pooling_ratio_[i],\n+          errors::InvalidArgument(\"Pooling ratio is higher than input \"\n+                                  \"dimension size for dimension \",\n+                                  i, \". Input dim size: \", input_size[i],\n+                                  \" pooling ratio: \", pooling_ratio_[i]));\n     }\n     // Output size.\n     for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n",
            "@@ -20,6 +20,7 @@ import numpy as np\n \n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import gen_nn_ops\n@@ -319,6 +320,24 @@ class FractionalMaxPoolTest(test.TestCase):\n       nn_ops.fractional_max_pool(\n           rand_mat, [1, 1.5, 1.5, 1], seed=1, seed2=1, deterministic=True)\n \n+  def testPoolingRatio(self):\n+    with self.cached_session() as _:\n+      with self.assertRaisesRegex(\n+          errors.InvalidArgumentError,\n+          r\"Pooling ratio is higher than input dimension size for dimension 1.*\"\n+      ):\n+        result = nn_ops.gen_nn_ops.fractional_max_pool(\n+            value=constant_op.constant(\n+                value=[[[[1, 4, 2, 3]]]], dtype=dtypes.int64),\n+            pooling_ratio=[1.0, 1.44, 1.73, 1.0],\n+            pseudo_random=False,\n+            overlapping=False,\n+            deterministic=False,\n+            seed=0,\n+            seed2=0,\n+            name=None)\n+        self.evaluate(result)\n+\n \n class FractionalMaxPoolGradTest(test.TestCase):\n   \"\"\"Tests for FractionalMaxPoolGrad.\n"
        ],
        "Title": "\n          Division by zero in `FractionalMaxPool`\n        "
    },
    {
        "Bug description": "The  implementation of   is vulnerable a  CHECK -fail if the key tensor is not a scalar:",
        "Sample Code": "import numpy as np\n\ntf.raw_ops.MapStage(\n    key = tf.constant(value=[4], shape= (1,2), dtype=tf.int64),\n    indices = np.array([[6]]),\n    values = np.array([-60]),\n    dtypes = [tf.int64], capacity=0, memory_limit=0,\n    container='', shared_name='', name=None\n)   \n)   ",
        "Bug fix": [
            "@@ -536,6 +536,11 @@ class MapStageOp : public OpKernel {\n     OP_REQUIRES(ctx, key_tensor->NumElements() > 0,\n                 errors::InvalidArgument(\"key must not be empty\"));\n \n+    OP_REQUIRES(ctx, key_tensor->NumElements() == 1,\n+                errors::InvalidArgument(\n+                    \"key must be an int64 scalar, got tensor with shape: \",\n+                    key_tensor->shape()));\n+\n     // Create copy for insertion into Staging Area\n     Tensor key(*key_tensor);\n \n",
            "@@ -12,8 +12,11 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-from tensorflow.python.framework import errors\n+import numpy as np\n+\n+from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n@@ -28,7 +31,7 @@ class MapStageTest(test.TestCase):\n \n   @test_util.run_deprecated_v1\n   def testSimple(self):\n-    with ops.Graph().as_default() as G:\n+    with ops.Graph().as_default() as g:\n       with ops.device('/cpu:0'):\n         x = array_ops.placeholder(dtypes.float32)\n         pi = array_ops.placeholder(dtypes.int64)\n@@ -40,9 +43,9 @@ class MapStageTest(test.TestCase):\n         k, y = stager.get(gi)\n         y = math_ops.reduce_max(math_ops.matmul(y, y))\n \n-    G.finalize()\n+    g.finalize()\n \n-    with self.session(graph=G) as sess:\n+    with self.session(graph=g) as sess:\n       sess.run(stage, feed_dict={x: -1, pi: 0})\n       for i in range(10):\n         _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})\n@@ -50,7 +53,7 @@ class MapStageTest(test.TestCase):\n \n   @test_util.run_deprecated_v1\n   def testMultiple(self):\n-    with ops.Graph().as_default() as G:\n+    with ops.Graph().as_default() as g:\n       with ops.device('/cpu:0'):\n         x = array_ops.placeholder(dtypes.float32)\n         pi = array_ops.placeholder(dtypes.int64)\n@@ -62,9 +65,9 @@ class MapStageTest(test.TestCase):\n         k, (z, y) = stager.get(gi)\n         y = math_ops.reduce_max(z * math_ops.matmul(y, y))\n \n-    G.finalize()\n+    g.finalize()\n \n-    with self.session(graph=G) as sess:\n+    with self.session(graph=g) as sess:\n       sess.run(stage, feed_dict={x: -1, pi: 0})\n       for i in range(10):\n         _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})\n@@ -73,26 +76,25 @@ class MapStageTest(test.TestCase):\n \n   @test_util.run_deprecated_v1\n   def testDictionary(self):\n-    with ops.Graph().as_default() as G:\n+    with ops.Graph().as_default() as g:\n       with ops.device('/cpu:0'):\n         x = array_ops.placeholder(dtypes.float32)\n         pi = array_ops.placeholder(dtypes.int64)\n         gi = array_ops.placeholder(dtypes.int64)\n         v = 2. * (array_ops.zeros([128, 128]) + x)\n       with ops.device(test.gpu_device_name()):\n-        stager = data_flow_ops.MapStagingArea(\n-            [dtypes.float32, dtypes.float32],\n-            shapes=[[], [128, 128]],\n-            names=['x', 'v'])\n+        stager = data_flow_ops.MapStagingArea([dtypes.float32, dtypes.float32],\n+                                              shapes=[[], [128, 128]],\n+                                              names=['x', 'v'])\n         stage = stager.put(pi, {'x': x, 'v': v})\n         key, ret = stager.get(gi)\n         z = ret['x']\n         y = ret['v']\n         y = math_ops.reduce_max(z * math_ops.matmul(y, y))\n \n-    G.finalize()\n+    g.finalize()\n \n-    with self.session(graph=G) as sess:\n+    with self.session(graph=g) as sess:\n       sess.run(stage, feed_dict={x: -1, pi: 0})\n       for i in range(10):\n         _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})\n@@ -102,7 +104,7 @@ class MapStageTest(test.TestCase):\n   def testColocation(self):\n     gpu_dev = test.gpu_device_name()\n \n-    with ops.Graph().as_default() as G:\n+    with ops.Graph().as_default() as g:\n       with ops.device('/cpu:0'):\n         x = array_ops.placeholder(dtypes.float32)\n         v = 2. * (array_ops.zeros([128, 128]) + x)\n@@ -119,58 +121,56 @@ class MapStageTest(test.TestCase):\n         self.assertEqual(y.device, '/device:CPU:0')\n         self.assertEqual(z[0].device, '/device:CPU:0')\n \n-    G.finalize()\n+    g.finalize()\n \n   @test_util.run_deprecated_v1\n   def testPeek(self):\n-    with ops.Graph().as_default() as G:\n+    with ops.Graph().as_default() as g:\n       with ops.device('/cpu:0'):\n         x = array_ops.placeholder(dtypes.int32, name='x')\n         pi = array_ops.placeholder(dtypes.int64)\n         gi = array_ops.placeholder(dtypes.int64)\n         p = array_ops.placeholder(dtypes.int32, name='p')\n       with ops.device(test.gpu_device_name()):\n-        stager = data_flow_ops.MapStagingArea(\n-            [\n-                dtypes.int32,\n-            ], shapes=[[]])\n+        stager = data_flow_ops.MapStagingArea([\n+            dtypes.int32,\n+        ], shapes=[[]])\n         stage = stager.put(pi, [x], [0])\n         peek = stager.peek(gi)\n         size = stager.size()\n \n-    G.finalize()\n+    g.finalize()\n \n     n = 10\n \n-    with self.session(graph=G) as sess:\n+    with self.session(graph=g) as sess:\n       for i in range(n):\n         sess.run(stage, feed_dict={x: i, pi: i})\n \n       for i in range(n):\n-        self.assertTrue(sess.run(peek, feed_dict={gi: i})[0] == i)\n+        self.assertEqual(sess.run(peek, feed_dict={gi: i})[0], i)\n \n-      self.assertTrue(sess.run(size) == 10)\n+      self.assertEqual(sess.run(size), 10)\n \n   @test_util.run_deprecated_v1\n   def testSizeAndClear(self):\n-    with ops.Graph().as_default() as G:\n+    with ops.Graph().as_default() as g:\n       with ops.device('/cpu:0'):\n         x = array_ops.placeholder(dtypes.float32, name='x')\n         pi = array_ops.placeholder(dtypes.int64)\n         gi = array_ops.placeholder(dtypes.int64)\n         v = 2. * (array_ops.zeros([128, 128]) + x)\n       with ops.device(test.gpu_device_name()):\n-        stager = data_flow_ops.MapStagingArea(\n-            [dtypes.float32, dtypes.float32],\n-            shapes=[[], [128, 128]],\n-            names=['x', 'v'])\n+        stager = data_flow_ops.MapStagingArea([dtypes.float32, dtypes.float32],\n+                                              shapes=[[], [128, 128]],\n+                                              names=['x', 'v'])\n         stage = stager.put(pi, {'x': x, 'v': v})\n         size = stager.size()\n         clear = stager.clear()\n \n-    G.finalize()\n+    g.finalize()\n \n-    with self.session(graph=G) as sess:\n+    with self.session(graph=g) as sess:\n       sess.run(stage, feed_dict={x: -1, pi: 3})\n       self.assertEqual(sess.run(size), 1)\n       sess.run(stage, feed_dict={x: -1, pi: 1})\n@@ -182,22 +182,23 @@ class MapStageTest(test.TestCase):\n   def testCapacity(self):\n     capacity = 3\n \n-    with ops.Graph().as_default() as G:\n+    with ops.Graph().as_default() as g:\n       with ops.device('/cpu:0'):\n         x = array_ops.placeholder(dtypes.int32, name='x')\n         pi = array_ops.placeholder(dtypes.int64, name='pi')\n         gi = array_ops.placeholder(dtypes.int64, name='gi')\n       with ops.device(test.gpu_device_name()):\n-        stager = data_flow_ops.MapStagingArea(\n-            [\n-                dtypes.int32,\n-            ], capacity=capacity, shapes=[[]])\n+        stager = data_flow_ops.MapStagingArea([\n+            dtypes.int32,\n+        ],\n+                                              capacity=capacity,\n+                                              shapes=[[]])\n \n       stage = stager.put(pi, [x], [0])\n       get = stager.get()\n       size = stager.size()\n \n-    G.finalize()\n+    g.finalize()\n \n     from six.moves import queue as Queue\n     import threading\n@@ -205,7 +206,7 @@ class MapStageTest(test.TestCase):\n     queue = Queue.Queue()\n     n = 8\n \n-    with self.session(graph=G) as sess:\n+    with self.session(graph=g) as sess:\n       # Stage data in a separate thread which will block\n       # when it hits the staging area's capacity and thus\n       # not fill the queue with n tokens\n@@ -234,13 +235,13 @@ class MapStageTest(test.TestCase):\n                                              capacity))\n \n       # Should have capacity elements in the staging area\n-      self.assertTrue(sess.run(size) == capacity)\n+      self.assertEqual(sess.run(size), capacity)\n \n       # Clear the staging area completely\n       for i in range(n):\n         sess.run(get)\n \n-      self.assertTrue(sess.run(size) == 0)\n+      self.assertEqual(sess.run(size), 0)\n \n   @test_util.run_deprecated_v1\n   def testMemoryLimit(self):\n@@ -248,28 +249,28 @@ class MapStageTest(test.TestCase):\n     chunk = 200 * 1024  # 256K\n     capacity = memory_limit // chunk\n \n-    with ops.Graph().as_default() as G:\n+    with ops.Graph().as_default() as g:\n       with ops.device('/cpu:0'):\n         x = array_ops.placeholder(dtypes.uint8, name='x')\n         pi = array_ops.placeholder(dtypes.int64, name='pi')\n         gi = array_ops.placeholder(dtypes.int64, name='gi')\n       with ops.device(test.gpu_device_name()):\n-        stager = data_flow_ops.MapStagingArea(\n-            [dtypes.uint8], memory_limit=memory_limit, shapes=[[]])\n+        stager = data_flow_ops.MapStagingArea([dtypes.uint8],\n+                                              memory_limit=memory_limit,\n+                                              shapes=[[]])\n         stage = stager.put(pi, [x], [0])\n         get = stager.get()\n         size = stager.size()\n \n-    G.finalize()\n+    g.finalize()\n \n     from six.moves import queue as Queue\n     import threading\n-    import numpy as np\n \n     queue = Queue.Queue()\n     n = 8\n \n-    with self.session(graph=G) as sess:\n+    with self.session(graph=g) as sess:\n       # Stage data in a separate thread which will block\n       # when it hits the staging area's capacity and thus\n       # not fill the queue with n tokens\n@@ -299,56 +300,57 @@ class MapStageTest(test.TestCase):\n                                              capacity))\n \n       # Should have capacity elements in the staging area\n-      self.assertTrue(sess.run(size) == capacity)\n+      self.assertEqual(sess.run(size), capacity)\n \n       # Clear the staging area completely\n       for i in range(n):\n         sess.run(get)\n \n-      self.assertTrue(sess.run(size) == 0)\n+      self.assertEqual(sess.run(size), 0)\n \n   @test_util.run_deprecated_v1\n   def testOrdering(self):\n     import six\n     import random\n \n-    with ops.Graph().as_default() as G:\n+    with ops.Graph().as_default() as g:\n       with ops.device('/cpu:0'):\n         x = array_ops.placeholder(dtypes.int32, name='x')\n         pi = array_ops.placeholder(dtypes.int64, name='pi')\n         gi = array_ops.placeholder(dtypes.int64, name='gi')\n       with ops.device(test.gpu_device_name()):\n-        stager = data_flow_ops.MapStagingArea(\n-            [\n-                dtypes.int32,\n-            ], shapes=[[]], ordered=True)\n+        stager = data_flow_ops.MapStagingArea([\n+            dtypes.int32,\n+        ],\n+                                              shapes=[[]],\n+                                              ordered=True)\n         stage = stager.put(pi, [x], [0])\n         get = stager.get()\n         size = stager.size()\n \n-    G.finalize()\n+    g.finalize()\n \n     n = 10\n \n-    with self.session(graph=G) as sess:\n+    with self.session(graph=g) as sess:\n       # Keys n-1..0\n       keys = list(reversed(six.moves.range(n)))\n \n       for i in keys:\n         sess.run(stage, feed_dict={pi: i, x: i})\n \n-      self.assertTrue(sess.run(size) == n)\n+      self.assertEqual(sess.run(size), n)\n \n       # Check that key, values come out in ascending order\n       for i, k in enumerate(reversed(keys)):\n         get_key, values = sess.run(get)\n         self.assertTrue(i == k == get_key == values)\n \n-      self.assertTrue(sess.run(size) == 0)\n+      self.assertEqual(sess.run(size), 0)\n \n   @test_util.run_deprecated_v1\n   def testPartialDictInsert(self):\n-    with ops.Graph().as_default() as G:\n+    with ops.Graph().as_default() as g:\n       with ops.device('/cpu:0'):\n         x = array_ops.placeholder(dtypes.float32)\n         f = array_ops.placeholder(dtypes.float32)\n@@ -366,41 +368,39 @@ class MapStageTest(test.TestCase):\n         size = stager.size()\n         isize = stager.incomplete_size()\n \n-    G.finalize()\n+    g.finalize()\n \n-    with self.session(graph=G) as sess:\n+    with self.session(graph=g) as sess:\n       # 0 complete and incomplete entries\n-      self.assertTrue(sess.run([size, isize]) == [0, 0])\n+      self.assertEqual(sess.run([size, isize]), [0, 0])\n       # Stage key 0, x and f tuple entries\n       sess.run(stage_xf, feed_dict={pi: 0, x: 1, f: 2})\n-      self.assertTrue(sess.run([size, isize]) == [0, 1])\n+      self.assertEqual(sess.run([size, isize]), [0, 1])\n       # Stage key 1, x and f tuple entries\n       sess.run(stage_xf, feed_dict={pi: 1, x: 1, f: 2})\n-      self.assertTrue(sess.run([size, isize]) == [0, 2])\n+      self.assertEqual(sess.run([size, isize]), [0, 2])\n \n       # Now complete key 0 with tuple entry v\n       sess.run(stage_v, feed_dict={pi: 0, v: 1})\n       # 1 complete and 1 incomplete entry\n-      self.assertTrue(sess.run([size, isize]) == [1, 1])\n+      self.assertEqual(sess.run([size, isize]), [1, 1])\n       # We can now obtain tuple associated with key 0\n-      self.assertTrue(\n-          sess.run([key, ret], feed_dict={\n-              gi: 0\n-          }) == [0, {\n+      self.assertEqual(\n+          sess.run([key, ret], feed_dict={gi: 0}),\n+          [0, {\n               'x': 1,\n               'f': 2,\n               'v': 1\n           }])\n \n       # 0 complete and 1 incomplete entry\n-      self.assertTrue(sess.run([size, isize]) == [0, 1])\n+      self.assertEqual(sess.run([size, isize]), [0, 1])\n       # Now complete key 1 with tuple entry v\n       sess.run(stage_v, feed_dict={pi: 1, v: 3})\n       # We can now obtain tuple associated with key 1\n-      self.assertTrue(\n-          sess.run([key, ret], feed_dict={\n-              gi: 1\n-          }) == [1, {\n+      self.assertEqual(\n+          sess.run([key, ret], feed_dict={gi: 1}),\n+          [1, {\n               'x': 1,\n               'f': 2,\n               'v': 3\n@@ -408,7 +408,7 @@ class MapStageTest(test.TestCase):\n \n   @test_util.run_deprecated_v1\n   def testPartialIndexInsert(self):\n-    with ops.Graph().as_default() as G:\n+    with ops.Graph().as_default() as g:\n       with ops.device('/cpu:0'):\n         x = array_ops.placeholder(dtypes.float32)\n         f = array_ops.placeholder(dtypes.float32)\n@@ -424,35 +424,35 @@ class MapStageTest(test.TestCase):\n         size = stager.size()\n         isize = stager.incomplete_size()\n \n-    G.finalize()\n+    g.finalize()\n \n-    with self.session(graph=G) as sess:\n+    with self.session(graph=g) as sess:\n       # 0 complete and incomplete entries\n-      self.assertTrue(sess.run([size, isize]) == [0, 0])\n+      self.assertEqual(sess.run([size, isize]), [0, 0])\n       # Stage key 0, x and f tuple entries\n       sess.run(stage_xf, feed_dict={pi: 0, x: 1, f: 2})\n-      self.assertTrue(sess.run([size, isize]) == [0, 1])\n+      self.assertEqual(sess.run([size, isize]), [0, 1])\n       # Stage key 1, x and f tuple entries\n       sess.run(stage_xf, feed_dict={pi: 1, x: 1, f: 2})\n-      self.assertTrue(sess.run([size, isize]) == [0, 2])\n+      self.assertEqual(sess.run([size, isize]), [0, 2])\n \n       # Now complete key 0 with tuple entry v\n       sess.run(stage_v, feed_dict={pi: 0, v: 1})\n       # 1 complete and 1 incomplete entry\n-      self.assertTrue(sess.run([size, isize]) == [1, 1])\n+      self.assertEqual(sess.run([size, isize]), [1, 1])\n       # We can now obtain tuple associated with key 0\n-      self.assertTrue(sess.run([key, ret], feed_dict={gi: 0}) == [0, [1, 1, 2]])\n+      self.assertEqual(sess.run([key, ret], feed_dict={gi: 0}), [0, [1, 1, 2]])\n \n       # 0 complete and 1 incomplete entry\n-      self.assertTrue(sess.run([size, isize]) == [0, 1])\n+      self.assertEqual(sess.run([size, isize]), [0, 1])\n       # Now complete key 1 with tuple entry v\n       sess.run(stage_v, feed_dict={pi: 1, v: 3})\n       # We can now obtain tuple associated with key 1\n-      self.assertTrue(sess.run([key, ret], feed_dict={gi: 1}) == [1, [1, 3, 2]])\n+      self.assertEqual(sess.run([key, ret], feed_dict={gi: 1}), [1, [1, 3, 2]])\n \n   @test_util.run_deprecated_v1\n   def testPartialDictGetsAndPeeks(self):\n-    with ops.Graph().as_default() as G:\n+    with ops.Graph().as_default() as g:\n       with ops.device('/cpu:0'):\n         x = array_ops.placeholder(dtypes.float32)\n         f = array_ops.placeholder(dtypes.float32)\n@@ -476,40 +476,38 @@ class MapStageTest(test.TestCase):\n         size = stager.size()\n         isize = stager.incomplete_size()\n \n-    G.finalize()\n+    g.finalize()\n \n-    with self.session(graph=G) as sess:\n+    with self.session(graph=g) as sess:\n       # 0 complete and incomplete entries\n-      self.assertTrue(sess.run([size, isize]) == [0, 0])\n+      self.assertEqual(sess.run([size, isize]), [0, 0])\n       # Stage key 0, x and f tuple entries\n       sess.run(stage_xf, feed_dict={pi: 0, x: 1, f: 2})\n-      self.assertTrue(sess.run([size, isize]) == [0, 1])\n+      self.assertEqual(sess.run([size, isize]), [0, 1])\n       # Stage key 1, x and f tuple entries\n       sess.run(stage_xf, feed_dict={pi: 1, x: 1, f: 2})\n-      self.assertTrue(sess.run([size, isize]) == [0, 2])\n+      self.assertEqual(sess.run([size, isize]), [0, 2])\n \n       # Now complete key 0 with tuple entry v\n       sess.run(stage_v, feed_dict={pi: 0, v: 1})\n       # 1 complete and 1 incomplete entry\n-      self.assertTrue(sess.run([size, isize]) == [1, 1])\n+      self.assertEqual(sess.run([size, isize]), [1, 1])\n \n       # We can now peek at 'x' and 'f' values associated with key 0\n-      self.assertTrue(sess.run(peek_xf, feed_dict={pei: 0}) == {'x': 1, 'f': 2})\n+      self.assertEqual(sess.run(peek_xf, feed_dict={pei: 0}), {'x': 1, 'f': 2})\n       # Peek at 'v' value associated with key 0\n-      self.assertTrue(sess.run(peek_v, feed_dict={pei: 0}) == {'v': 1})\n+      self.assertEqual(sess.run(peek_v, feed_dict={pei: 0}), {'v': 1})\n       # 1 complete and 1 incomplete entry\n-      self.assertTrue(sess.run([size, isize]) == [1, 1])\n+      self.assertEqual(sess.run([size, isize]), [1, 1])\n \n       # We can now obtain 'x' and 'f' values associated with key 0\n-      self.assertTrue(\n-          sess.run([key_xf, get_xf], feed_dict={\n-              gi: 0\n-          }) == [0, {\n+      self.assertEqual(\n+          sess.run([key_xf, get_xf], feed_dict={gi: 0}), [0, {\n               'x': 1,\n               'f': 2\n           }])\n       # Still have 1 complete and 1 incomplete entry\n-      self.assertTrue(sess.run([size, isize]) == [1, 1])\n+      self.assertEqual(sess.run([size, isize]), [1, 1])\n \n       # We can no longer get 'x' and 'f' from key 0\n       with self.assertRaises(errors.InvalidArgumentError) as cm:\n@@ -517,40 +515,36 @@ class MapStageTest(test.TestCase):\n \n       exc_str = (\"Tensor at index '0' for key '0' \" 'has already been removed.')\n \n-      self.assertTrue(exc_str in cm.exception.message)\n+      self.assertIn(exc_str, cm.exception.message)\n \n       # Obtain 'v' value associated with key 0\n-      self.assertTrue(\n-          sess.run([key_v, get_v], feed_dict={\n-              gi: 0\n-          }) == [0, {\n+      self.assertEqual(\n+          sess.run([key_v, get_v], feed_dict={gi: 0}), [0, {\n               'v': 1\n           }])\n       # 0 complete and 1 incomplete entry\n-      self.assertTrue(sess.run([size, isize]) == [0, 1])\n+      self.assertEqual(sess.run([size, isize]), [0, 1])\n \n       # Now complete key 1 with tuple entry v\n       sess.run(stage_v, feed_dict={pi: 1, v: 1})\n       # 1 complete and 1 incomplete entry\n-      self.assertTrue(sess.run([size, isize]) == [1, 0])\n+      self.assertEqual(sess.run([size, isize]), [1, 0])\n \n       # Pop without key to obtain 'x' and 'f' values associated with key 1\n-      self.assertTrue(sess.run([pop_key_xf, pop_xf]) == [1, {'x': 1, 'f': 2}])\n+      self.assertEqual(sess.run([pop_key_xf, pop_xf]), [1, {'x': 1, 'f': 2}])\n       # still 1 complete and 1 incomplete entry\n-      self.assertTrue(sess.run([size, isize]) == [1, 0])\n+      self.assertEqual(sess.run([size, isize]), [1, 0])\n       # We can now obtain 'x' and 'f' values associated with key 1\n-      self.assertTrue(\n-          sess.run([pop_key_v, pop_v], feed_dict={\n-              pi: 1\n-          }) == [1, {\n+      self.assertEqual(\n+          sess.run([pop_key_v, pop_v], feed_dict={pi: 1}), [1, {\n               'v': 1\n           }])\n       # Nothing is left\n-      self.assertTrue(sess.run([size, isize]) == [0, 0])\n+      self.assertEqual(sess.run([size, isize]), [0, 0])\n \n   @test_util.run_deprecated_v1\n   def testPartialIndexGets(self):\n-    with ops.Graph().as_default() as G:\n+    with ops.Graph().as_default() as g:\n       with ops.device('/cpu:0'):\n         x = array_ops.placeholder(dtypes.float32)\n         f = array_ops.placeholder(dtypes.float32)\n@@ -568,28 +562,72 @@ class MapStageTest(test.TestCase):\n         size = stager.size()\n         isize = stager.incomplete_size()\n \n-    G.finalize()\n+    g.finalize()\n \n-    with self.session(graph=G) as sess:\n+    with self.session(graph=g) as sess:\n       # Stage complete tuple\n       sess.run(stage_xvf, feed_dict={pi: 0, x: 1, f: 2, v: 3})\n \n-      self.assertTrue(sess.run([size, isize]) == [1, 0])\n+      self.assertEqual(sess.run([size, isize]), [1, 0])\n \n       # Partial get using indices\n-      self.assertTrue(\n-          sess.run([key_xf, get_xf], feed_dict={\n-              gi: 0\n-          }) == [0, [1, 2]])\n+      self.assertEqual(\n+          sess.run([key_xf, get_xf], feed_dict={gi: 0}), [0, [1, 2]])\n \n       # Still some of key 0 left\n-      self.assertTrue(sess.run([size, isize]) == [1, 0])\n+      self.assertEqual(sess.run([size, isize]), [1, 0])\n \n       # Partial get of remaining index\n-      self.assertTrue(sess.run([key_v, get_v], feed_dict={gi: 0}) == [0, [3]])\n+      self.assertEqual(sess.run([key_v, get_v], feed_dict={gi: 0}), [0, [3]])\n \n       # All gone\n-      self.assertTrue(sess.run([size, isize]) == [0, 0])\n+      self.assertEqual(sess.run([size, isize]), [0, 0])\n+\n+  @test_util.run_deprecated_v1\n+  def testNonScalarKeyOrderedMap(self):\n+    with ops.Graph().as_default() as g:\n+      x = array_ops.placeholder(dtypes.float32)\n+      v = 2. * (array_ops.zeros([128, 128]) + x)\n+      t = data_flow_ops.gen_data_flow_ops.ordered_map_stage(\n+          key=constant_op.constant(value=[1], shape=(1, 3), dtype=dtypes.int64),\n+          indices=np.array([[6]]),\n+          values=[x, v],\n+          dtypes=[dtypes.int64],\n+          capacity=0,\n+          memory_limit=0,\n+          container='container1',\n+          shared_name='',\n+          name=None)\n+\n+    g.finalize()\n+\n+    with self.session(graph=g) as sess:\n+      with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                  'key must be an int64 scalar'):\n+        sess.run(t, feed_dict={x: 1})\n+\n+  @test_util.run_deprecated_v1\n+  def testNonScalarKeyUnorderedMap(self):\n+    with ops.Graph().as_default() as g:\n+      x = array_ops.placeholder(dtypes.float32)\n+      v = 2. * (array_ops.zeros([128, 128]) + x)\n+      t = data_flow_ops.gen_data_flow_ops.map_stage(\n+          key=constant_op.constant(value=[1], shape=(1, 3), dtype=dtypes.int64),\n+          indices=np.array([[6]]),\n+          values=[x, v],\n+          dtypes=[dtypes.int64],\n+          capacity=0,\n+          memory_limit=0,\n+          container='container1',\n+          shared_name='',\n+          name=None)\n+\n+    g.finalize()\n+\n+    with self.session(graph=g) as sess:\n+      with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                  'key must be an int64 scalar'):\n+        sess.run(t, feed_dict={x: 1})\n \n \n if __name__ == '__main__':\n"
        ],
        "Title": "\n          `CHECK`-failures in `MapStage`\n        "
    },
    {
        "Bug description": "The  implementation of   is vulnerable to an integer overflow which results in a  CHECK -fail when building new  TensorShape  objects (so, an assert failure based denial of service):",
        "Sample Code": "import numpy as np\n\ntf.raw_ops.AddManySparseToTensorsMap(\n    sparse_indices=[(0,0),(0,1),(0,2),(4,3),(5,0),(5,1)],\n    sparse_values=[1,1,1,1,1,1],\n    sparse_shape=[2**32,2**32],\n    container='',\n    shared_name='',\n    ,\n    name=None)",
        "Bug fix": [
            "@@ -231,16 +231,29 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\n                 errors::InvalidArgument(\n                     \"Input indices should be a matrix but received shape \",\n                     input_indices->shape().DebugString()));\n-\n     OP_REQUIRES(context, TensorShapeUtils::IsVector(input_values->shape()),\n                 errors::InvalidArgument(\n                     \"Input values should be a vector but received shape \",\n                     input_values->shape().DebugString()));\n-\n     OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape->shape()),\n                 errors::InvalidArgument(\n                     \"Input shape should be a vector but received shape \",\n                     input_shape->shape().DebugString()));\n+    OP_REQUIRES(\n+        context,\n+        input_values->shape().dim_size(0) == input_indices->shape().dim_size(0),\n+        errors::InvalidArgument(\n+            \"Number of values must match first dimension of indices. \", \"Got \",\n+            input_values->shape().dim_size(0),\n+            \" values, indices shape: \", input_indices->shape().DebugString()));\n+    OP_REQUIRES(\n+        context,\n+        input_shape->shape().dim_size(0) == input_indices->shape().dim_size(1),\n+        errors::InvalidArgument(\n+            \"Number of dimensions must match second dimension of indices. \",\n+            \"Got \", input_shape->shape().dim_size(0),\n+            \" dimensions, indices shape: \",\n+            input_indices->shape().DebugString()));\n \n     int rank = input_shape->NumElements();\n \n"
        ],
        "Title": "\n          Integer overflows in `AddManySparseToTensorsMap`\n        "
    }
]
[
    {
        "Bug description": "The  implementations of   are vulnerable to integer overflows. These can be used to trigger large allocations (so, OOM based denial of service) or  CHECK -fails when building new  TensorShape  objects (so, assert failures based denial of service):",
        "Sample Code": "import numpy as np\n\ntf.raw_ops.SparseDenseCwiseDiv(\n    sp_indices=np.array([[9]]),\n    sp_values=np.array([5]),\n    sp_shape=np.array([92233720368., 92233720368]),\n    ]),\n    dense=np.array([4]))",
        "Bug fix": [
            "@@ -78,11 +78,24 @@ class SparseDenseBinaryOpShared : public OpKernel {\n                     \"but received shapes: \",\n                     values_t->shape().DebugString(), \" and \",\n                     shape_t->shape().DebugString()));\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsVector(shape_t->shape()),\n+        errors::InvalidArgument(\"Input sp_shape must be a vector. Got: \",\n+                                shape_t->shape().DebugString()));\n     OP_REQUIRES(\n         ctx, values_t->dim_size(0) == indices_t->dim_size(0),\n         errors::InvalidArgument(\n             \"The first dimension of values and indices should match. (\",\n             values_t->dim_size(0), \" vs. \", indices_t->dim_size(0), \")\"));\n+    OP_REQUIRES(\n+        ctx, shape_t->shape().dim_size(0) == indices_t->shape().dim_size(1),\n+        errors::InvalidArgument(\n+            \"Number of dimensions must match second dimension of indices. \",\n+            \"Got \", shape_t->shape().dim_size(0),\n+            \" dimensions, indices shape: \", indices_t->shape().DebugString()));\n+    OP_REQUIRES(ctx, shape_t->NumElements() > 0,\n+                errors::InvalidArgument(\n+                    \"The shape argument requires at least one element.\"));\n \n     const auto indices_mat = indices_t->matrix<int64_t>();\n     const auto shape_vec = shape_t->vec<int64_t>();\n"
        ],
        "Title": "\n          Integer overflows in most sparse component-wise ops\n        "
    },
    {
        "Bug description": "The  code for boosted trees in TensorFlow  is still missing validation. This allows malicious users to read and write outside of bounds of heap allocated data as well as trigger denial of service (via dereferencing  nullptr s or via  CHECK -failures).",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          More incomplete validation in boosted trees code\n        "
    },
    {
        "Bug description": "The  implementation of   can be used to trigger a denial of service attack by causing an OOM condition after an integer overflow:",
        "Sample Code": "tf.raw_ops.StringNGrams(\n  data=['123456'],\n  data_splits=[0,1],\n  separator='a'*15,\n  ngram_widths=[],\n  left_pad='',\n  right_pad='',\n  pad_width=-5, \n  , \n  preserve_short_sequences=True)",
        "Bug fix": [
            "@@ -152,6 +152,16 @@ class StringNGramsOp : public tensorflow::OpKernel {\n         // We don't have to worry about dynamic padding sizes here: if padding\n         // was dynamic, every sequence would have had sufficient padding to\n         // generate at least one ngram.\n+\n+        // If reached here, pad_width should be > 0, pad_width_ = -1,\n+        // which indicates max(ngram_widths) - 1 cannot be used here since\n+        // ngram_width is not known.\n+        OP_REQUIRES(\n+            context, pad_width_ >= 0,\n+            errors::InvalidArgument(\"Pad width should be >= 0 when \"\n+                                    \"preserve_short_sequences is True and \"\n+                                    \"ngram_widths are not provided, got \",\n+                                    pad_width_));\n         int ngram_width = data_length + 2 * pad_width_;\n         auto output_start = &ngrams_data[output_start_idx];\n         int num_ngrams = 1;\n",
            "@@ -28,7 +28,6 @@ from tensorflow.python.platform import test\n \n \n @test_util.run_all_in_graph_and_eager_modes\n-@test_util.disable_tfrt\n class RawOpsTest(test.TestCase, parameterized.TestCase):\n \n   def testSimple(self):\n@@ -63,8 +62,9 @@ class RawOpsTest(test.TestCase, parameterized.TestCase):\n   @parameterized.parameters([[0, 8]], [[-1, 6]])\n   def testStringNGramsBadDataSplits(self, splits):\n     data = [\"aa\", \"bb\", \"cc\", \"dd\", \"ee\", \"ff\"]\n-    with self.assertRaisesRegex(errors.InvalidArgumentError,\n-                                \"Invalid split value\"):\n+    with self.assertRaisesRegex(\n+        errors.InvalidArgumentError,\n+        r\"Invalid split value|First split value must be 0\"):\n       self.evaluate(\n           gen_string_ops.string_n_grams(\n               data=data,\n@@ -76,6 +76,25 @@ class RawOpsTest(test.TestCase, parameterized.TestCase):\n               pad_width=0,\n               preserve_short_sequences=False))\n \n+  def testStringSplit(self):\n+    data = [\"123456\"]\n+    data_splits = [0, 1]\n+    separator = \"a\" * 15\n+    ngram_widths = []\n+    pad_width = -5\n+    left_pad = right_pad = \"\"\n+    with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                \"Pad width should be >= 0\"):\n+      self.evaluate(gen_string_ops.string_n_grams(\n+          data=data,\n+          data_splits=data_splits,\n+          separator=separator,\n+          ngram_widths=ngram_widths,\n+          left_pad=left_pad,\n+          right_pad=right_pad,\n+          pad_width=pad_width,\n+          preserve_short_sequences=True))\n+\n   def testGetSessionHandle(self):\n     if context.executing_eagerly():\n       with self.assertRaisesRegex(\n"
        ],
        "Title": "\n          OOM due to integer overflow in `StringNGrams`\n        "
    },
    {
        "Bug description": "The  implementation of   can be used to trigger a denial of service attack by allocating too much memory:",
        "Sample Code": " tensorflow as tf\ny = tf.raw_ops.ThreadPoolHandle(num_threads=0x60000000,display_name='tf')",
        "Bug fix": [
            "@@ -39,6 +39,22 @@ namespace experimental {\n     PrivateThreadPoolDatasetOp::kDatasetType;\n /* static */ constexpr const char* const PrivateThreadPoolDatasetOp::kDatasetOp;\n \n+namespace {\n+// To prevent integer overflow issues when allocating threadpool memory for an\n+// unreasonable number of threads.\n+constexpr int kThreadLimit = 65536;\n+\n+Status ValidateNumThreads(int32_t num_threads) {\n+  if (num_threads < 0) {\n+    return errors::InvalidArgument(\"`num_threads` must be >= 0\");\n+  }\n+  if (num_threads >= kThreadLimit) {\n+    return errors::InvalidArgument(\"`num_threads` must be < \", kThreadLimit);\n+  }\n+  return Status::OK();\n+}\n+}  // namespace\n+\n class ThreadPoolResource : public ResourceBase {\n  public:\n   ThreadPoolResource(Env* env, const ThreadOptions& thread_options,\n@@ -83,9 +99,7 @@ class ThreadPoolHandleOp : public OpKernel {\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"num_threads\", &num_threads_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"max_intra_op_parallelism\",\n                                      &max_intra_op_parallelism_));\n-    OP_REQUIRES(\n-        ctx, num_threads_ > 0,\n-        errors::InvalidArgument(\"`num_threads` must be greater than zero.\"));\n+    OP_REQUIRES_OK(ctx, ValidateNumThreads(num_threads_));\n   }\n \n   // The resource is deleted from the resource manager only when it is private\n@@ -531,8 +545,7 @@ void PrivateThreadPoolDatasetOp::MakeDatasetFromOptions(OpKernelContext* ctx,\n                                                         DatasetBase* input,\n                                                         int32_t num_threads,\n                                                         DatasetBase** output) {\n-  OP_REQUIRES(ctx, num_threads >= 0,\n-              errors::InvalidArgument(\"`num_threads` must be >= 0\"));\n+  OP_REQUIRES_OK(ctx, ValidateNumThreads(num_threads));\n   *output = new Dataset(ctx,\n                         DatasetContext(DatasetContext::Params(\n                             {PrivateThreadPoolDatasetOp::kDatasetType,\n@@ -546,8 +559,7 @@ void PrivateThreadPoolDatasetOp::MakeDataset(OpKernelContext* ctx,\n   int64_t num_threads = 0;\n   OP_REQUIRES_OK(\n       ctx, ParseScalarArgument<int64_t>(ctx, \"num_threads\", &num_threads));\n-  OP_REQUIRES(ctx, num_threads >= 0,\n-              errors::InvalidArgument(\"`num_threads` must be >= 0\"));\n+  OP_REQUIRES_OK(ctx, ValidateNumThreads(num_threads));\n   *output = new Dataset(ctx, input, num_threads);\n }\n \n"
        ],
        "Title": "\n          OOM in `ThreadPoolHandle`\n        "
    },
    {
        "Bug description": "The  implementation of shape inference for   can be used to trigger a denial of service attack via a segfault caused by a type confusion:",
        "Sample Code": "@\ndef test():\n  y = tf.raw_ops.ConcatV2(\n    values=[[1,2,3],[4,5,6]],\n    axis = 0xb500005b)\n  return y\n\n\n\ntest()",
        "Bug fix": [
            "@@ -2005,7 +2005,7 @@ Status ConcatShapeHelper(InferenceContext* c, int start_value_index,\n   }\n \n   // Minimum required number of dimensions.\n-  const int min_rank = concat_dim < 0 ? -concat_dim : concat_dim + 1;\n+  const int64 min_rank = concat_dim < 0 ? -concat_dim : concat_dim + 1;\n \n   ShapeHandle output_before;\n   ShapeHandle output_after;\n",
            "@@ -16,6 +16,7 @@\n \n import numpy as np\n \n+from tensorflow.python.eager import def_function\n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n from tensorflow.python.framework import errors_impl\n@@ -570,6 +571,17 @@ class ConcatOpTest(test.TestCase):\n         t2 = [2]\n         gen_array_ops.concat_v2([t1, t2], 1).eval()\n \n+  def testConcatInvalidAxisInTfFunction(self):\n+\n+    @def_function.function\n+    def concat_wrapper():\n+      y = gen_array_ops.concat_v2(\n+          values=[[1, 2, 3], [4, 5, 6]], axis=0xb500005b)\n+      return y\n+\n+    with self.assertRaises(ValueError):\n+      concat_wrapper()\n+\n   def testConcatNegativeAxis(self):\n     with test_util.use_gpu():\n       t1 = [[1, 2, 3], [4, 5, 6]]\n"
        ],
        "Title": "\n          Type confusion in shape inference for `ConcatV2`\n        "
    },
    {
        "Bug description": "The  implementation of   is vulnerable to a division by zero caused by an integer overflow bug:",
        "Sample Code": " tensorflow as tf\n\ntf.raw_ops.UnravelIndex(indices=-0x100000,dims=[0x100000,0x100000])",
        "Bug fix": [
            "@@ -13,6 +13,10 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include <cstdint>\n+\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/platform/types.h\"\n #define EIGEN_USE_THREADS\n \n #include \"tensorflow/core/framework/op_kernel.h\"\n@@ -35,7 +39,8 @@ typedef Eigen::ThreadPoolDevice CPUDevice;\n template <typename Tidx>\n class UnravelIndexOp : public OpKernel {\n  public:\n-  explicit UnravelIndexOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n+  explicit UnravelIndexOp(OpKernelConstruction* ctx)\n+      : OpKernel(ctx), dtidx_(DataTypeToEnum<Tidx>::v()) {}\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& indices_tensor = ctx->input(0);\n@@ -54,12 +59,31 @@ class UnravelIndexOp : public OpKernel {\n \n     auto dims = dims_tensor.vec<Tidx>();\n     // Make sure dims does not contain a zero\n+    double prod = 1;\n+    uint64_t limit;\n+    if (dtidx_ == DataType::DT_INT64) {\n+      limit = kint64max;\n+    } else {\n+      limit = kint32max;\n+    }\n+\n     for (int i = 0; i < dims.size(); i++) {\n       OP_REQUIRES(\n           ctx, dims(i) != 0,\n           errors::InvalidArgument(\"Input dims cannot contain a dim of zero, \"\n                                   \"but dims contains zero at index \",\n                                   i));\n+      OP_REQUIRES(ctx, dims(i) > 0,\n+                  errors::InvalidArgument(\n+                      \"Input dims cannot be negative. Got dim = \", dims(i),\n+                      \" at index \", i));\n+      // Check interger overflow\n+      OP_REQUIRES(\n+          ctx, prod <= limit / dims(i),\n+          errors::InvalidArgument(\"Input dims product is causing integer \"\n+                                  \"overflow: (\",\n+                                  dims, \")\"));\n+      prod = (prod * dims(i));\n     }\n \n     // Check to make sure indices is not out of boundary\n@@ -132,6 +156,7 @@ class UnravelIndexOp : public OpKernel {\n                strides_shifted.reshape(reshape).broadcast(bcast);\n     }\n   }\n+  const DataType dtidx_;\n };\n \n #define REGISTER_KERNEL(type)                                               \\\n",
            "@@ -1580,6 +1580,20 @@ class UnravelIndexTest(test_util.TensorFlowTestCase):\n           dims = constant_op.constant([3, 0], dtype=dtype)\n           self.evaluate(array_ops.unravel_index(indices=indices, dims=dims))\n \n+  def testUnravelIndexIntegerOverflow(self):\n+    with self.cached_session():\n+      for dtype in [dtypes.int32, dtypes.int64]:\n+        with self.assertRaisesRegex(\n+            errors.InvalidArgumentError,\n+            r\"Input dims product is causing integer overflow\"):\n+          indices = constant_op.constant(-0x100000, dtype=dtype)\n+          if dtype == dtypes.int32:\n+            value = 0x10000000\n+          else:\n+            value = 0x7FFFFFFFFFFFFFFF\n+          dims = constant_op.constant([value, value], dtype=dtype)\n+          self.evaluate(array_ops.unravel_index(indices=indices, dims=dims))\n+\n \n class GuaranteeConstOpTest(test_util.TensorFlowTestCase):\n \n"
        ],
        "Title": "\n          Overflow and divide by zero in `UnravelIndex`\n        "
    },
    {
        "Bug description": "The  implementation of   does not consider cases where the input tensors are invalid allowing an attacker to read from outside of bounds of heap:",
        "Sample Code": "@\ndef test():\n  y = tf.raw_ops.FractionalAvgPoolGrad(\n    orig_input_tensor_shape=[2,2,2,2],\n    out_backprop=[[[[1,2], [3, 4], [5, 6]], [[7, 8], [9,10], [11,12]]]],\n    row_pooling_sequence=[-10,1,2,3],\n    col_pooling_sequence=[1,2,3,4],\n    overlapping=True)\n  return y\n    \n\n    \ntest()",
        "Bug fix": [
            "@@ -311,15 +311,26 @@ class FractionalAvgPoolGradOp : public OpKernel {\n     for (int64_t b = 0; b < out_batch; ++b) {\n       for (int64_t r = 0; r < out_rows; ++r) {\n         const int64_t in_row_start = row_seq_tensor_flat(r);\n+\n         int64_t in_row_end = overlapping_ ? row_seq_tensor_flat(r + 1)\n                                           : row_seq_tensor_flat(r + 1) - 1;\n         in_row_end = std::min(in_row_end, in_max_row_index);\n+        OP_REQUIRES(context, in_row_start >= 0 && in_row_end >= 0,\n+                    errors::InvalidArgument(\n+                        \"Row sequence tensor values must not be negative, got \",\n+                        row_seq_tensor_flat));\n+\n         for (int64_t c = 0; c < out_cols; ++c) {\n           const int64_t in_col_start = col_seq_tensor_flat(c);\n           int64_t in_col_end = overlapping_ ? col_seq_tensor_flat(c + 1)\n                                             : col_seq_tensor_flat(c + 1) - 1;\n           in_col_end = std::min(in_col_end, in_max_col_index);\n \n+          OP_REQUIRES(\n+              context, in_col_start >= 0 && in_col_end >= 0,\n+              errors::InvalidArgument(\n+                  \"Column sequence tensor values must not be negative, got \",\n+                  col_seq_tensor_flat));\n           const int64_t num_elements_in_pooling_cell =\n               (in_row_end - in_row_start + 1) * (in_col_end - in_col_start + 1);\n           const int64_t out_index = (b * out_rows + r) * out_cols + c;\n",
            "@@ -20,6 +20,7 @@ import numpy as np\n \n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import gen_nn_ops\n@@ -306,6 +307,32 @@ class FractionalAvgTest(test.TestCase):\n           input_b, row_seq, col_seq, overlapping)\n       self.assertSequenceEqual(expected.shape, actual.shape)\n \n+  def testNegativeSeqValuesForGradOp(self):\n+    with self.assertRaisesRegex(\n+        errors.InvalidArgumentError,\n+        r\"Row sequence tensor values must not be negative.*\"):\n+      y = nn_ops.gen_nn_ops.fractional_avg_pool_grad(\n+          orig_input_tensor_shape=[2, 2, 2, 2],\n+          out_backprop=[[[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11,\n+                                                                      12]]]],\n+          row_pooling_sequence=[-10, 1, 2, 3],\n+          col_pooling_sequence=[1, 2, 3, 4],\n+          overlapping=True)\n+\n+      self.evaluate(y)\n+      with self.assertRaisesRegex(\n+          errors.InvalidArgumentError,\n+          r\"Column sequence tensor values must not be negative.*\"):\n+        z = nn_ops.gen_nn_ops.fractional_avg_pool_grad(\n+            orig_input_tensor_shape=[2, 2, 2, 2],\n+            out_backprop=[[[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11,\n+                                                                        12]]]],\n+            row_pooling_sequence=[10, 1, 2, 3],\n+            col_pooling_sequence=[1, 2, -3, 4],\n+            overlapping=True)\n+\n+        self.evaluate(z)\n+\n \n class FractionalAvgPoolGradTest(test.TestCase):\n   \"\"\"Tests for FractionalAvgPoolGrad.\n"
        ],
        "Title": "\n          Heap OOB access in `FractionalAvgPoolGrad`\n        "
    },
    {
        "Bug description": "The  implementation of shape inference for   is vulnerable to an integer overflow weakness:",
        "Sample Code": "input = tf.constant([1,1],dtype=tf.qint32)\n\n@\ndef test():\n  y = tf.raw_ops.Dequantize(\n    input=input,\n    min_range=[1.0],\n    max_range=[10.0],\n    mode='MIN_COMBINED',\n    narrow_range=False,\n    axis=2**31-1,\n    dtype=tf.bfloat16)\n  return y\n\n\n\ntest()",
        "Bug fix": [
            "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/framework/types.pb.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/platform/types.h\"\n #include \"tensorflow/core/util/mirror_pad_mode.h\"\n #include \"tensorflow/core/util/padding.h\"\n #include \"tensorflow/core/util/strided_slice_op.h\"\n@@ -3028,6 +3029,12 @@ REGISTER_OP(\"Dequantize\")\n         return errors::InvalidArgument(\"axis should be at least -1, got \",\n                                        axis);\n       }\n+      auto input_dims = c->Rank(c->input(0));\n+      if (axis > input_dims) {\n+        return errors::InvalidArgument(\n+            \"Axis must be less than input dimension(\", input_dims, \"), got \",\n+            axis);\n+      }\n       const int minmax_rank = (axis == -1) ? 0 : 1;\n       TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n       ShapeHandle minmax;\n@@ -3035,6 +3042,13 @@ REGISTER_OP(\"Dequantize\")\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(2), minmax_rank, &minmax));\n       if (axis != -1) {\n         ShapeHandle input;\n+        if (axis >= kint32max) {\n+          // Check int32 max bound for a corner case to prevent integer flow\n+          // when input actually has kint32max rank and above bound check is not\n+          // triggered.\n+          return errors::InvalidArgument(\n+              \"Axis cannot be >= kint32max value, got \", axis);\n+        }\n         TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n         DimensionHandle depth;\n         TF_RETURN_IF_ERROR(\n",
            "@@ -1704,6 +1704,21 @@ class QuantizeAndDequantizeTest(test_util.TensorFlowTestCase):\n       output_grad = gradient_checker_v2.compute_gradient(f, [input_tensor])\n       self.assertAllClose(output_grad[0], np.zeros([1, 4, 4]))\n \n+  def testOutOfBoundAxis(self):\n+    input_tensor = constant_op.constant([1., 1.])\n+    input_min = [0]\n+    input_max = [1]\n+    q_input, _, _ = array_ops.quantize(input_tensor, 0, 1, dtypes.qint32)\n+    error = (errors.InvalidArgumentError, ValueError)\n+    with self.assertRaisesRegex(error,\n+                                r\".*Axis must be less than input dimension.*\"):\n+      self.evaluate(\n+          gen_array_ops.dequantize(\n+              input=q_input,\n+              min_range=input_min,\n+              max_range=input_max,\n+              axis=2**31 - 1))\n+\n \n @test_util.run_all_in_graph_and_eager_modes\n class SortedSearchTest(test_util.TensorFlowTestCase):\n"
        ],
        "Title": "\n          Integer overflow in shape inference for `Dequantize`\n        "
    },
    {
        "Bug description": "The  implementation of   does not fully validate the value of  axis  and can result in heap OOB accesses:",
        "Sample Code": "@\ndef test():\n  y = tf.raw_ops.Dequantize(\n    input=tf.constant([1,1],dtype=tf.qint32),\n    min_range=[1.0],\n    max_range=[10.0],\n    mode='MIN_COMBINED',\n    narrow_range=False,\n    axis=2**31-1,\n    dtype=tf.bfloat16)\n  return y\n\n\n\ntest()",
        "Bug fix": [
            "@@ -94,6 +94,11 @@ class DequantizeOp : public OpKernel {\n     const Tensor& input_min_tensor = ctx->input(1);\n     const Tensor& input_max_tensor = ctx->input(2);\n \n+    OP_REQUIRES(\n+        ctx, axis_ < input.dims(),\n+        errors::InvalidArgument(\"Axis must be less than input dimension(\",\n+                                input.dims(), \"), got \", axis_));\n+\n     int num_slices = 1;\n     if (axis_ > -1) {\n       num_slices = input.dim_size(axis_);\n"
        ],
        "Title": "\n          Heap OOB access in `Dequantize`\n        "
    },
    {
        "Bug description": "The  implementation of shape inference for   does not fully validate the value of  batch_dim  and can result in a heap OOB read:",
        "Sample Code": "@\ndef test():\n  y = tf.raw_ops.ReverseSequence(\n    input = ['aaa','bbb'],\n    seq_lengths = [1,1,1],\n    seq_dim = -10,\n    batch_dim = -10 )\n  return y\n    \n\n    \ntest()",
        "Bug fix": [
            "@@ -1653,11 +1653,21 @@ REGISTER_OP(\"ReverseSequence\")\n         return errors::InvalidArgument(\n             \"batch_dim must be < input rank: \", batch_dim, \" vs. \", input_rank);\n       }\n+\n       if (seq_dim >= input_rank) {\n         return errors::InvalidArgument(\n             \"seq_dim must be < input rank: \", seq_dim, \" vs. \", input_rank);\n       }\n \n+      // To prevent out of bound access when calling c->Dim(input, batch_dim),\n+      // batch_dim range [-1 * input rank, input rank) is allowed. However,\n+      // the op implementation has a stricter bound for batch_dim requiring >= 0\n+      // value. Thus, perform strict check here.\n+      if (batch_dim < 0) {\n+        return errors::InvalidArgument(\"batch_dim must be >=0, got \",\n+                                       batch_dim);\n+      }\n+\n       DimensionHandle batch_dim_dim = c->Dim(input, batch_dim);\n       TF_RETURN_IF_ERROR(\n           c->Merge(batch_dim_dim, c->Dim(seq_lens_shape, 0), &batch_dim_dim));\n"
        ],
        "Title": "\n          Heap OOB read in shape inference for `ReverseSequence`\n        "
    }
]
[
    {
        "Bug description": "The  estimator for the cost of some convolution operations  can be made to execute a division by 0:",
        "Sample Code": "@\ndef test():\n  y=tf.raw_ops.AvgPoolGrad(\n    orig_input_shape=[1,1,1,1],\n    grad=[[[[1.0],[1.0],[1.0]]],[[[2.0],[2.0],[2.0]]],[[[3.0],[3.0],[3.0]]]],\n    ksize=[1,1,1,1],\n    strides=[1,1,1,0],\n    padding='VALID',\n    data_format='NCHW')\n  return y\n\n\n\ntest()",
        "Bug fix": [
            "@@ -355,6 +355,7 @@ tf_cc_test(\n         \"//tensorflow/core:protos_all_cc\",\n         \"//tensorflow/core:test\",\n         \"//tensorflow/core:test_main\",\n+        \"//tensorflow/core/platform:status_matchers\",\n     ],\n )\n \n",
            "@@ -2153,7 +2153,7 @@ OpInfo::TensorProperties OpLevelCostEstimator::DescribeTensor(\n }\n \n /* static */\n-OpLevelCostEstimator::ConvolutionDimensions\n+StatusOr<OpLevelCostEstimator::ConvolutionDimensions>\n OpLevelCostEstimator::OpDimensionsFromInputs(\n     const TensorShapeProto& original_image_shape, const OpInfo& op_info,\n     bool* found_unknown_shapes) {\n@@ -2190,6 +2190,11 @@ OpLevelCostEstimator::OpDimensionsFromInputs(\n   std::vector<int64_t> strides = GetStrides(op_info);\n   int64_t sx = strides[x_index];\n   int64_t sy = strides[y_index];\n+  if (sx == 0 || sy == 0) {\n+    return errors::InvalidArgument(\n+        \"Stride must be > 0 for Height and Width, but got (\", sy, \", \", sx,\n+        \")\");\n+  }\n   const auto padding = GetPadding(op_info);\n \n   int64_t ox = GetOutputSize(ix, kx, sx, padding);\n@@ -2206,8 +2211,9 @@ Status OpLevelCostEstimator::PredictMaxPool(const OpContext& op_context,\n   bool found_unknown_shapes = false;\n   const auto& op_info = op_context.op_info;\n   // x: op_info.inputs(0)\n-  ConvolutionDimensions dims = OpDimensionsFromInputs(\n-      op_info.inputs(0).shape(), op_info, &found_unknown_shapes);\n+  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n+                      OpDimensionsFromInputs(op_info.inputs(0).shape(), op_info,\n+                                             &found_unknown_shapes));\n   // kx * ky - 1 comparisons per output (kx * xy > 1)\n   // or 1 copy per output (kx * k1 = 1).\n   int per_output_ops = dims.kx * dims.ky == 1 ? 1 : dims.kx * dims.ky - 1;\n@@ -2248,8 +2254,9 @@ Status OpLevelCostEstimator::PredictMaxPoolGrad(const OpContext& op_context,\n                                    op_info.ShortDebugString());\n   }\n \n-  ConvolutionDimensions dims = OpDimensionsFromInputs(\n-      op_info.inputs(0).shape(), op_info, &found_unknown_shapes);\n+  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n+                      OpDimensionsFromInputs(op_info.inputs(0).shape(), op_info,\n+                                             &found_unknown_shapes));\n \n   int64_t ops = 0;\n   if (dims.kx == 1 && dims.ky == 1) {\n@@ -2324,8 +2331,9 @@ Status OpLevelCostEstimator::PredictAvgPool(const OpContext& op_context,\n   bool found_unknown_shapes = false;\n   const auto& op_info = op_context.op_info;\n   // x: op_info.inputs(0)\n-  ConvolutionDimensions dims = OpDimensionsFromInputs(\n-      op_info.inputs(0).shape(), op_info, &found_unknown_shapes);\n+  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n+                      OpDimensionsFromInputs(op_info.inputs(0).shape(), op_info,\n+                                             &found_unknown_shapes));\n \n   // kx * ky - 1 additions and 1 multiplication per output.\n   int64_t ops = dims.batch * dims.ox * dims.oy * dims.oz * dims.kx * dims.ky;\n@@ -2382,8 +2390,9 @@ Status OpLevelCostEstimator::PredictAvgPoolGrad(const OpContext& op_context,\n     found_unknown_shapes = true;\n   }\n \n-  ConvolutionDimensions dims =\n-      OpDimensionsFromInputs(x_shape, op_info, &found_unknown_shapes);\n+  TF_ASSIGN_OR_RETURN(\n+      ConvolutionDimensions dims,\n+      OpDimensionsFromInputs(x_shape, op_info, &found_unknown_shapes));\n \n   int64_t ops = 0;\n   if (dims.kx <= dims.sx && dims.ky <= dims.sy) {\n@@ -2409,8 +2418,9 @@ Status OpLevelCostEstimator::PredictFusedBatchNorm(\n   // offset: op_info.inputs(2)\n   // mean: op_info.inputs(3)  --> only for inference\n   // variance: op_info.inputs(4) --> only for inference\n-  ConvolutionDimensions dims = OpDimensionsFromInputs(\n-      op_info.inputs(0).shape(), op_info, &found_unknown_shapes);\n+  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n+                      OpDimensionsFromInputs(op_info.inputs(0).shape(), op_info,\n+                                             &found_unknown_shapes));\n   const bool is_training = IsTraining(op_info);\n \n   int64_t ops = 0;\n@@ -2459,8 +2469,9 @@ Status OpLevelCostEstimator::PredictFusedBatchNormGrad(\n   // scale: op_info.inputs(2)\n   // mean: op_info.inputs(3)\n   // variance or inverse of variance: op_info.inputs(4)\n-  ConvolutionDimensions dims = OpDimensionsFromInputs(\n-      op_info.inputs(1).shape(), op_info, &found_unknown_shapes);\n+  TF_ASSIGN_OR_RETURN(ConvolutionDimensions dims,\n+                      OpDimensionsFromInputs(op_info.inputs(1).shape(), op_info,\n+                                             &found_unknown_shapes));\n \n   int64_t ops = 0;\n   const auto rsqrt_cost = Eigen::internal::functor_traits<\n",
            "@@ -290,7 +290,7 @@ class OpLevelCostEstimator {\n       bool* found_unknown_shapes);\n \n   // For Pooling, FusedBatchNorm, and their grad ops.\n-  static ConvolutionDimensions OpDimensionsFromInputs(\n+  static StatusOr<ConvolutionDimensions> OpDimensionsFromInputs(\n       const TensorShapeProto& original_image_shape, const OpInfo& op_info,\n       bool* found_unknown_shapes);\n \n",
            "@@ -24,6 +24,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/tensor_shape.pb.h\"\n #include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/platform/status_matchers.h\"\n #include \"tensorflow/core/platform/test.h\"\n #include \"tensorflow/core/protobuf/device_properties.pb.h\"\n \n@@ -558,9 +559,10 @@ class OpLevelCostEstimatorTest : public ::testing::Test {\n     }\n \n     bool found_unknown_shapes;\n-    auto dims = OpLevelCostEstimator::OpDimensionsFromInputs(\n-        op_context.op_info.inputs(0).shape(), op_context.op_info,\n-        &found_unknown_shapes);\n+    TF_ASSERT_OK_AND_ASSIGN(\n+        auto dims, OpLevelCostEstimator::OpDimensionsFromInputs(\n+                       op_context.op_info.inputs(0).shape(), op_context.op_info,\n+                       &found_unknown_shapes));\n     Padding padding_enum;\n     if (padding == \"VALID\") {\n       padding_enum = Padding::VALID;\n@@ -581,6 +583,38 @@ class OpLevelCostEstimatorTest : public ::testing::Test {\n     EXPECT_EQ(padding_enum, dims.padding);\n   }\n \n+  StatusOr<OpLevelCostEstimator::ConvolutionDimensions>\n+  CallOpDimensionsFromInputs(const int n, const int h, const int w, const int c,\n+                             const int kx, const int ky, const int sx,\n+                             const int sy, const string& data_format,\n+                             const string& padding) {\n+    OpContext op_context;\n+\n+    const std::vector<int> x = {n, h, w, c};\n+    const std::vector<int> ksize = {1, kx, ky, 1};\n+    std::vector<int> strides;\n+    if (data_format == \"NHWC\") {\n+      strides = {1, sy, sx, 1};\n+    } else {\n+      strides = {1, 1, sy, sx};\n+    }\n+\n+    auto& op_info = op_context.op_info;\n+    SetCpuDevice(&op_info);\n+    op_info.set_op(\"MaxPool\");\n+\n+    DescribeTensor4D(x[0], x[1], x[2], x[3], op_info.add_inputs());\n+    auto* attr = op_info.mutable_attr();\n+    SetAttrValue(data_format, &(*attr)[\"data_format\"]);\n+    SetAttrValue(padding, &(*attr)[\"padding\"]);\n+    SetAttrValue(strides, &(*attr)[\"strides\"]);\n+    SetAttrValue(ksize, &(*attr)[\"ksize\"]);\n+    bool found_unknown_shapes;\n+    return OpLevelCostEstimator::OpDimensionsFromInputs(\n+        op_context.op_info.inputs(0).shape(), op_context.op_info,\n+        &found_unknown_shapes);\n+  }\n+\n   OpLevelCostEstimator estimator_;\n };\n \n@@ -1383,6 +1417,26 @@ TEST_F(OpLevelCostEstimatorTest, OpDimensionsFromInputs) {\n   }\n }\n \n+TEST_F(OpLevelCostEstimatorTest, OpDimensionsFromInputsError) {\n+  std::vector<string> paddings = {\"VALID\", \"SAME\"};\n+  std::vector<string> formats = {\"NHWC\", \"NCHW\"};\n+  for (const auto& p : paddings) {\n+    for (const auto& f : formats) {\n+      // n, h, w, c, kx, ky, sx, sy, data_format, padding.\n+      ASSERT_THAT(\n+          CallOpDimensionsFromInputs(10, 14, 14, 3840, 3, 3, 0, 2, f, p),\n+          testing::StatusIs(\n+              error::INVALID_ARGUMENT,\n+              \"Stride must be > 0 for Height and Width, but got (2, 0)\"));\n+      ASSERT_THAT(\n+          CallOpDimensionsFromInputs(10, 14, 14, 3840, 3, 3, 2, 0, f, p),\n+          testing::StatusIs(\n+              error::INVALID_ARGUMENT,\n+              \"Stride must be > 0 for Height and Width, but got (0, 2)\"));\n+    }\n+  }\n+}\n+\n TEST_F(OpLevelCostEstimatorTest, PredictMaxPool) {\n   auto predict_max_pool = [this](const int n, const int in, const int c,\n                                  const int k, const int s,\n"
        ],
        "Title": "\n          Floating point division by 0 when executing convolution operators\n        "
    },
    {
        "Bug description": "TensorFlow's  saved_model_cli  tool is vulnerable to a code injection as it  calls ",
        "Sample Code": "for input_raw in filter(bool, input_exprs_str.split(';')):\n    ...\n    input_key, expr = input_raw.split('=', 1)\n    input_dict[input_key] = eval(expr)\n  ...)\n  ...",
        "Bug fix": [
            "@@ -249,6 +249,8 @@\n         endpoint.\n *   TF SavedModel:\n     *   Custom gradients are now saved by default. See `tf.saved_model.SaveOptions` to disable this.\n+    *   The saved_model_cli's `--input_examples` inputs are now restricted to\n+        python literals to avoid code injection.\n *   XLA:\n     * Added a new API that allows custom call functions to signal errors. The\n       old API will be deprecated in a future release. See\n",
            "@@ -20,6 +20,7 @@ https://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmod\n \"\"\"\n \n import argparse\n+import ast\n import os\n import re\n import sys\n@@ -521,7 +522,7 @@ def preprocess_inputs_arg_string(inputs_str):\n   return input_dict\n \n \n-def preprocess_input_exprs_arg_string(input_exprs_str):\n+def preprocess_input_exprs_arg_string(input_exprs_str, safe=True):\n   \"\"\"Parses input arg into dictionary that maps input key to python expression.\n \n   Parses input string in the format of 'input_key=<python expression>' into a\n@@ -529,8 +530,10 @@ def preprocess_input_exprs_arg_string(input_exprs_str):\n \n   Args:\n     input_exprs_str: A string that specifies python expression for input keys.\n-    Each input is separated by semicolon. For each input key:\n+      Each input is separated by semicolon. For each input key:\n         'input_key=<python expression>'\n+    safe: Whether to evaluate the python expression as literals or allow\n+      arbitrary calls (e.g. numpy usage).\n \n   Returns:\n     A dictionary that maps input keys to their values.\n@@ -545,8 +548,15 @@ def preprocess_input_exprs_arg_string(input_exprs_str):\n       raise RuntimeError('--input_exprs \"%s\" format is incorrect. Please follow'\n                          '\"<input_key>=<python expression>\"' % input_exprs_str)\n     input_key, expr = input_raw.split('=', 1)\n-    # ast.literal_eval does not work with numpy expressions\n-    input_dict[input_key] = eval(expr)  # pylint: disable=eval-used\n+    if safe:\n+      try:\n+        input_dict[input_key] = ast.literal_eval(expr)\n+      except:\n+        raise RuntimeError(\n+            f'Expression \"{expr}\" is not a valid python literal.')\n+    else:\n+      # ast.literal_eval does not work with numpy expressions\n+      input_dict[input_key] = eval(expr)  # pylint: disable=eval-used\n   return input_dict\n \n \n@@ -659,7 +669,7 @@ def load_inputs_from_input_arg_string(inputs_str, input_exprs_str,\n   tensor_key_feed_dict = {}\n \n   inputs = preprocess_inputs_arg_string(inputs_str)\n-  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str)\n+  input_exprs = preprocess_input_exprs_arg_string(input_exprs_str, safe=False)\n   input_examples = preprocess_input_examples_arg_string(input_examples_str)\n \n   for input_tensor_key, (filename, variable_name) in inputs.items():\n@@ -923,8 +933,10 @@ def add_run_subparser(subparsers):\n   parser_run.add_argument('--inputs', type=str, default='', help=msg)\n   msg = ('Specifying inputs by python expressions, in the format of'\n          ' \"<input_key>=\\'<python expression>\\'\", separated by \\';\\'. '\n-         'numpy module is available as \\'np\\'. '\n-         'Will override duplicate input keys from --inputs option.')\n+         'numpy module is available as \\'np\\'. Please note that the expression '\n+         'will be evaluated as-is, and is susceptible to code injection. '\n+         'When this is set, the value will override duplicate input keys from '\n+         '--inputs option.')\n   parser_run.add_argument('--input_exprs', type=str, default='', help=msg)\n   msg = (\n       'Specifying tf.Example inputs as list of dictionaries. For example: '\n",
            "@@ -382,7 +382,7 @@ Defined Functions:\n     input_expr_str = 'input3=np.zeros([2,2]);input4=[4,5]'\n     input_dict = saved_model_cli.preprocess_inputs_arg_string(input_str)\n     input_expr_dict = saved_model_cli.preprocess_input_exprs_arg_string(\n-        input_expr_str)\n+        input_expr_str, safe=False)\n     self.assertTrue(input_dict['input1'] == ('/path/file.txt', 'ab3'))\n     self.assertTrue(input_dict['input2'] == ('file2', None))\n     print(input_expr_dict['input3'])\n@@ -418,6 +418,11 @@ Defined Functions:\n           }\n     \"\"\", feature)\n \n+  def testInputPreprocessExampleWithCodeInjection(self):\n+    input_examples_str = 'inputs=os.system(\"echo hacked\")'\n+    with self.assertRaisesRegex(RuntimeError, 'not a valid python literal.'):\n+      saved_model_cli.preprocess_input_examples_arg_string(input_examples_str)\n+\n   def testInputPreProcessFileNames(self):\n     input_str = (r'inputx=C:\\Program Files\\data.npz[v:0];'\n                  r'input:0=c:\\PROGRA~1\\data.npy')\n@@ -434,8 +439,8 @@ Defined Functions:\n     with self.assertRaises(RuntimeError):\n       saved_model_cli.preprocess_inputs_arg_string(input_str)\n     input_str = 'inputx:np.zeros((5))'\n-    with self.assertRaises(RuntimeError):\n-      saved_model_cli.preprocess_input_exprs_arg_string(input_str)\n+    with self.assertRaisesRegex(RuntimeError, 'format is incorrect'):\n+      saved_model_cli.preprocess_input_exprs_arg_string(input_str, safe=False)\n \n   def testInputParserNPY(self):\n     x0 = np.array([[1], [2]])\n"
        ],
        "Title": "\n          Code injection in `saved_model_cli`\n        "
    },
    {
        "Bug description": "TensorFlow's Grappler optimizer has a  use of unitialized variable :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -152,7 +152,7 @@ Status AutoParallel::Initialize(const GrapplerItem& item) {\n   TF_RETURN_IF_ERROR(ComputeTransitiveFanin(graph_, item.fetch, &train_nodes));\n   LOG(INFO) << \"Number of training nodes: \" << train_nodes.size();\n \n-  const NodeDef* dequeue_node;\n+  const NodeDef* dequeue_node = nullptr;\n   for (const auto& train_node : train_nodes) {\n     if (IsDequeueOp(*train_node)) {\n       dequeue_node = train_node;\n",
            "@@ -126,6 +126,30 @@ TEST_F(AutoParallelTest, SimpleParallel) {\n   EXPECT_EQ(\"^AutoParallel-Control-Fetch\", node_gradient.input(0));\n }\n \n+TEST_F(AutoParallelTest, SimpleParallelNoDequeue) {\n+  tensorflow::Scope s = tensorflow::Scope::DisabledShapeInferenceScope();\n+  Output constant_a = ops::Const(s.WithOpName(\"constant_a\"), 1.0f, {1});\n+  Output constant_c = ops::Const(s.WithOpName(\"constant_c\"), 1.0f, {1});\n+  Output constant_b = ops::Const(s.WithOpName(\"constant_b\"), 1, {1});\n+  Output var = ops::Variable(s.WithOpName(\"var\"), {1}, DT_FLOAT);\n+  Output assign = ops::Assign(s.WithOpName(\"assign\"), {var}, {constant_a});\n+  Output add = ops::AddN(s.WithOpName(\"add\"), {constant_a, constant_c});\n+  Output learning_rate = ops::Const(s.WithOpName(\"learning_rate\"), 0.01f, {1});\n+  Output apply_gradient = ops::ApplyGradientDescent(\n+      s.WithOpName(\"apply_gradient\"), {var}, {learning_rate}, {add});\n+\n+  GrapplerItem item;\n+  item.init_ops.push_back(\"assign\");\n+  item.fetch.push_back(\"apply_gradient\");\n+  item.init_ops.push_back(\"assign\");\n+  TF_CHECK_OK(s.ToGraphDef(&item.graph));\n+\n+  AutoParallel parallel(2);\n+  GraphDef output;\n+  Status status = parallel.Optimize(nullptr, item, &output);\n+  TF_EXPECT_OK(status);\n+}\n+\n }  // namespace\n }  // namespace grappler\n }  // namespace tensorflow\n"
        ],
        "Title": "\n          A use of uninitialized value vulnerability in Tensorflow\n        "
    },
    {
        "Bug description": "The  implementation  of  FusedBatchNorm  kernels is vulnerable to a heap OOB:",
        "Sample Code": "tf.raw_ops.FusedBatchNormGrad(\n  y_backprop=tf.constant([i for i in range(9)],shape=(1,1,3,3),dtype=tf.float32)\n  x=tf.constant([i for i in range(2)],shape=(1,1,1,2),dtype=tf.float32)\n  scale=[1,1],\n  reserve_space_1=[1,1],\n  reserve_space_2=[1,1,1],\n  epsilon=1.0,\n  data_format='NCHW',\n  ,\n  is_training=True) ",
        "Bug fix": [
            "@@ -1340,18 +1340,20 @@ class FusedBatchNormOpBase : public OpKernel {\n         errors::InvalidArgument(\"offset must have the same number of elements \"\n                                 \"as the channels of x, got \",\n                                 offset.NumElements(), \" and \", num_channels));\n-    if (estimated_mean.NumElements() != 0) {\n+    if (!is_training_ || exponential_avg_factor_ != 1.) {\n+      std::string prefix_msg = is_training_ ? \"When exponential_avg_factor != 1\"\n+                                            : \"When is_training=false\";\n       OP_REQUIRES(context, estimated_mean.NumElements() == num_channels,\n                   errors::InvalidArgument(\n-                      \"mean must be empty or have the same number of \"\n-                      \"elements as the channels of x, got \",\n+                      prefix_msg,\n+                      \", mean must have the same number \"\n+                      \"of elements as the channels of x, got \",\n                       estimated_mean.NumElements(), \" and \", num_channels));\n-    }\n-    if (estimated_variance.NumElements() != 0) {\n       OP_REQUIRES(context, estimated_variance.NumElements() == num_channels,\n                   errors::InvalidArgument(\n-                      \"variance must be empty or have the same number of \"\n-                      \"elements as the channels of x, got \",\n+                      prefix_msg,\n+                      \", variance must have the same \"\n+                      \"number of elements as the channels of x, got \",\n                       estimated_variance.NumElements(), \" and \", num_channels));\n     }\n \n@@ -1543,6 +1545,11 @@ class FusedBatchNormGradOpBase : public OpKernel {\n                 errors::InvalidArgument(\n                     \"saved variance must be 1-dimensional\",\n                     saved_maybe_inv_var_or_pop_var.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, x.shape() == y_backprop.shape(),\n+        errors::InvalidArgument(\n+            \"x and y_backprop must have same shape, but x has shape \",\n+            x.shape(), \" and y_backprop has shape \", y_backprop.shape()));\n     if (use_activation) {\n       OP_REQUIRES(\n           context, x.dim_size(3) % 4 == 0,\n@@ -1569,6 +1576,23 @@ class FusedBatchNormGradOpBase : public OpKernel {\n                   errors::InvalidArgument(\"Error during tensor copy.\"));\n     }\n \n+    const auto num_channels = GetTensorDim(x, tensor_format_, 'C');\n+    OP_REQUIRES(\n+        context, scale.NumElements() == num_channels,\n+        errors::InvalidArgument(\"scale must have the same number of elements \"\n+                                \"as the channels of x, got \",\n+                                scale.NumElements(), \" and \", num_channels));\n+    OP_REQUIRES(\n+        context, saved_mean_or_pop_mean.NumElements() == num_channels,\n+        errors::InvalidArgument(\"reserve_space_1 must have the same number of \"\n+                                \"elements as the channels of x, got \",\n+                                scale.NumElements(), \" and \", num_channels));\n+    OP_REQUIRES(\n+        context, saved_maybe_inv_var_or_pop_var.NumElements() == num_channels,\n+        errors::InvalidArgument(\"reserve_space_2 must have the same number of \"\n+                                \"elements as the channels of x, got \",\n+                                scale.NumElements(), \" and \", num_channels));\n+\n     Tensor* x_backprop = nullptr;\n     auto alloc_shape = use_reshape ? dest_shape : x_shape;\n     OP_REQUIRES_OK(context,\n",
            "@@ -16,10 +16,13 @@\n \n import numpy as np\n \n+from tensorflow.python.eager import context\n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors_impl\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import gen_nn_ops\n from tensorflow.python.ops import gradient_checker\n from tensorflow.python.ops import gradients_impl\n from tensorflow.python.ops import math_ops\n@@ -694,6 +697,126 @@ class BatchNormalizationTest(test.TestCase):\n     y_ref = np.maximum(y_ref, 0.)\n     self.assertAllClose(y_ref, y_val, atol=1e-3)\n \n+  def testEagerShapeErrors(self):\n+    with context.eager_mode():\n+      x = array_ops.ones((2, 2, 2, 2))\n+      scale = array_ops.ones((3,))\n+      offset = array_ops.ones((2,))\n+      with self.assertRaisesRegex(\n+          errors_impl.InvalidArgumentError,\n+          'scale must have the same number of elements'):\n+        nn_impl.fused_batch_norm(x, scale, offset)\n+\n+      x = array_ops.ones((2, 2, 2, 2))\n+      scale = array_ops.ones((2,))\n+      offset = array_ops.ones((3,))\n+      with self.assertRaisesRegex(\n+          errors_impl.InvalidArgumentError,\n+          'offset must have the same number of elements'):\n+        nn_impl.fused_batch_norm(x, scale, offset)\n+\n+      x = array_ops.ones((2, 2, 2, 2))\n+      scale = array_ops.ones((2,))\n+      offset = array_ops.ones((2,))\n+      mean = array_ops.ones((0,))\n+      variance = array_ops.ones((2,))\n+      with self.assertRaisesRegex(\n+          errors_impl.InvalidArgumentError,\n+          'When is_training=false, mean must have the same number of elements'):\n+        nn_impl.fused_batch_norm(\n+            x, scale, offset, mean=mean, variance=variance, is_training=False)\n+\n+      x = array_ops.ones((2, 2, 2, 2))\n+      scale = array_ops.ones((2,))\n+      offset = array_ops.ones((2,))\n+      mean = array_ops.ones((2,))\n+      variance = array_ops.ones((0,))\n+      with self.assertRaisesRegex(\n+          errors_impl.InvalidArgumentError,\n+          'When is_training=false, variance must have the same number of '\n+          'elements'):\n+        nn_impl.fused_batch_norm(\n+            x, scale, offset, mean=mean, variance=variance, is_training=False)\n+\n+      x = array_ops.ones((2, 2, 2, 2))\n+      scale = array_ops.ones((2,))\n+      offset = array_ops.ones((2,))\n+      mean = array_ops.ones((0,))\n+      variance = array_ops.ones((2,))\n+      with self.assertRaisesRegex(\n+          errors_impl.InvalidArgumentError,\n+          'When exponential_avg_factor != 1, mean must have the same number of '\n+          'elements'):\n+        nn_impl.fused_batch_norm(\n+            x,\n+            scale,\n+            offset,\n+            mean=mean,\n+            variance=variance,\n+            exponential_avg_factor=0.5)\n+\n+      x = array_ops.ones((2, 2, 2, 2))\n+      scale = array_ops.ones((2,))\n+      offset = array_ops.ones((2,))\n+      mean = array_ops.ones((2,))\n+      variance = array_ops.ones((0,))\n+      with self.assertRaisesRegex(\n+          errors_impl.InvalidArgumentError,\n+          'When exponential_avg_factor != 1, variance must have the same '\n+          'number of elements'):\n+        nn_impl.fused_batch_norm(\n+            x,\n+            scale,\n+            offset,\n+            mean=mean,\n+            variance=variance,\n+            exponential_avg_factor=0.5)\n+\n+  def testEagerShapeGradErrors(self):\n+    with context.eager_mode():\n+      y_backprop = array_ops.ones((2, 2, 2, 3))\n+      x = array_ops.ones((2, 2, 2, 2))\n+      scale = array_ops.ones((2,))\n+      reserve_space_1 = array_ops.ones((2,))\n+      reserve_space_2 = array_ops.ones((2,))\n+      with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n+                                  'x and y_backprop must have same shape,'):\n+        gen_nn_ops.fused_batch_norm_grad_v2(y_backprop, x, scale,\n+                                            reserve_space_1, reserve_space_2)\n+\n+      y_backprop = array_ops.ones((2, 2, 2, 2))\n+      x = array_ops.ones((2, 2, 2, 2))\n+      scale = array_ops.ones((3,))\n+      reserve_space_1 = array_ops.ones((2,))\n+      reserve_space_2 = array_ops.ones((2,))\n+      with self.assertRaisesRegex(\n+          errors_impl.InvalidArgumentError,\n+          'scale must have the same number of elements'):\n+        gen_nn_ops.fused_batch_norm_grad_v2(y_backprop, x, scale,\n+                                            reserve_space_1, reserve_space_2)\n+\n+      y_backprop = array_ops.ones((2, 2, 2, 2))\n+      x = array_ops.ones((2, 2, 2, 2))\n+      scale = array_ops.ones((2,))\n+      reserve_space_1 = array_ops.ones((3,))\n+      reserve_space_2 = array_ops.ones((2,))\n+      with self.assertRaisesRegex(\n+          errors_impl.InvalidArgumentError,\n+          'reserve_space_1 must have the same number of elements'):\n+        gen_nn_ops.fused_batch_norm_grad_v2(y_backprop, x, scale,\n+                                            reserve_space_1, reserve_space_2)\n+\n+      y_backprop = array_ops.ones((2, 2, 2, 2))\n+      x = array_ops.ones((2, 2, 2, 2))\n+      scale = array_ops.ones((2,))\n+      reserve_space_1 = array_ops.ones((2,))\n+      reserve_space_2 = array_ops.ones((3,))\n+      with self.assertRaisesRegex(\n+          errors_impl.InvalidArgumentError,\n+          'reserve_space_2 must have the same number of elements'):\n+        gen_nn_ops.fused_batch_norm_grad_v2(y_backprop, x, scale,\n+                                            reserve_space_1, reserve_space_2)\n+\n \n if __name__ == '__main__':\n   test.main()\n"
        ],
        "Title": "\n          Heap OOB in `FusedBatchNorm` kernels\n        "
    },
    {
        "Bug description": "The  ImmutableConst  operation in TensorFlow can be tricked into reading arbitrary memory contents:",
        "Sample Code": "with open('/tmp/test','wb') as f:\n    f.write(b'*128)\n    data = tf.raw_ops.ImmutableConst(dtype=tf.string,shape=3,memory_region_name='/tmp/test')\n  \n)\n  \nprint(data)",
        "Bug fix": [
            "@@ -63,9 +63,9 @@ static inline uint32_t TF_swap32(uint32_t host_int) {\n #endif\n \n #if TF_TSTRING_LITTLE_ENDIAN\n-#define TF_le32toh(x) TF_swap32(x)\n-#else  // TF_TSTRING_LITTLE_ENDIAN\n #define TF_le32toh(x) x\n+#else  // TF_TSTRING_LITTLE_ENDIAN\n+#define TF_le32toh(x) TF_swap32(x)\n #endif  // TF_TSTRING_LITTLE_ENDIAN\n \n static inline size_t TF_align16(size_t i) { return (i + 0xF) & ~0xF; }\n",
            "@@ -18,6 +18,7 @@ limitations under the License.\n #include <memory>\n #include <string>\n \n+#include \"tensorflow/core/platform/ctstring_internal.h\"\n #include \"tensorflow/core/platform/test.h\"\n \n static const char kLongString[] =\n@@ -380,3 +381,29 @@ TEST(TF_CTStringTest, ResizeReserve) {\n     TF_TString_Dealloc(&s70);\n   }\n }\n+\n+TEST(TF_CTStringTest, OffsetType) {\n+  {\n+    TF_TString s71;\n+\n+    TF_TString_Init(&s71);\n+    size_t header_length = 24;\n+    size_t size = 8;\n+    TF_TString_ResizeUninitialized(&s71, header_length + size);\n+    uint32_t save_size = s71.u.offset.size;\n+    uint32_t save_offset = s71.u.offset.offset;\n+    uint32_t save_count = s71.u.offset.count;\n+\n+    s71.u.offset.size = TF_TString_ToInternalSizeT(size, TF_TSTR_OFFSET);\n+    s71.u.offset.offset = header_length;\n+    s71.u.offset.count = 0;\n+    EXPECT_EQ(size, TF_TString_GetSize(&s71));\n+    EXPECT_EQ(TF_TSTR_OFFSET, TF_TString_GetType(&s71));\n+\n+    // restore state so string can be deallocated\n+    s71.u.offset.size = save_size;\n+    s71.u.offset.offset = save_offset;\n+    s71.u.offset.count = save_count;\n+    TF_TString_Dealloc(&s71);\n+  }\n+}\n"
        ],
        "Title": "\n          Arbitrary memory read in `ImmutableConst`\n        "
    },
    {
        "Bug description": "The  implementation  of  SparseBinCount  is vulnerable to a heap OOB:",
        "Sample Code": "tf.raw_ops.SparseBincount(\n  indices=[[0],[1],[2]]\n  values=[0,-10000000]\n  dense_shape=[1,1]\n  size=[1]\n  weights=[3,2,1]\n  ]\n  binary_output=False)",
        "Bug fix": [
            "@@ -405,6 +405,16 @@ class SparseBincountOp : public OpKernel {\n       for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {\n         const int64_t batch = indices_mat(i, 0);\n         const Tidx bin = values(i);\n+        OP_REQUIRES(\n+            ctx, batch < out.dimension(0),\n+            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,\n+                                    \") must be less than the dimension size (\",\n+                                    out.dimension(0), \").\"));\n+        OP_REQUIRES(\n+            ctx, bin < out.dimension(1),\n+            errors::InvalidArgument(\"Index out ouf bound. `bin` (\", bin,\n+                                    \") must be less then the dimension size (\",\n+                                    out.dimension(1), \").\"));\n         if (bin < size) {\n           if (binary_output_) {\n             out(batch, bin) = T(1);\n"
        ],
        "Title": "\n          Heap OOB in `SparseBinCount`\n        "
    },
    {
        "Bug description": "The  implementation  of  SparseFillEmptyRows  can be made to trigger a heap OOB access:",
        "Sample Code": "data=tf.raw_ops.SparseFillEmptyRows(\n  indices=[[0,0],[0,0],[0,0]],\n  values=['sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss'],\n  dense_shape=[5,3],\n  ],\n  default_value='o')",
        "Bug fix": [
            "@@ -24,11 +24,13 @@ limitations under the License.\n #include <vector>\n \n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/op_requires.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor_util.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/lib/gtl/inlined_vector.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/util/sparse/sparse_tensor.h\"\n \n namespace tensorflow {\n@@ -222,6 +224,12 @@ void SparseFillEmptyRowsOpImpl(OpKernelContext* context,\n                     errors::InvalidArgument(\"values must be a vector, saw: \",\n                                             values_t.shape().DebugString()),\n                     done);\n+  OP_REQUIRES_ASYNC(\n+      context, indices_t.dim_size(0) == values_t.dim_size(0),\n+      errors::InvalidArgument(\"The length of `values` (\", values_t.dim_size(0),\n+                              \") must match the first dimension of `indices` (\",\n+                              indices_t.dim_size(0), \").\"),\n+      done);\n   OP_REQUIRES_ASYNC(\n       context, TensorShapeUtils::IsScalar(default_value_t.shape()),\n       errors::InvalidArgument(\"default_value must be a scalar, saw: \",\n"
        ],
        "Title": "\n          `SparseFillEmptyRows` heap OOB\n        "
    },
    {
        "Bug description": "The  implementation  of  SplitV  can trigger a segfault is an attacker supplies negative arguments:",
        "Sample Code": "tf.raw_ops.SplitV(\n  value=tf.constant([]),\n  size_splits=[-1, -2]\n  ,axis=0,\n  ,\n  num_split=2)",
        "Bug fix": [
            "@@ -138,6 +138,13 @@ class SplitVOpBase : public OpKernel {\n       (*split_sizes_vec)[neg_one_dim] = input_size_split_dim - determined_size;\n     }\n \n+    for (int i = 0; i < split_sizes_vec->size(); ++i) {\n+      const Tlen& split_size = (*split_sizes_vec)[i];\n+      OP_REQUIRES(context, split_size >= Tlen(0),\n+                  errors::InvalidArgument(\"Split size at index \", i,\n+                                          \" must be >= 0. Got: \", split_size));\n+    }\n+\n     // Special case 2: split along the 1st dimension. The requirements are that\n     // either we are splitting the outer dimension of two or more such that\n     // every outer subpart is aligned or that the split sizes mean that they are\n",
            "@@ -681,6 +681,12 @@ REGISTER_OP(\"SplitV\")\n           if (data[i] == -1 && c->ValueKnown(split_dim_size)) {\n             size = split_dim_size - total_size;\n           }\n+          // If we have a negative known size (either explicit, or computed\n+          // via -1), then the split sizes are invalid.\n+          if (size < -1 || (size == -1 && c->ValueKnown(split_dim_size))) {\n+            return errors::InvalidArgument(\"Split size at index \", i,\n+                                           \" must be >= 0. Got: \", size);\n+          }\n           TF_RETURN_IF_ERROR(\n               c->ReplaceDim(input, split_dim, c->MakeDim(size), &output_shape));\n           c->set_output(i, output_shape);\n",
            "@@ -384,6 +384,24 @@ class SplitOpTest(test.TestCase):\n                                   \"must have exactly one element\"):\n         sess.run(y, {x: np.array([], dtype=np.int32), splits: [4, 11, 15]})\n \n+  @test_util.run_in_graph_and_eager_modes\n+  def testNegativeSizes(self):\n+    x = constant_op.constant([1, 2, 3], dtypes.float32)\n+    # A size of -1 signifies to determine size based on sum of other splits.\n+    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n+                                \"Split size at index 1 must be >= 0. Got: -2\"):\n+      splits = [-1, -2]\n+      self.evaluate(array_ops.split(x, splits, axis=0))\n+\n+  @test_util.run_in_graph_and_eager_modes\n+  def testBadSplitSizes(self):\n+    x = constant_op.constant([1, 2], dtypes.float32)\n+    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n+                                \"Determined shape must either match input\"\n+                                \"|can't split axis\"):\n+      splits = [1, 2]\n+      self.evaluate(array_ops.split(x, splits, axis=0))\n+\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          Segfault due to negative splits in `SplitV`\n        "
    },
    {
        "Bug description": "The  shape inference code  for the  Cudnn*  operations in TensorFlow can be tricked into accessing invalid memory, via a heap buffer overflow:",
        "Sample Code": "@\ndef func():\n  return tf.raw_ops.CudnnRNNV3(\n    input=[0.1, 0.1],\n    input_h=[0.5],\n    input_c=[0.1, 0.1, 0.1], \n    params=[0.5, 0.5],\n    sequence_lengths=[-1, 0, 1])\n  \n])\n  \nfunc() ",
        "Bug fix": [
            "@@ -81,11 +81,17 @@ REGISTER_OP(\"CudnnRNN\")\n     .Attr(\"seed2: int = 0\")\n     .Attr(\"is_training: bool = true\")\n     .SetShapeFn([](InferenceContext* c) {\n+      ShapeHandle unused;\n       auto input_shape = c->input(0);\n       auto input_h_shape = c->input(1);\n+      TF_RETURN_IF_ERROR(c->WithRank(input_shape, 3, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(input_h_shape, 3, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 1, &unused));\n+\n       auto seq_length = c->Dim(input_shape, 0);\n       auto batch_size = c->Dim(input_shape, 1);\n       auto num_units = c->Dim(input_h_shape, 2);\n+\n       string direction;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n       string rnn_mode;\n@@ -124,8 +130,13 @@ REGISTER_OP(\"CudnnRNNV2\")\n     .Attr(\"seed2: int = 0\")\n     .Attr(\"is_training: bool = true\")\n     .SetShapeFn([](InferenceContext* c) {\n+      ShapeHandle unused;\n       auto input_shape = c->input(0);\n       auto input_h_shape = c->input(1);\n+      TF_RETURN_IF_ERROR(c->WithRank(input_shape, 3, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(input_h_shape, 3, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 1, &unused));\n+\n       auto seq_length = c->Dim(input_shape, 0);\n       auto batch_size = c->Dim(input_shape, 1);\n       auto num_units = c->Dim(input_h_shape, 2);\n@@ -171,16 +182,26 @@ REGISTER_OP(\"CudnnRNNV3\")\n     .Attr(\"is_training: bool = true\")\n     .Attr(\"time_major: bool = true\")\n     .SetShapeFn([](InferenceContext* c) {\n+      ShapeHandle unused;\n       auto input_shape = c->input(0);\n       auto input_h_shape = c->input(1);\n       auto input_c_shape = c->input(2);\n+      TF_RETURN_IF_ERROR(c->WithRank(input_shape, 3, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(input_h_shape, 3, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 1, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 1, &unused));\n+\n       auto max_seq_length = c->Dim(input_shape, 0);\n       auto batch_size = c->Dim(input_shape, 1);\n       auto num_units = c->Dim(input_h_shape, 2);\n+\n       string direction;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n       string rnn_mode;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"rnn_mode\", &rnn_mode));\n+      if (rnn_mode == \"lstm\") {\n+        TF_RETURN_IF_ERROR(c->WithRank(input_c_shape, 3, &unused));\n+      }\n       int dir_count = (direction == \"bidirectional\") ? 2 : 1;\n       DimensionHandle output_size;\n       TF_RETURN_IF_ERROR(c->Multiply(num_units, dir_count, &output_size));\n",
            "@@ -68,6 +68,11 @@ TEST(CudnnRNNOpsTest, ForwardLstm_ShapeFn) {\n                    .Attr(\"direction\", \"unidirectional\")\n                    .Finalize(&op.node_def));\n   INFER_OK(op, input_shapes_desc, output_shapes_desc);\n+  INFER_ERROR(\"Shape must be rank 3 \", op, \"[];[?,?,?];[?,?,?];[?]\");\n+  INFER_ERROR(\"Shape must be rank 3 \", op, \"[?,?,?];[];[?,?,?];[?]\");\n+  // Disabled because the kernel does not check shape of input_c.\n+  // INFER_ERROR(\"Shape must be rank 3 \", op, \"[?,?,?];[?,?,?];[?];[?]\");\n+  INFER_ERROR(\"Shape must be rank 1 \", op, \"[?,?,?];[?,?,?];[?,?,?];[]\");\n }\n \n TEST(CudnnRNNOpsTest, ForwardV2Lstm_ShapeFn) {\n@@ -100,6 +105,11 @@ TEST(CudnnRNNOpsTest, ForwardV2Lstm_ShapeFn) {\n                    .Attr(\"direction\", \"unidirectional\")\n                    .Finalize(&op.node_def));\n   INFER_OK(op, input_shapes_desc, output_shapes_desc);\n+  INFER_ERROR(\"Shape must be rank 3 \", op, \"[];[?,?,?];[?,?,?];[?]\");\n+  INFER_ERROR(\"Shape must be rank 3 \", op, \"[?,?,?];[];[?,?,?];[?]\");\n+  // Disabled because the kernel does not check shape of input_c.\n+  // INFER_ERROR(\"Shape must be rank 3 \", op, \"[?,?,?];[?,?,?];[?];[?]\");\n+  INFER_ERROR(\"Shape must be rank 1 \", op, \"[?,?,?];[?,?,?];[?,?,?];[]\");\n }\n \n TEST(CudnnRNNOpsTest, ForwardV3Lstm_ShapeFn) {\n@@ -137,6 +147,52 @@ TEST(CudnnRNNOpsTest, ForwardV3Lstm_ShapeFn) {\n                    .Attr(\"direction\", \"unidirectional\")\n                    .Finalize(&op.node_def));\n   INFER_OK(op, input_shapes_desc, output_shapes_desc);\n+  INFER_ERROR(\"Shape must be rank 3 \", op, \"[];[?,?,?];[?,?,?];[?];[?]\");\n+  INFER_ERROR(\"Shape must be rank 3 \", op, \"[?,?,?];[];[?,?,?];[?];[?]\");\n+  INFER_ERROR(\"Shape must be rank 3 \", op, \"[?,?,?];[?,?,?];[];[?];[?]\");\n+  INFER_ERROR(\"Shape must be rank 1 \", op, \"[?,?,?];[?,?,?];[?,?,?];[];[?]\");\n+  INFER_ERROR(\"Shape must be rank 1 \", op, \"[?,?,?];[?,?,?];[?,?,?];[?];[]\");\n+}\n+\n+TEST(CudnnRNNOpsTest, ForwardV3Gru) {\n+  int max_seq_length = 2;\n+  int batch_size = 3;\n+  int num_units = 4;\n+  int num_layers = 5;\n+  int dir_count = 1;\n+  std::vector<int> input_shape = {max_seq_length, batch_size, num_units};\n+  std::vector<int> input_h_shape = {num_layers * dir_count, batch_size,\n+                                    num_units};\n+  std::vector<int> input_c_shape = {num_layers * dir_count, batch_size,\n+                                    num_units};\n+  std::vector<int> output_shape = {max_seq_length, batch_size,\n+                                   num_units * dir_count};\n+  std::vector<int> seq_lengths_shape = {batch_size};\n+  auto shape_to_str = [](const std::vector<int>& v) {\n+    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n+  };\n+  string input_shapes_desc = strings::StrCat(\n+      shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n+      shape_to_str(input_c_shape), \";\", \"[?]\", \";\",\n+      shape_to_str(seq_lengths_shape));\n+  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;[];?;?\";\n+\n+  ShapeInferenceTestOp op(\"CudnnRNNV3\");\n+  TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNNV3\")\n+                   .Input({\"input\", 0, DT_FLOAT})\n+                   .Input({\"input_h\", 0, DT_FLOAT})\n+                   .Input({\"input_c\", 0, DT_FLOAT})\n+                   .Input({\"params\", 0, DT_FLOAT})\n+                   .Input({\"sequence_lengths\", 0, DT_INT32})\n+                   .Attr(\"rnn_mode\", \"gru\")\n+                   .Attr(\"input_mode\", \"auto_select\")\n+                   .Attr(\"direction\", \"unidirectional\")\n+                   .Finalize(&op.node_def));\n+  INFER_OK(op, input_shapes_desc, output_shapes_desc);\n+  INFER_ERROR(\"Shape must be rank 3 \", op, \"[];[?,?,?];[];[?];[?]\");\n+  INFER_ERROR(\"Shape must be rank 3 \", op, \"[?,?,?];[];[];[?];[?]\");\n+  INFER_ERROR(\"Shape must be rank 1 \", op, \"[?,?,?];[?,?,?];[];[];[?]\");\n+  INFER_ERROR(\"Shape must be rank 1 \", op, \"[?,?,?];[?,?,?];[];[?];[]\");\n }\n \n }  // end namespace tensorflow\n"
        ],
        "Title": "\n          Access to invalid memory during shape inference in `Cudnn*` ops\n        "
    },
    {
        "Bug description": "The  process of building the control flow graph  for a TensorFlow model is vulnerable to a null pointer exception when nodes that should be paired are not:",
        "Sample Code": "@\ndef func():\n  return tf.raw_ops.Exit(data=[False,False])\n    \n])\n    \nfunc()",
        "Bug fix": [
            "@@ -316,6 +316,10 @@ Status ImmutableExecutorState::BuildControlFlowInfo(const Graph* g,\n     } else if (IsExit(curr_node)) {\n       // Exit to the parent frame.\n       parent = parent_nodes[curr_id];\n+      if (!parent) {\n+        return errors::InvalidArgument(\n+            \"Invalid Exit op: Cannot find a corresponding Enter op.\");\n+      }\n       frame_name = cf_info->frame_names[parent->id()];\n       parent = parent_nodes[parent->id()];\n     } else {\n"
        ],
        "Title": "\n          Null pointer exception when `Exit` node is not preceded by `Enter` op\n        "
    }
]
[
    {
        "Bug description": "The  shape inference code for   can be made to execute a division by 0:",
        "Sample Code": "@\ndef func():\n  return tf.raw_ops.AllToAll(\n    input=[0.0, 0.1652, 0.6543],\n    group_assignment=[1, -1],\n    concat_dimension=0,\n    split_dimension=0,\n    split_count=0)\n\n)\n\nfunc()",
        "Bug fix": [
            "@@ -32,6 +32,7 @@ REGISTER_OP(\"AllToAll\")\n     .Attr(\"split_count: int\")\n     .SetShapeFn([](InferenceContext* c) {\n       ShapeHandle input = c->input(0);\n+      ShapeHandle group_assignment = c->input(1);\n       if (!c->RankKnown(input)) {\n         c->set_output(0, c->UnknownShape());\n         return Status::OK();\n@@ -42,6 +43,21 @@ REGISTER_OP(\"AllToAll\")\n       int split_dimension;\n       int split_count;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"split_count\", &split_count));\n+      if (split_count < 1) {\n+        return errors::InvalidArgument(\"split_count \", split_count,\n+                                       \" must at least be one.\");\n+      }\n+      if (c->RankKnown(group_assignment) && c->Rank(group_assignment) != 2) {\n+        return errors::InvalidArgument(\"group_assignment must have rank 2.\");\n+      }\n+      DimensionHandle num_replicas_per_group = c->Dim(group_assignment, 1);\n+      if (c->ValueKnown(num_replicas_per_group) &&\n+          (c->Value(num_replicas_per_group) != split_count)) {\n+        return errors::InvalidArgument(\n+            \"split_count \", split_count,\n+            \" must equal the size of the second dimension of group_assignment \",\n+            c->Value(num_replicas_per_group));\n+      }\n \n       TF_RETURN_IF_ERROR(c->GetAttr(\"concat_dimension\", &concat_dimension));\n \n@@ -65,6 +81,12 @@ REGISTER_OP(\"AllToAll\")\n           dims[i] = c->MakeDim(c->Value(dims[i]) * split_count);\n         }\n         if (i == split_dimension) {\n+          if (c->ValueKnown(dims[i]) &&\n+              (c->Value(dims[i]) % split_count != 0)) {\n+            return errors::InvalidArgument(\n+                \"input dimension \", c->Value(dims[i]),\n+                \" not divisible by split_count \", split_count);\n+          }\n           dims[i] = c->MakeDim(c->Value(dims[i]) / split_count);\n         }\n       }\n",
            "@@ -32,6 +32,7 @@ from tensorflow.python.platform import test\n from tensorflow.python.tpu import tpu\n from tensorflow.python.tpu import tpu_feed\n from tensorflow.python.tpu import training_loop\n+from tensorflow.python.tpu.ops import tpu_ops\n \n \n class TPUContextTest(test.TestCase):\n@@ -165,6 +166,51 @@ class TPUGraphPruneTest(test.TestCase):\n         graph.get_operation_by_name(\"import/y\").get_attr(\n             tpu._TPU_REPLICATE_ATTR)\n \n+\n+class TPUOpsTest(test.TestCase):\n+\n+  def test_all_to_all_zero_split_count(self):\n+    with self.assertRaisesRegex(\n+        ValueError, \"split_count 0 must at least be one\"):\n+      tpu_ops.all_to_all(\n+          x=[0.0, 0.1652, 0.6543],\n+          group_assignment=[1, -1],\n+          concat_dimension=0,\n+          split_dimension=0,\n+          split_count=0)\n+\n+  def test_all_to_all_group_assignment_wrong_shape(self):\n+    with self.assertRaisesRegex(\n+        ValueError, \"group_assignment must have rank 2\"):\n+      tpu_ops.all_to_all(\n+          x=[0.0, 0.1652, 0.6543],\n+          group_assignment=[1, -1],\n+          concat_dimension=0,\n+          split_dimension=0,\n+          split_count=2)\n+\n+  def test_all_to_all_split_count_not_equal_to_group_assignment_shape(self):\n+    with self.assertRaisesRegex(\n+        ValueError, \"split_count 1 must equal the size of the second dimension \"\n+        \"of group_assignment 2\"):\n+      tpu_ops.all_to_all(\n+          x=[0.0, 0.1652, 0.6543],\n+          group_assignment=[[0, 1], [2, 3]],\n+          concat_dimension=0,\n+          split_dimension=0,\n+          split_count=1)\n+\n+  def test_all_to_all_split_count_not_divide_input_shape(self):\n+    with self.assertRaisesRegex(\n+        ValueError, \"input dimension 3 not divisible by split_count 2\"):\n+      tpu_ops.all_to_all(\n+          x=[[0.0], [0.1652], [0.6543]],\n+          group_assignment=[[0, 1], [2, 3]],\n+          concat_dimension=1,\n+          split_dimension=0,\n+          split_count=2)\n+\n+\n def do_einsum():\n   a = array_ops.placeholder(dtype=dtypes.float32, name=\"a\", shape=[2, 3, 4])\n   b = array_ops.placeholder(dtype=dtypes.float32, name=\"b\", shape=[2, 4, 5])\n"
        ],
        "Title": "\n          Integer division by 0 in `tf.raw_ops.AllToAll`\n        "
    },
    {
        "Bug description": "The  async implementation  of  CollectiveReduceV2  suffers from a memory leak and a use after free:",
        "Sample Code": "tf.raw_ops.CollectiveReduceV2(\n  input=[],\n  group_size=[-10, -10, -10],\n  group_key=[-10, -10],\n  instance_key=[-10],\n  ordering_token=[],\n  merge_op='Mul',\n  ,\n  final_op='Div')",
        "Bug fix": [
            "@@ -494,15 +494,17 @@ class CollectiveOpV2Kernel : public AsyncOpKernel {\n                               const Tensor& group_size, const Tensor& group_key,\n                               const Tensor& instance_key) {\n     if (group_size.dims() > 0) {\n-      return errors::Internal(\"Unexpected dimensions on input group_size, got \",\n-                              group_size.shape().DebugString());\n+      return errors::InvalidArgument(\n+          \"Unexpected dimensions on input group_size, got \",\n+          group_size.shape().DebugString());\n     }\n     if (group_key.dims() > 0) {\n-      return errors::Internal(\"Unexpected dimensions on input group_key, got \",\n-                              group_key.shape().DebugString());\n+      return errors::InvalidArgument(\n+          \"Unexpected dimensions on input group_key, got \",\n+          group_key.shape().DebugString());\n     }\n     if (instance_key.dims() > 0) {\n-      return errors::Internal(\n+      return errors::InvalidArgument(\n           \"Unexpected dimensions on input instance_key, got \",\n           instance_key.shape().DebugString());\n     }\n@@ -625,7 +627,7 @@ class CollectiveReduceV2OpKernel : public CollectiveOpV2Kernel {\n                                               /*group_size*/ c->input(1),\n                                               /*group_key*/ c->input(2),\n                                               /*instance_key*/ c->input(3)),\n-                         done);\n+                         done_with_cleanup);\n     col_params->instance.shape = c->input(0).shape();\n     col_params->merge_op = merge_op_.get();\n     col_params->final_op = final_op_.get();\n@@ -855,14 +857,15 @@ class CollectiveInitializeCommunicatorOpKernel : public AsyncOpKernel {\n \n   Status CheckInputs(Tensor group_size_t, Tensor group_key_t) {\n     if (group_size_t.dims() > 0) {\n-      return errors::Internal(\n+      return errors::InvalidArgument(\n           \"Unexpected dimensions on input group_size. \"\n           \"It shoulbe a scalar, got tensor with shape \",\n           group_size_t.shape().DebugString());\n     }\n     if (group_key_t.dims() > 0) {\n-      return errors::Internal(\"Unexpected dimensions on input group_key, got \",\n-                              group_key_t.shape().DebugString());\n+      return errors::InvalidArgument(\n+          \"Unexpected dimensions on input group_key, got \",\n+          group_key_t.shape().DebugString());\n     }\n \n     auto group_size = group_size_t.unaligned_flat<int32>()(0);\n@@ -1084,7 +1087,7 @@ class CollectiveReduceV3OpKernel : public CollectiveOpV3Kernel {\n     };\n     core::RefCountPtr<CollectiveGroupResource> resource;\n     OP_REQUIRES_OK_ASYNC(c, LookupResource(c, HandleFromInput(c, 1), &resource),\n-                         done);\n+                         done_with_cleanup);\n \n     Tensor group_assignment = c->input(2);\n \n@@ -1134,7 +1137,7 @@ class CollectiveAllToAllV3OpKernel : public CollectiveOpV3Kernel {\n     };\n     core::RefCountPtr<CollectiveGroupResource> resource;\n     OP_REQUIRES_OK_ASYNC(c, LookupResource(c, HandleFromInput(c, 1), &resource),\n-                         done);\n+                         done_with_cleanup);\n \n     Tensor group_assignment = c->input(2);\n \n",
            "@@ -1182,6 +1182,69 @@ class InputPipelineTest(test.TestCase):\n     self.assertAllEqual(self.evaluate(f()), [[3.], [3.]])\n \n \n+@combinations.generate(\n+    combinations.times(\n+        combinations.combine(collective_op=[\n+            combinations.NamedObject('all_reduce_v2',\n+                                     CollectiveOpsV2.all_reduce),\n+            combinations.NamedObject('all_gather_v2',\n+                                     CollectiveOpsV2.all_gather)\n+        ]), device_combination))\n+class InvalidInputTest(test.TestCase, parameterized.TestCase):\n+\n+  def setUp(self):\n+    _setup_context()\n+    super().setUp()\n+\n+  def testInvalidGroupKey(self, collective_op, device, communication):\n+    dev0 = '/device:%s:0' % device\n+    group_size = 2\n+    group_key = [100]\n+    instance_key = 100\n+    in_tensor = constant_op.constant([1.])\n+\n+    with self.assertRaises(errors.InvalidArgumentError):\n+      with ops.device(dev0):\n+        collective_op(\n+            in_tensor,\n+            group_size,\n+            group_key,\n+            instance_key,\n+            communication_hint=communication)\n+\n+  def testInvalidGroupSize(self, collective_op, device, communication):\n+    dev0 = '/device:%s:0' % device\n+    group_size = -2\n+    group_key = 100\n+    instance_key = 100\n+    in_tensor = constant_op.constant([1.])\n+\n+    with self.assertRaises(errors.InvalidArgumentError):\n+      with ops.device(dev0):\n+        collective_op(\n+            in_tensor,\n+            group_size,\n+            group_key,\n+            instance_key,\n+            communication_hint=communication)\n+\n+  def testInvalidInstanceKey(self, collective_op, device, communication):\n+    dev0 = '/device:%s:0' % device\n+    group_size = 2\n+    group_key = 100\n+    instance_key = [100]\n+    in_tensor = constant_op.constant([1.])\n+\n+    with self.assertRaises(errors.InvalidArgumentError):\n+      with ops.device(dev0):\n+        collective_op(\n+            in_tensor,\n+            group_size,\n+            group_key,\n+            instance_key,\n+            communication_hint=communication)\n+\n+\n class CollectiveOpsV3Test(test.TestCase, parameterized.TestCase):\n \n   def setUp(self):\n"
        ],
        "Title": "\n          Use after free / memory leak in `CollectiveReduceV2`\n        "
    },
    {
        "Bug description": "The  code for sparse matrix multiplication  is vulnerable to undefined behavior via binding a reference to  nullptr :",
        "Sample Code": "tf.raw_ops.SparseMatMul(\n  a=[[1.0,1.0,1.0]],\n  b=[[],[],[]],\n  transpose_a=False,\n  transpose_b=False,\n  a_is_sparse=False, \n  , \n  b_is_sparse=True)",
        "Bug fix": [
            "@@ -32,6 +32,7 @@ limitations under the License.\n #include \"tensorflow/core/kernels/fill_functor.h\"\n #include \"tensorflow/core/lib/core/blocking_counter.h\"\n #include \"tensorflow/core/lib/core/threadpool.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/platform/logging.h\"\n #include \"tensorflow/core/platform/macros.h\"\n #include \"tensorflow/core/platform/mutex.h\"\n@@ -980,9 +981,18 @@ class SparseMatMulOp : public OpKernel {\n                 errors::InvalidArgument(\n                     \"Matrix size incompatible: a: \", a.shape().DebugString(),\n                     \", b: \", b.shape().DebugString()));\n+    OP_REQUIRES(ctx, m >= 0 && n >= 0 && k >= 0,\n+                errors::InvalidArgument(\n+                    \"Matrix dimensions cannot be negative: a: \",\n+                    a.shape().DebugString(), \", b: \", b.shape().DebugString()));\n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({m, n}), &output));\n \n+    // Return early if at least one of the output dimension size is 0.\n+    if (m == 0 || n == 0) {\n+      return;\n+    }\n+\n     if (k == 0) {\n       // If the inner dimension k in the matrix multiplication is zero, we fill\n       // the output with zeros.\n"
        ],
        "Title": "\n          Undefined behavior via `nullptr` reference binding in sparse matrix multiplication\n        "
    },
    {
        "Bug description": "The  shape inference function for   is vulnerable to a heap buffer overflow:",
        "Sample Code": "@\ndef test():\n  y = tf.raw_ops.Transpose(x=[1,2,3,4],perm=[-10])\n  return y\n\n\n\ntest()",
        "Bug fix": [
            "@@ -168,7 +168,7 @@ Status TransposeShapeFn(InferenceContext* c) {\n \n     for (int32_t i = 0; i < rank; ++i) {\n       int64_t in_idx = data[i];\n-      if (in_idx >= rank) {\n+      if (in_idx >= rank || in_idx <= -rank) {\n         return errors::InvalidArgument(\"perm dim \", in_idx,\n                                        \" is out of range of input rank \", rank);\n       }\n"
        ],
        "Title": "\n          Heap buffer overflow in `Transpose`\n        "
    },
    {
        "Bug description": "The  code behind   can be made to deadlock when two  tf.function  decorated Python functions are mutually recursive:",
        "Sample Code": "@               \ndef fun1(num):\n    if num == 1:\n        return\n    print(num)\n    fun2(num-1)\n\n@\ndef fun2(num):\n    if num == 0:\n        return\n    print(num)\n    fun1(num-1)\n\n)\n\nfun1(9)",
        "Bug fix": [
            "@@ -572,7 +572,7 @@ class Function(core.GenericFunction):\n       ValueError: if `input_signature` is not None and the `python_function`'s\n         argspec has keyword arguments.\n     \"\"\"\n-    self._lock = threading.Lock()\n+    self._lock = threading.RLock()\n     self._python_function = python_function\n     self._function_spec = function_lib.FunctionSpec.from_function_and_signature(\n         python_function,\n@@ -613,7 +613,7 @@ class Function(core.GenericFunction):\n   def __setstate__(self, state):\n     \"\"\"Restore from pickled state.\"\"\"\n     self.__dict__ = state\n-    self._lock = threading.Lock()\n+    self._lock = threading.RLock()\n     self._descriptor_cache = weakref.WeakKeyDictionary()\n     self._key_for_call_stats = self._get_key_for_call_stats()\n \n",
            "@@ -25,6 +25,7 @@ from absl.testing import parameterized\n from six.moves import range\n \n from tensorflow.python.autograph.core import converter\n+from tensorflow.python.eager import backprop\n from tensorflow.python.eager import def_function\n from tensorflow.python.eager import lift_to_graph\n from tensorflow.python.framework import constant_op\n@@ -36,6 +37,7 @@ from tensorflow.python.framework import tensor_spec\n from tensorflow.python.framework import test_util\n from tensorflow.python.module import module\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import cond_v2\n from tensorflow.python.ops import control_flow_ops\n from tensorflow.python.ops import math_ops\n from tensorflow.python.ops import random_ops\n@@ -1261,6 +1263,117 @@ class DefFunctionTest(test.TestCase, parameterized.TestCase):\n     self.assertAllEqual(obj2.testDouble.experimental_get_tracing_count(), 3)\n     self.assertAllEqual(obj1.testDouble.experimental_get_tracing_count(), 2)\n \n+  def test_recursive_tf_function(self):\n+\n+    @def_function.function\n+    def recursive_fn(n):\n+      if n > 0:\n+        return recursive_fn(n - 1)\n+      return 1\n+\n+    self.assertEqual(recursive_fn(5).numpy(), 1)\n+\n+  def test_recursive_tf_function_with_gradients(self):\n+\n+    @def_function.function\n+    def recursive_fn(n, x):\n+      if n > 0:\n+        return n * recursive_fn(n - 1, x)\n+      else:\n+        return x\n+\n+    x = variables.Variable(1.0)\n+    with backprop.GradientTape() as tape:\n+      g = recursive_fn(5, x)\n+\n+    dg_dx = tape.gradient(g, x)\n+    self.assertEqual(dg_dx.numpy(), 120)\n+\n+  def test_recursive_python_function(self):\n+\n+    def recursive_py_fn(n):\n+      if n > 0:\n+        return recursive_py_fn(n - 1)\n+      return 1\n+\n+    @def_function.function\n+    def recursive_fn(n):\n+      return recursive_py_fn(n)\n+\n+    self.assertEqual(recursive_fn(5).numpy(), 1)\n+\n+  def test_recursive_python_function_with_gradients(self):\n+\n+    def recursive_py_fn(n, x):\n+      if n > 0:\n+        return n * recursive_py_fn(n - 1, x)\n+      return x\n+\n+    @def_function.function\n+    def recursive_fn(n, x):\n+      return recursive_py_fn(n, x)\n+\n+    x = variables.Variable(1.0)\n+    with backprop.GradientTape() as tape:\n+      g = recursive_fn(5, x)\n+\n+    dg_dx = tape.gradient(g, x)\n+    self.assertEqual(dg_dx.numpy(), 120)\n+\n+  def test_recursive_tf_function_call_each_other(self):\n+\n+    @def_function.function\n+    def recursive_fn1(n):\n+      if n <= 1:\n+        return 1\n+      return recursive_fn2(n - 1)\n+\n+    @def_function.function\n+    def recursive_fn2(n):\n+      if n <= 1:\n+        return 2\n+      return recursive_fn1(n - 1)\n+\n+    self.assertEqual(recursive_fn1(5).numpy(), 1)\n+    self.assertEqual(recursive_fn1(6).numpy(), 2)\n+    self.assertEqual(recursive_fn2(5).numpy(), 2)\n+    self.assertEqual(recursive_fn2(6).numpy(), 1)\n+\n+  def test_recursive_tf_function_call_each_other_with_gradients(self):\n+\n+    @def_function.function\n+    def recursive_fn1(n, x):\n+      if n <= 1:\n+        return x\n+      return n * recursive_fn2(n - 1, x)\n+\n+    @def_function.function\n+    def recursive_fn2(n, x):\n+      if n <= 1:\n+        return 2 * x\n+      return n * recursive_fn1(n - 1, x)\n+\n+    x = variables.Variable(1.0)\n+    with backprop.GradientTape() as tape:\n+      g1 = recursive_fn1(5, x)\n+\n+    dg1_dx = tape.gradient(g1, x)\n+    self.assertEqual(dg1_dx.numpy(), 120)\n+\n+    with backprop.GradientTape() as tape:\n+      g2 = recursive_fn2(5, x)\n+\n+    dg2_dx = tape.gradient(g2, x)\n+    self.assertEqual(dg2_dx.numpy(), 240)\n+\n+  def test_recursive_tf_function_with_cond(self):\n+    @def_function.function(autograph=False)\n+    def recursive_fn(n):\n+      return cond_v2.cond_v2(n > 0, recursive_fn(n - 1), 1)\n+\n+    with self.assertRaises(RecursionError):\n+      recursive_fn(constant_op.constant(5))\n+\n \n if __name__ == '__main__':\n   ops.enable_eager_execution()\n",
            "@@ -3037,7 +3037,7 @@ class Function(object):\n     if self.input_signature is not None:\n       self._hashable_input_signature = hash(self.flat_input_signature)\n \n-    self._lock = threading.Lock()\n+    self._lock = threading.RLock()\n     # _descriptor_cache is a of instance of a class to an instance-specific\n     # `Function`, used to make sure defun-decorated methods create different\n     # functions for each instance.\n"
        ],
        "Title": "\n          Deadlock in mutually recursive `tf.function` objects\n        "
    },
    {
        "Bug description": "The  shape inference code for   can trigger a null pointer dereference:",
        "Sample Code": "dataset = tf.data.Dataset.range(3)\n  \n@                 \ndef test():                  \n  y = tf.raw_ops.DeserializeSparse(\n    serialized_sparse=tf.data.experimental.to_variant(dataset),\n    dtype=tf.int32)\n\n)\n\ntest()",
        "Bug fix": [
            "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/common_shape_fns.h\"\n #include \"tensorflow/core/framework/op.h\"\n #include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n #include \"tensorflow/core/platform/errors.h\"\n \n namespace tensorflow {\n@@ -159,6 +160,8 @@ REGISTER_OP(\"DeserializeSparse\")\n     .Attr(\"Tserialized: {string, variant} = DT_STRING\")\n     .SetShapeFn([](InferenceContext* c) {\n       // serialized sparse is [?, ..., ?, 3] vector.\n+      ShapeHandle unused_shape;\n+      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused_shape));\n       DimensionHandle unused;\n       TF_RETURN_IF_ERROR(c->WithValue(c->Dim(c->input(0), -1), 3, &unused));\n       c->set_output(0, c->Matrix(InferenceContext::kUnknownDim,\n",
            "@@ -16,10 +16,12 @@\n \n import numpy as np\n \n+from tensorflow.python.eager import def_function\n from tensorflow.python.framework import dtypes\n from tensorflow.python.framework import sparse_tensor as sparse_tensor_lib\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import gen_resource_variable_ops\n from tensorflow.python.ops import sparse_ops\n from tensorflow.python.platform import test\n \n@@ -460,6 +462,18 @@ class SerializeSparseTest(test.TestCase):\n     self._testDeserializeFailsInvalidProtoHelper(\n         sparse_ops.serialize_sparse, sparse_ops.deserialize_many_sparse)\n \n+  def testDeserializeInvalidVariant(self):\n+    mu = gen_resource_variable_ops.mutex_v2()\n+    mu_lock = gen_resource_variable_ops.mutex_lock(mutex=mu)\n+\n+    @def_function.function\n+    def f():\n+      return sparse_ops.deserialize_sparse(\n+          serialized_sparse=mu_lock, dtype=dtypes.int32)\n+\n+    with self.assertRaisesRegex(ValueError, r\"Shape must be at least rank 1\"):\n+      f()\n+\n \n if __name__ == \"__main__\":\n   test.main()\n"
        ],
        "Title": "\n          Null pointer exception in `DeserializeSparse`\n        "
    },
    {
        "Bug description": "The  shape inference code for   has an undefined behavior due to binding a reference to  nullptr . In the following scenario, this results in a crash:",
        "Sample Code": "@                 \ndef test():     \n  y = tf.ragged.cross([tf.ragged.constant([['1']]),'2'])\n  return y                   \n                             \n                   \n                             \ntest()        ",
        "Bug fix": [
            "@@ -99,6 +99,13 @@ REGISTER_OP(\"RaggedCross\")\n       int dense_start = num_ragged * 2 + num_sparse * 3;\n       for (int i = 0; i < dense_types.size(); ++i) {\n         ShapeHandle dense_input = c->input(i + dense_start);\n+        int32 rank = c->Rank(dense_input);\n+        if (rank == InferenceContext::kUnknownRank) {\n+          continue;\n+        } else if (rank != 2) {\n+          return errors::InvalidArgument(\n+              \"tf.ragged.cross only supports inputs with rank=2\");\n+        }\n         int64_t batch_size = c->Value(c->Dim(dense_input, 0));\n         if (batch_size != InferenceContext::kUnknownDim) {\n           ShapeHandle row_splits = c->Vector(batch_size + 1);\n",
            "@@ -18,10 +18,12 @@ from absl.testing import parameterized\n \n import numpy as np\n \n+from tensorflow.python.eager import def_function\n from tensorflow.python.framework import dtypes\n from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import sparse_tensor\n+from tensorflow.python.framework import tensor_spec\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import sparse_ops\n from tensorflow.python.ops.ragged import ragged_array_ops\n@@ -358,6 +360,16 @@ class RaggedCrossOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n                   dense_const([[2], [3]])],\n           exception=(ValueError, errors.InvalidArgumentError),\n           message='inputs must all have the same batch dimension size'),\n+      dict(\n+          testcase_name='3DDenseTensor',\n+          inputs=[dense_const([[[1]]])],\n+          exception=(ValueError, errors.InvalidArgumentError),\n+          message='tf.ragged.cross only supports inputs with rank=2'),\n+      dict(\n+          testcase_name='0DDenseTensor',\n+          inputs=[dense_const(1)],\n+          exception=(ValueError, errors.InvalidArgumentError),\n+          message='tf.ragged.cross only supports inputs with rank=2'),\n   ])\n   def testStaticError(self, inputs, exception=ValueError, message=None):\n     with self.assertRaisesRegex(exception, message):\n@@ -368,17 +380,36 @@ class RaggedCrossOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n           testcase_name='3DRaggedTensor',\n           inputs=[ragged_const([[[1]]], ragged_rank=1)],\n           message='tf.ragged.cross only supports inputs with rank=2'),\n+      dict(\n+          testcase_name='0DDenseTensor',\n+          inputs=[dense_const(1)],\n+          signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\n+          exception=(ValueError, errors.InvalidArgumentError),\n+          message='tf.ragged.cross only supports inputs with rank=2'),\n+      dict(\n+          testcase_name='1DDenseTensor',\n+          inputs=[dense_const([1])],\n+          signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\n+          exception=(ValueError, errors.InvalidArgumentError),\n+          message='tf.ragged.cross only supports inputs with rank=2'),\n       dict(\n           testcase_name='3DDenseTensor',\n           inputs=[dense_const([[[1]]])],\n+          signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\n+          exception=(ValueError, errors.InvalidArgumentError),\n           message='tf.ragged.cross only supports inputs with rank=2'),\n   ])\n   def testRuntimeError(self,\n                        inputs,\n                        exception=errors.InvalidArgumentError,\n-                       message=None):\n+                       message=None,\n+                       signature=None):\n+    @def_function.function(input_signature=signature)\n+    def fn(x):\n+      return ragged_array_ops.cross(x)\n+\n     with self.assertRaisesRegex(exception, message):\n-      self.evaluate(ragged_array_ops.cross(inputs))\n+      self.evaluate(fn(inputs))\n \n   def _ragged_to_sparse(self, t):\n     if ragged_tensor.is_ragged(t):\n"
        ],
        "Title": "\n          Reference binding to `nullptr` in `tf.ragged.cross`\n        "
    },
    {
        "Bug description": "The  shape inference code for   can trigger a read outside of bounds of heap allocated array:",
        "Sample Code": "@\ndef test():\n  y = tf.raw_ops.RaggedCross(ragged_values=[],\n                             ragged_row_splits=[],\n                             sparse_indices=[[5]],\n                             sparse_values=[],\n                             sparse_shape=[5],\n                             dense_inputs=[['a']],\n                             input_order='RD',\n                             hashed_output=False,\n                             num_buckets=5,\n                             hash_key=2,\n                             out_values_type=tf.string,\n                             out_row_splits_type=tf.int64)\n  return y\n\n\n\ntest()",
        "Bug fix": [
            "@@ -99,6 +99,13 @@ REGISTER_OP(\"RaggedCross\")\n       int dense_start = num_ragged * 2 + num_sparse * 3;\n       for (int i = 0; i < dense_types.size(); ++i) {\n         ShapeHandle dense_input = c->input(i + dense_start);\n+        int32 rank = c->Rank(dense_input);\n+        if (rank == InferenceContext::kUnknownRank) {\n+          continue;\n+        } else if (rank != 2) {\n+          return errors::InvalidArgument(\n+              \"tf.ragged.cross only supports inputs with rank=2\");\n+        }\n         int64_t batch_size = c->Value(c->Dim(dense_input, 0));\n         if (batch_size != InferenceContext::kUnknownDim) {\n           ShapeHandle row_splits = c->Vector(batch_size + 1);\n",
            "@@ -18,10 +18,12 @@ from absl.testing import parameterized\n \n import numpy as np\n \n+from tensorflow.python.eager import def_function\n from tensorflow.python.framework import dtypes\n from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import sparse_tensor\n+from tensorflow.python.framework import tensor_spec\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import sparse_ops\n from tensorflow.python.ops.ragged import ragged_array_ops\n@@ -358,6 +360,16 @@ class RaggedCrossOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n                   dense_const([[2], [3]])],\n           exception=(ValueError, errors.InvalidArgumentError),\n           message='inputs must all have the same batch dimension size'),\n+      dict(\n+          testcase_name='3DDenseTensor',\n+          inputs=[dense_const([[[1]]])],\n+          exception=(ValueError, errors.InvalidArgumentError),\n+          message='tf.ragged.cross only supports inputs with rank=2'),\n+      dict(\n+          testcase_name='0DDenseTensor',\n+          inputs=[dense_const(1)],\n+          exception=(ValueError, errors.InvalidArgumentError),\n+          message='tf.ragged.cross only supports inputs with rank=2'),\n   ])\n   def testStaticError(self, inputs, exception=ValueError, message=None):\n     with self.assertRaisesRegex(exception, message):\n@@ -368,17 +380,36 @@ class RaggedCrossOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n           testcase_name='3DRaggedTensor',\n           inputs=[ragged_const([[[1]]], ragged_rank=1)],\n           message='tf.ragged.cross only supports inputs with rank=2'),\n+      dict(\n+          testcase_name='0DDenseTensor',\n+          inputs=[dense_const(1)],\n+          signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\n+          exception=(ValueError, errors.InvalidArgumentError),\n+          message='tf.ragged.cross only supports inputs with rank=2'),\n+      dict(\n+          testcase_name='1DDenseTensor',\n+          inputs=[dense_const([1])],\n+          signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\n+          exception=(ValueError, errors.InvalidArgumentError),\n+          message='tf.ragged.cross only supports inputs with rank=2'),\n       dict(\n           testcase_name='3DDenseTensor',\n           inputs=[dense_const([[[1]]])],\n+          signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\n+          exception=(ValueError, errors.InvalidArgumentError),\n           message='tf.ragged.cross only supports inputs with rank=2'),\n   ])\n   def testRuntimeError(self,\n                        inputs,\n                        exception=errors.InvalidArgumentError,\n-                       message=None):\n+                       message=None,\n+                       signature=None):\n+    @def_function.function(input_signature=signature)\n+    def fn(x):\n+      return ragged_array_ops.cross(x)\n+\n     with self.assertRaisesRegex(exception, message):\n-      self.evaluate(ragged_array_ops.cross(inputs))\n+      self.evaluate(fn(inputs))\n \n   def _ragged_to_sparse(self, t):\n     if ragged_tensor.is_ragged(t):\n"
        ],
        "Title": "\n          Heap OOB read in `tf.ragged.cross`\n        "
    },
    {
        "Bug description": "The  shape inference code for   can trigger a read outside of bounds of heap allocated array:",
        "Sample Code": "@\ndef test():\n  data=tf.raw_ops.QuantizeV2(\n    input=[1.0,1.0],\n    min_range=[1.0,10.0],\n    max_range=[1.0,10.0],\n    T=tf.qint32,\n    mode='MIN_COMBINED',\n    round_mode='HALF_TO_EVEN',\n    narrow_range=False,\n    axis=-100,\n    ensure_minimum_range=10)\n  return data\n\n\n\ntest()",
        "Bug fix": [
            "@@ -2559,6 +2559,9 @@ Status QuantizeV2Shape(InferenceContext* c) {\n   if (!s.ok() && s.code() != error::NOT_FOUND) {\n     return s;\n   }\n+  if (axis < -1) {\n+    return errors::InvalidArgument(\"axis should be at least -1, got \", axis);\n+  }\n   const int minmax_rank = (axis == -1) ? 0 : 1;\n   TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n   ShapeHandle minmax;\n"
        ],
        "Title": "\n          Heap OOB in shape inference for `QuantizeV2`\n        "
    },
    {
        "Bug description": "The  shape inference functions for the   can trigger a read outside of bounds of heap allocated array as illustrated in the following sets of PoCs:",
        "Sample Code": "@\ndef test():\n  data=tf.raw_ops.QuantizeAndDequantizeV2(\n    input=[1.0,1.0],\n    input_min=[1.0,10.0],\n    input_max=[1.0,10.0],\n    signed_input=False,\n    num_bits=10,\n    range_given=False,\n    round_mode='HALF_TO_EVEN',\n    narrow_range=False,\n    axis=-100)\n  return data\n\n\n\ntest()",
        "Bug fix": [
            "@@ -2863,7 +2863,10 @@ REGISTER_OP(\"QuantizeAndDequantizeV2\")\n       ShapeHandle minmax;\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(1), minmax_rank, &minmax));\n       TF_RETURN_IF_ERROR(c->Merge(c->input(2), minmax, &minmax));\n-      if (axis != -1) {\n+      if (axis < -1) {\n+        return errors::InvalidArgument(\"axis should be at least -1, got \",\n+                                       axis);\n+      } else if (axis != -1) {\n         ShapeHandle input;\n         TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n         DimensionHandle depth;\n@@ -2895,7 +2898,10 @@ REGISTER_OP(\"QuantizeAndDequantizeV4\")\n       ShapeHandle minmax;\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(1), minmax_rank, &minmax));\n       TF_RETURN_IF_ERROR(c->Merge(c->input(2), minmax, &minmax));\n-      if (axis != -1) {\n+      if (axis < -1) {\n+        return errors::InvalidArgument(\"axis should be at least -1, got \",\n+                                       axis);\n+      } else if (axis != -1) {\n         ShapeHandle input;\n         TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n         DimensionHandle depth;\n@@ -2923,7 +2929,10 @@ REGISTER_OP(\"QuantizeAndDequantizeV4Grad\")\n       ShapeHandle minmax;\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(2), minmax_rank, &minmax));\n       TF_RETURN_IF_ERROR(c->Merge(c->input(3), minmax, &minmax));\n-      if (axis != -1) {\n+      if (axis < -1) {\n+        return errors::InvalidArgument(\"axis should be at least -1, got \",\n+                                       axis);\n+      } else if (axis != -1) {\n         ShapeHandle input;\n         TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n         DimensionHandle depth;\n@@ -2956,7 +2965,10 @@ REGISTER_OP(\"QuantizeAndDequantizeV3\")\n       ShapeHandle minmax;\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(1), minmax_rank, &minmax));\n       TF_RETURN_IF_ERROR(c->Merge(c->input(2), minmax, &minmax));\n-      if (axis != -1) {\n+      if (axis < -1) {\n+        return errors::InvalidArgument(\"axis should be at least -1, got \",\n+                                       axis);\n+      } else if (axis != -1) {\n         ShapeHandle input;\n         TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), axis + 1, &input));\n         DimensionHandle depth;\n",
            "@@ -1374,6 +1374,8 @@ TEST(ArrayOpsTest, QuantizeAndDequantizeV2_ShapeFn) {\n   INFER_ERROR(\"Shapes must be equal rank, but are 1 and 0\", op,\n               \"[1,2,?,4,5];[];[1]\");\n   INFER_ERROR(\"Shape must be rank 0 but is rank 1\", op, \"[1,2,?,4,5];[1];[1]\");\n+  (*op.node_def.mutable_attr())[\"axis\"].set_i(-2);\n+  INFER_ERROR(\"axis should be at least -1, got -2\", op, \"?;?;?\");\n }\n \n TEST(ArrayOpsTest, SpaceToBatch_ShapeFn) {\n"
        ],
        "Title": "\n          Heap OOB read in all `tf.raw_ops.QuantizeAndDequantizeV*` ops\n        "
    }
]
[
    {
        "Bug description": "The  implementation of   misses some input validation and can produce a division by 0:",
        "Sample Code": "@\ndef test():\n  y = tf.raw_ops.ParallelConcat(values=[['tf']],shape=0)\n  return y\n\n\n\ntest()",
        "Bug fix": [
            "@@ -71,6 +71,15 @@ class ParallelConcatUpdate : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     auto value = ctx->input(0);\n+    // Value should be at least rank 1. Also the 0th dimension should be\n+    // at least loc_.\n+    OP_REQUIRES(ctx, value.dims() >= 1,\n+                errors::InvalidArgument(\"value should be at least rank 1.\"));\n+    OP_REQUIRES(\n+        ctx, value.dim_size(0) > loc_,\n+        errors::InvalidArgument(\"0th dimension of value = \", value.dim_size(0),\n+                                \" is less than loc_=\", loc_));\n+\n     auto update = ctx->input(1);\n \n     OP_REQUIRES(\n",
            "@@ -16,12 +16,16 @@\n \n import numpy as np\n \n+from tensorflow.python import tf2\n from tensorflow.python.eager import context\n+from tensorflow.python.eager import def_function\n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import gen_array_ops\n from tensorflow.python.ops import gradient_checker_v2\n from tensorflow.python.platform import test\n \n@@ -69,6 +73,19 @@ class StackOpTest(test.TestCase):\n             c = array_ops.parallel_stack(xs)\n             self.assertAllEqual(c, data)\n \n+  def testParallelConcatShapeZero(self):\n+    if not tf2.enabled():\n+      self.skipTest(\"only fails in TF2\")\n+\n+    @def_function.function\n+    def f():\n+      y = gen_array_ops.parallel_concat(values=[[\"tf\"]], shape=0)\n+      return y\n+\n+    with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                r\"0th dimension of value .* is less than\"):\n+      f()\n+\n   def testSimpleParallelGPU(self):\n     # tf.parallel_stack is only supported in graph mode.\n     with ops.Graph().as_default():\n"
        ],
        "Title": "\n          FPE in `ParallelConcat`\n        "
    },
    {
        "Bug description": "The  implementations for convolution operators  trigger a division by 0 if passed empty filter tensor arguments.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -71,6 +71,15 @@ class ParallelConcatUpdate : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     auto value = ctx->input(0);\n+    // Value should be at least rank 1. Also the 0th dimension should be\n+    // at least loc_.\n+    OP_REQUIRES(ctx, value.dims() >= 1,\n+                errors::InvalidArgument(\"value should be at least rank 1.\"));\n+    OP_REQUIRES(\n+        ctx, value.dim_size(0) > loc_,\n+        errors::InvalidArgument(\"0th dimension of value = \", value.dim_size(0),\n+                                \" is less than loc_=\", loc_));\n+\n     auto update = ctx->input(1);\n \n     OP_REQUIRES(\n",
            "@@ -16,12 +16,16 @@\n \n import numpy as np\n \n+from tensorflow.python import tf2\n from tensorflow.python.eager import context\n+from tensorflow.python.eager import def_function\n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import gen_array_ops\n from tensorflow.python.ops import gradient_checker_v2\n from tensorflow.python.platform import test\n \n@@ -69,6 +73,19 @@ class StackOpTest(test.TestCase):\n             c = array_ops.parallel_stack(xs)\n             self.assertAllEqual(c, data)\n \n+  def testParallelConcatShapeZero(self):\n+    if not tf2.enabled():\n+      self.skipTest(\"only fails in TF2\")\n+\n+    @def_function.function\n+    def f():\n+      y = gen_array_ops.parallel_concat(values=[[\"tf\"]], shape=0)\n+      return y\n+\n+    with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                r\"0th dimension of value .* is less than\"):\n+      f()\n+\n   def testSimpleParallelGPU(self):\n     # tf.parallel_stack is only supported in graph mode.\n     with ops.Graph().as_default():\n"
        ],
        "Title": "\n          FPE in convolutions with zero size filters\n        "
    },
    {
        "Bug description": "The  shape inference functions for   can trigger a read outside of bounds of heap allocated array:",
        "Sample Code": "@\ndef func():\n  return tf.raw_ops.SparseCountSparseOutput(\n    indices=[1],\n    values=[[1]],\n    dense_shape=[10],\n    weights=[],\n    binary_output= True)\n\n)\n\nfunc()",
        "Bug fix": [
            "@@ -41,6 +41,8 @@ Status DenseCountSparseOutputShapeFn(InferenceContext *c) {\n }\n \n Status SparseCountSparseOutputShapeFn(InferenceContext *c) {\n+  ShapeHandle unused;\n+  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &unused));\n   auto rank = c->Dim(c->input(0), 1);\n   auto nvals = c->UnknownDim();\n   c->set_output(0, c->Matrix(nvals, rank));  // out.indices\n",
            "@@ -831,6 +831,25 @@ class TestSparseCountFailureModes(test.TestCase):\n       self.evaluate(bincount_ops.sparse_bincount(x, weights=weights, axis=-1))\n \n \n+class RawOpsHeapOobTest(test.TestCase, parameterized.TestCase):\n+\n+  @test_util.run_v1_only(\"Test security error\")\n+  def testSparseCountSparseOutputBadIndicesShapeTooSmall(self):\n+    indices = [1]\n+    values = [[1]]\n+    weights = []\n+    dense_shape = [10]\n+    with self.assertRaisesRegex(ValueError,\n+                                \"Shape must be rank 2 but is rank 1 for\"):\n+      self.evaluate(\n+          gen_count_ops.SparseCountSparseOutput(\n+              indices=indices,\n+              values=values,\n+              dense_shape=dense_shape,\n+              weights=weights,\n+              binary_output=True))\n+\n+\n @test_util.run_all_in_graph_and_eager_modes\n @test_util.disable_tfrt\n class RawOpsTest(test.TestCase, parameterized.TestCase):\n"
        ],
        "Title": "\n          Heap OOB read in `tf.raw_ops.SparseCountSparseOutput`\n        "
    },
    {
        "Bug description": "The  code for boosted trees in TensorFlow  is still missing validation. As a result, attackers can trigger denial of service (via dereferencing  nullptr s or via  CHECK -failures) as well as abuse undefined behavior (binding references to  nullptr s). An attacker can also read and write from heap buffers, depending on the API that gets used and the arguments that are passed to the call.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -72,7 +72,10 @@ class BoostedTreesCalculateBestGainsPerFeatureOp : public OpKernel {\n                                                 &stats_summary_list));\n     const int64_t num_buckets = stats_summary_list[0].dim_size(1);\n     // Check for single logit: 1 gradient + 1 hessian value.\n-    DCHECK_EQ(stats_summary_list[0].dim_size(2), 2);\n+    OP_REQUIRES(context, stats_summary_list[0].dim_size(2) == 2,\n+                errors::InvalidArgument(\"stats_summary_list[0] must have \"\n+                                        \"exactly 2 dimensions, obtained: \",\n+                                        stats_summary_list[0].dim_size(2)));\n     std::vector<TTypes<float, 3>::ConstTensor> stats_summary;\n     stats_summary.reserve(stats_summary_list.size());\n     for (const auto& tensor : stats_summary_list) {\n@@ -275,8 +278,13 @@ class BoostedTreesCalculateBestFeatureSplitOp : public OpKernel {\n     const int32_t num_buckets = stats_summary_t->dim_size(2) - 1;\n     const int32_t logits_dim = logits_dim_;\n     const int32_t hessian_dim = stats_summary_t->dim_size(3) - logits_dim;\n-    DCHECK_GT(hessian_dim, 0);\n-    DCHECK_LE(hessian_dim, logits_dim * logits_dim);\n+    OP_REQUIRES(context, hessian_dim > 0,\n+                errors::InvalidArgument(\"hessian dim should be < 0, got \",\n+                                        hessian_dim));\n+    OP_REQUIRES(context, hessian_dim <= logits_dim * logits_dim,\n+                errors::InvalidArgument(\n+                    \"hessian dim should be <= \", logits_dim * logits_dim,\n+                    \" but got: \", hessian_dim));\n \n     const Tensor* l1_t;\n     OP_REQUIRES_OK(context, context->input(\"l1\", &l1_t));\n@@ -624,8 +632,13 @@ class BoostedTreesCalculateBestFeatureSplitV2 : public OpKernel {\n     const int32_t logits_dim = logits_dim_;\n     const int32_t hessian_dim =\n         stats_summaries_list[0].dim_size(3) - logits_dim;\n-    DCHECK_GT(hessian_dim, 0);\n-    DCHECK_LE(hessian_dim, logits_dim * logits_dim);\n+    OP_REQUIRES(context, hessian_dim > 0,\n+                errors::InvalidArgument(\"hessian dim should be < 0, got \",\n+                                        hessian_dim));\n+    OP_REQUIRES(context, hessian_dim <= logits_dim * logits_dim,\n+                errors::InvalidArgument(\n+                    \"hessian dim should be <= \", logits_dim * logits_dim,\n+                    \" but got: \", hessian_dim));\n \n     // Vector of stats_summaries; each element is stats for feature of shape\n     // [max_splits, feature_dim, num_buckets, logits_dim + hessian_dim].\n@@ -1002,6 +1015,10 @@ class BoostedTreesSparseCalculateBestFeatureSplitOp : public OpKernel {\n     const Tensor* node_id_range_t;\n     OP_REQUIRES_OK(context, context->input(\"node_id_range\", &node_id_range_t));\n     const auto node_id_range = node_id_range_t->vec<int32>();\n+    OP_REQUIRES(\n+        context, node_id_range.size() == 2,\n+        errors::InvalidArgument(\"node_id_range should have 2 entries, got: \",\n+                                node_id_range.size()));\n     const int32_t node_id_first = node_id_range(0);  // inclusive\n     const int32_t node_id_last = node_id_range(1);   // exclusive\n \n@@ -1075,6 +1092,11 @@ class BoostedTreesSparseCalculateBestFeatureSplitOp : public OpKernel {\n                       \"dims, the last value in stats_summary_shape, which was \",\n                       stats_dims, \". At index (\", idx,\n                       \", 4), stats_summary_indices contains value \", stat_dim));\n+      OP_REQUIRES(context, stat_dim >= 0,\n+                  errors::InvalidArgument(\n+                      \"Stat dim, the sum of logits dim and hessian dim in \"\n+                      \"stats_summary_indices, should be >= 0, which was \",\n+                      stat_dim, \" at index \", idx));\n       std::pair<FeatureMapIterator, bool> const& f_insert_result = f_map.insert(\n           FeatureMapIterator::value_type(feature_dim, BucketMap()));\n       auto& b_map = f_insert_result.first->second;\n@@ -1307,6 +1329,12 @@ class BoostedTreesMakeStatsSummaryOp : public OpKernel {\n     const Tensor* gradients_t;\n     OP_REQUIRES_OK(context, context->input(\"gradients\", &gradients_t));\n     const auto gradients = gradients_t->matrix<float>();\n+    OP_REQUIRES(\n+        context, node_ids.size() == gradients.dimension(0),\n+        errors::InvalidArgument(\n+            \"node_ids size should match 0th dim of gradients. node ids \"\n+            \"size: \",\n+            node_ids.size(), \", gradients dim0: \", gradients.dimension(0)));\n     // hessians\n     const Tensor* hessians_t;\n     OP_REQUIRES_OK(context, context->input(\"hessians\", &hessians_t));\n@@ -1376,6 +1404,13 @@ class BoostedTreesAggregateStatsOp : public OpKernel {\n     OP_REQUIRES_OK(context, context->input(\"gradients\", &gradients_t));\n     const auto gradients = gradients_t->matrix<float>();\n \n+    OP_REQUIRES(\n+        context, node_ids.size() == gradients.dimension(0),\n+        errors::InvalidArgument(\n+            \"node_ids size should match 0th dim of gradients. node ids \"\n+            \"size: \",\n+            node_ids.size(), \", gradients dim0: \", gradients.dimension(0)));\n+\n     // hessians.\n     const Tensor* hessians_t;\n     OP_REQUIRES_OK(context, context->input(\"hessians\", &hessians_t));\n@@ -1406,6 +1441,9 @@ class BoostedTreesAggregateStatsOp : public OpKernel {\n \n     for (int i = 0; i < batch_size; ++i) {\n       const int32_t node = node_ids(i);\n+      OP_REQUIRES(context, node >= 0,\n+                  errors::InvalidArgument(\n+                      \"node_ids \", i, \"th entry should be >=0, got: \", node));\n       for (int feature_dim = 0; feature_dim < feature_dims; ++feature_dim) {\n         const int32_t feature_value = feature(i, feature_dim);\n         const int32_t bucket =\n@@ -1612,7 +1650,12 @@ class BoostedTreesSparseAggregateStatsOp : public OpKernel {\n     const int64_t stats_dims = logits_dims + hessians_dims;\n     const int64_t num_sparse_entries = feature_indices_t->dim_size(0);\n     const int32_t feature_dims = feature_shape(1);\n-    DCHECK_LE(num_sparse_entries, batch_size * feature_dims);\n+    OP_REQUIRES(context, num_sparse_entries <= batch_size * feature_dims,\n+                errors::InvalidArgument(\n+                    \"feature_indices dim0 should be <= gradients dim0 * \"\n+                    \"feature_shape[1]. features_indices dim0: \",\n+                    num_sparse_entries, \" gradients dim0: \", batch_size,\n+                    \", feature_shape[1]: \", feature_dims));\n \n     // Aggregate statistics info to map.\n     StatsPartitionMap stats_map;\n",
            "@@ -17,9 +17,11 @@ import numpy as np\n \n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import boosted_trees_ops\n+from tensorflow.python.ops import gen_boosted_trees_ops\n from tensorflow.python.ops import sparse_ops\n from tensorflow.python.platform import googletest\n \n@@ -1665,6 +1667,199 @@ class StatsOpsTest(test_util.TensorFlowTestCase):\n     \"\"\"Tests numeric precision.\"\"\"\n     self._verify_precision(length=50000000)\n \n+  def testBoostedTreesCalculateBestGainsPerFeatureSecurity(self):\n+    node_id_range = [1, 2]\n+    stats_summary_list = [[[[]]]]\n+    l1 = [1.0]\n+    l2 = [1.0]\n+    tree_complexity = [1.0]\n+    min_node_weight = [1.17]\n+    max_splits = 1\n+    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n+      gen_boosted_trees_ops.boosted_trees_calculate_best_gains_per_feature(\n+          node_id_range=node_id_range,\n+          stats_summary_list=stats_summary_list,\n+          l1=l1,\n+          l2=l2,\n+          tree_complexity=tree_complexity,\n+          min_node_weight=min_node_weight,\n+          max_splits=max_splits)\n+\n+  def testBoostedTreesCalculateBestFeatureSplitSecurity(self):\n+    node_id_range = [1, 2]\n+    stats_summary = [[[[]]]]\n+    split_type = 'equality'\n+    l1 = [1.0]\n+    l2 = [1.0]\n+    tree_complexity = [1.0]\n+    min_node_weight = [1.17]\n+    logits_dimension = 5\n+    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n+      gen_boosted_trees_ops.boosted_trees_calculate_best_feature_split(\n+          node_id_range=node_id_range,\n+          stats_summary=stats_summary,\n+          l1=l1,\n+          l2=l2,\n+          tree_complexity=tree_complexity,\n+          min_node_weight=min_node_weight,\n+          logits_dimension=logits_dimension,\n+          split_type=split_type)\n+\n+  def testBoostedTreesCalculateBestFeatureSplitSecurity2(self):\n+    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n+      gen_boosted_trees_ops.boosted_trees_calculate_best_feature_split(\n+          node_id_range=[0, 8],\n+          stats_summary=[[[[1.0], [2.0], [3.0]]]],\n+          l1=[0.5],\n+          l2=[0.5],\n+          tree_complexity=[0.1],\n+          min_node_weight=[1.0],\n+          logits_dimension=8)\n+\n+  def testBoostedTreesCalculateBestFeatureSplitV2Security(self):\n+    node_id_range = [1, 2]\n+    stats_summaries_list = [[[[[]]]]]\n+    split_types = ['inequality']\n+    candidate_feature_ids = [1, 2, 3, 4]\n+    l1 = [1.0]\n+    l2 = [1.0]\n+    tree_complexity = [1.0]\n+    min_node_weight = [1.17]\n+    logits_dimension = 5\n+    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n+      gen_boosted_trees_ops.boosted_trees_calculate_best_feature_split_v2(\n+          node_id_range=node_id_range,\n+          stats_summaries_list=stats_summaries_list,\n+          split_types=split_types,\n+          candidate_feature_ids=candidate_feature_ids,\n+          l1=l1,\n+          l2=l2,\n+          tree_complexity=tree_complexity,\n+          min_node_weight=min_node_weight,\n+          logits_dimension=logits_dimension)\n+\n+  def testBoostedTreesSparseCalculateBestFeatureSplitSecurity(self):\n+    node_id_range = []\n+    stats_summary_indices = [[]]\n+    stats_summary_values = [1.0]\n+    stats_summary_shape = [1, 1, 1, 1]\n+    l1 = [1.0]\n+    l2 = [1.0]\n+    tree_complexity = [0.5]\n+    min_node_weight = [1.0]\n+    logits_dimension = 3\n+    split_type = 'inequality'\n+    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n+      gen_boosted_trees_ops.boosted_trees_sparse_calculate_best_feature_split(\n+          node_id_range=node_id_range,\n+          stats_summary_indices=stats_summary_indices,\n+          stats_summary_values=stats_summary_values,\n+          stats_summary_shape=stats_summary_shape,\n+          l1=l1,\n+          l2=l2,\n+          tree_complexity=tree_complexity,\n+          min_node_weight=min_node_weight,\n+          logits_dimension=logits_dimension,\n+          split_type=split_type)\n+\n+  def testBoostedTreesSparseCalculateBestFeatureSplitSecurity2(self):\n+    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n+      gen_boosted_trees_ops.boosted_trees_sparse_calculate_best_feature_split(\n+          node_id_range=[0, 1],\n+          stats_summary_indices=[[0, -1, -1, -1], [1, 0, -1, 0], [1, 0, 0, -1]],\n+          stats_summary_values=[0.1, 0.2, 0.3],\n+          stats_summary_shape=[1, 1, 1, 1],\n+          l1=[0.5],\n+          l2=[0.5],\n+          tree_complexity=[0.1],\n+          min_node_weight=[1.0],\n+          logits_dimension=1)\n+\n+  def testBoostedTreesMakeStatsSummarySecurity(self):\n+    node_ids = [1, 2]\n+    gradients = [[]]\n+    hessians = [[0.2], [0.1]]\n+    bucketized_features_list = [[1], [2]]\n+    max_splits = 3\n+    num_buckets = 3\n+    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n+      gen_boosted_trees_ops.boosted_trees_make_stats_summary(\n+          node_ids=node_ids,\n+          gradients=gradients,\n+          hessians=hessians,\n+          bucketized_features_list=bucketized_features_list,\n+          max_splits=max_splits,\n+          num_buckets=num_buckets)\n+\n+  def testBoostedTreesMakeStatsSummarySecurity2(self):\n+    node_ids = [1, 2, 3]\n+    gradients = [[0.1], [0.2]]\n+    hessians = [[0.2], [0.1]]\n+    bucketized_features_list = [[1], [2]]\n+    max_splits = 3\n+    num_buckets = 3\n+    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n+      gen_boosted_trees_ops.boosted_trees_make_stats_summary(\n+          node_ids=node_ids,\n+          gradients=gradients,\n+          hessians=hessians,\n+          bucketized_features_list=bucketized_features_list,\n+          max_splits=max_splits,\n+          num_buckets=num_buckets)\n+\n+  def testBoostedTreesAggregateStatsSecurity(self):\n+    node_ids = [1, 2]\n+    gradients = [[]]\n+    hessians = [[100.0]]\n+    feature = [[0, 0, 0]]\n+    max_splits = 100\n+    num_buckets = 100\n+    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n+      gen_boosted_trees_ops.boosted_trees_aggregate_stats(\n+          node_ids=node_ids,\n+          gradients=gradients,\n+          hessians=hessians,\n+          feature=feature,\n+          max_splits=max_splits,\n+          num_buckets=num_buckets)\n+\n+  def testBoostedTreesAggregateStatsSecurity2(self):\n+    node_ids = [-10]\n+    gradients = [[0.0, 0.0]]\n+    hessians = [[100.0]]\n+    feature = [[0, 0, 0]]\n+    max_splits = 100\n+    num_buckets = 100\n+    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n+      self.evaluate(\n+          gen_boosted_trees_ops.boosted_trees_aggregate_stats(\n+              node_ids=node_ids,\n+              gradients=gradients,\n+              hessians=hessians,\n+              feature=feature,\n+              max_splits=max_splits,\n+              num_buckets=num_buckets))\n+\n+  def testBoostedTreesSparseAggregateStatsSecurity(self):\n+    node_ids = []\n+    gradients = [[1.0]]\n+    hessians = [[100.0]]\n+    feature_indices = [[0, 0, 0]]\n+    feature_values = [0, 0, 0]\n+    feature_shape = [0, 0, 0]\n+    max_splits = 100\n+    num_buckets = 100\n+    with self.assertRaises((errors.InvalidArgumentError, ValueError)):\n+      gen_boosted_trees_ops.boosted_trees_sparse_aggregate_stats(\n+          node_ids=node_ids,\n+          gradients=gradients,\n+          hessians=hessians,\n+          feature_indices=feature_indices,\n+          feature_values=feature_values,\n+          feature_shape=feature_shape,\n+          max_splits=max_splits,\n+          num_buckets=num_buckets)\n+\n \n class BestMultiDimFeatureSplitMultiClassV2Op(StatsOpsTest):\n   \"\"\"Tests multi-class/multi-regression for best splits using V2 op.\"\"\"\n"
        ],
        "Title": "\n          Incomplete validation in boosted trees code\n        "
    },
    {
        "Bug description": "Several TensorFlow operations are missing validation for the shapes of the tensor arguments involved in the call. Depending on the API, this can result in undefined behavior and segfault or  CHECK -fail related crashes but in some scenarios writes and reads from heap populated arrays are also possible.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -66,6 +66,12 @@ class TridiagonalMatMulOpGpu : public OpKernel {\n     const Tensor& rhs = context->input(3);\n \n     const int ndims = rhs.dims();\n+    OP_REQUIRES(\n+        context, ndims >= 2,\n+        errors::InvalidArgument(\"Input must have rank >= 2, but got \", ndims));\n+    OP_REQUIRES_OK(context, ValidateInputTensor(superdiag, \"superdiag\", rhs));\n+    OP_REQUIRES_OK(context, ValidateInputTensor(maindiag, \"maindiag\", rhs));\n+    OP_REQUIRES_OK(context, ValidateInputTensor(subdiag, \"subdiag\", rhs));\n     int64 batch_size = 1;\n     for (int i = 0; i < ndims - 2; i++) {\n       batch_size *= rhs.dim_size(i);\n@@ -85,6 +91,39 @@ class TridiagonalMatMulOpGpu : public OpKernel {\n         maindiag.flat<Scalar>().data(), subdiag.flat<Scalar>().data(),\n         rhs.flat<Scalar>().data(), output->flat<Scalar>().data()));\n   }\n+\n+ private:\n+  Status ValidateInputTensor(const Tensor& tensor,\n+                             const std::string& tensor_name,\n+                             const Tensor& rhs) {\n+    const int ndims = rhs.dims();\n+    if (tensor.dims() != ndims) {\n+      return errors::InvalidArgument(tensor_name,\n+                                     \" must have same rank as rhs, but got \",\n+                                     tensor.dims(), \" and \", ndims);\n+    }\n+    for (int i = 0; i < ndims - 2; i++) {\n+      if (tensor.dim_size(i) != rhs.dim_size(i)) {\n+        return errors::InvalidArgument(\n+            tensor_name,\n+            \" must have same outer dimensions as rhs, but for index \", i,\n+            \", got \", tensor.dim_size(i), \" and \", rhs.dim_size(i));\n+      }\n+    }\n+    if (tensor.dim_size(ndims - 2) != 1) {\n+      return errors::InvalidArgument(\n+          tensor_name, \"'s second-to-last dimension must be 1, but got \",\n+          tensor.dim_size(ndims - 2));\n+    }\n+    if (tensor.dim_size(ndims - 1) != rhs.dim_size(ndims - 2)) {\n+      return errors::InvalidArgument(tensor_name,\n+                                     \"'s last dimension size must be rhs's \"\n+                                     \"second-to-last dimension size, but got \",\n+                                     tensor.dim_size(ndims - 1), \" and \",\n+                                     rhs.dim_size(ndims - 2));\n+    }\n+    return Status::OK();\n+  }\n };\n \n REGISTER_LINALG_OP_GPU(\"TridiagonalMatMul\", (TridiagonalMatMulOpGpu<float>),\n",
            "@@ -19,12 +19,15 @@ import itertools\n import numpy as np\n \n from tensorflow.python.client import session\n+from tensorflow.python.eager import context\n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors_impl\n from tensorflow.python.framework import ops\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import control_flow_ops\n from tensorflow.python.ops import gradient_checker_v2\n+from tensorflow.python.ops import linalg_ops\n from tensorflow.python.ops import math_ops\n from tensorflow.python.ops import variables\n from tensorflow.python.ops.linalg import linalg_impl\n@@ -175,6 +178,37 @@ class TridiagonalMulOpTest(test.TestCase):\n     rhs = self._randomComplexArray((b, m, n))\n     self._gradientTest(diags, rhs, dtype=dtypes.complex128)\n \n+  def _testErrorWithShapesEager(self, exception_regex, superdiag_shape,\n+                                maindiag_shape, subdiag_shape, rhs_shape):\n+    with context.eager_mode():\n+      superdiag = array_ops.ones(superdiag_shape)\n+      maindiag = array_ops.ones(maindiag_shape)\n+      subdiag = array_ops.ones(subdiag_shape)\n+      rhs = array_ops.ones(rhs_shape)\n+      with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n+                                  exception_regex):\n+        linalg_ops.tridiagonal_mat_mul(superdiag, maindiag, subdiag, rhs)\n+\n+  def testInvalidShapesEagerGpu(self):\n+    if not test.is_gpu_available():\n+      self.skipTest('Test requires GPU')\n+    self._testErrorWithShapesEager('Input must have rank >= 2, but got ',\n+                                   [2], [2], [2], [2])\n+    self._testErrorWithShapesEager(\n+        'superdiag must have same rank as rhs, but got 3 and 2',\n+        [2, 1, 2], [2, 1], [2, 1], [2, 2])\n+    self._testErrorWithShapesEager(\n+        'maindiag must have same outer dimensions as rhs, but for index 0, got '\n+        '3 and 2',\n+        [2, 1, 2], [3, 1, 2], [2, 1, 2], [2, 2, 2])\n+    self._testErrorWithShapesEager(\n+        \"subdiag's second-to-last dimension must be 1, but got 3\",\n+        [2, 1, 2], [2, 1, 2], [2, 3, 2], [2, 2, 2])\n+    self._testErrorWithShapesEager(\n+        \"subdiag's last dimension size must be rhs's second-to-last dimension \"\n+        \"size, but got 3 and 2\",\n+        [2, 1, 2], [2, 1, 2], [2, 1, 3], [2, 2, 2])\n+\n   # Benchmark\n   class TridiagonalMatMulBenchmark(test.Benchmark):\n     sizes = [(100000, 1, 1), (1000000, 1, 1), (10000000, 1, 1), (100000, 10, 1),\n"
        ],
        "Title": "\n          Incomplete validation of shapes in multiple TF ops\n        "
    },
    {
        "Bug description": "During TensorFlow's Grappler optimizer phase, constant folding might attempt to deep copy a resource tensor. This results in a segfault, as these tensors are supposed to not change.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -30,6 +30,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/log_memory.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n #include \"tensorflow/core/graph/algorithm.h\"\n #include \"tensorflow/core/graph/node_builder.h\"\n #include \"tensorflow/core/graph/subgraph.h\"\n@@ -223,7 +224,8 @@ bool IsConstantFoldable(\n     std::unordered_map<const Node*, std::vector<Tensor>>*\n         shape_replacement_map) {\n   if (n->IsConstant()) {\n-    return true;\n+    // Skip constant folding resources as they cannot be deep copied.\n+    return n->output_type(0) != DT_RESOURCE;\n   }\n   if (MaybeReplaceShapeOp(n, shape_map, shape_replacement_map)) {\n     return true;\n"
        ],
        "Title": "\n          Segfault while copying constant resource tensor\n        "
    },
    {
        "Bug description": "During execution,  EinsumHelper::ParseEquation()  is supposed to set the flags in  input_has_ellipsis  vector and  *output_has_ellipsis  boolean to indicate whether there is ellipsis in the corresponding inputs and output.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -153,6 +153,7 @@ struct EinsumHelper {\n     input_has_ellipsis->resize(num_inputs);\n     for (int i = 0; i < num_inputs; ++i) {\n       input_label_counts->at(i).resize(num_labels);\n+      input_has_ellipsis->at(i) = false;\n       for (const int label : input_labels->at(i)) {\n         if (label != kEllipsisLabel)\n           input_label_counts->at(i)[label] += 1;\n@@ -161,6 +162,7 @@ struct EinsumHelper {\n       }\n     }\n     output_label_counts->resize(num_labels);\n+    *output_has_ellipsis = false;\n     for (const int label : *output_labels) {\n       if (label != kEllipsisLabel)\n         output_label_counts->at(label) += 1;\n"
        ],
        "Title": "\n          Unitialized access in `EinsumHelper::ParseEquation`\n        "
    },
    {
        "Bug description": "An attacker can trigger undefined behavior, integer overflows, segfaults and  CHECK -fail crashes if they can change saved checkpoints from outside of TensorFlow.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -168,7 +168,9 @@ void TensorSliceReader::LoadShard(int shard) const {\n                           \"checkpoint\");\n   if (!status_.ok()) return;\n   for (const SavedSliceMeta& ssm : sts.meta().tensor()) {\n-    TensorShape ssm_shape(ssm.shape());\n+    TensorShape ssm_shape;\n+    status_ = TensorShape::BuildTensorShapeBase(ssm.shape(), &ssm_shape);\n+    if (!status_.ok()) return;\n     for (const TensorSliceProto& tsp : ssm.slice()) {\n       TensorSlice ss_slice(tsp);\n       status_ = RegisterTensorSlice(ssm.name(), ssm_shape, ssm.type(), fname,\n",
            "@@ -18,6 +18,7 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n+#include \"tensorflow/core/framework/tensor_shape.pb.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/framework/versions.pb.h\"\n #include \"tensorflow/core/lib/core/status_test_util.h\"\n@@ -410,6 +411,31 @@ TEST(TensorSliceReaderTest, UnsupportedTensorType) {\n   EXPECT_FALSE(reader.GetTensor(\"test\", &tensor).ok());\n }\n \n+TEST(TensorSliceReaderTest, NegativeTensorShapeDimension) {\n+  const string fname =\n+      io::JoinPath(testing::TmpDir(), \"negative_dim_checkpoint\");\n+  TensorSliceWriter writer(fname, CreateTableTensorSliceBuilder);\n+  const int32 data[] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9};\n+  TF_CHECK_OK(writer.Add(\"test\", TensorShape({4, 5}),\n+                         TensorSlice::ParseOrDie(\"0,2:-\"), data));\n+  TF_CHECK_OK(writer.Finish());\n+\n+  MutateSavedTensorSlices(fname, [](SavedTensorSlices sts) {\n+    if (sts.has_meta()) {\n+      for (auto& tensor : *sts.mutable_meta()->mutable_tensor()) {\n+        for (auto& dim : *tensor.mutable_shape()->mutable_dim()) {\n+          dim.set_size(-dim.size());\n+        }\n+      }\n+    }\n+    return sts.SerializeAsString();\n+  });\n+\n+  TensorSliceReader reader(fname, OpenTableTensorSliceReader);\n+  // The negative dimension should cause loading to fail.\n+  EXPECT_FALSE(reader.status().ok());\n+}\n+\n void CachedTensorSliceReaderTesterHelper(\n     const TensorSliceWriter::CreateBuilderFunction& create_function,\n     const TensorSliceReader::OpenTableFunction& open_function) {\n"
        ],
        "Title": "\n          Missing validation during checkpoint loading\n        "
    },
    {
        "Bug description": "While calculating the size of the output within the  tf.range  kernel, there is a conditional statement of type  int64 = condition ? int64 : double . Due to C++ implicit conversion rules, both branches of the condition will be cast to  double  and the result would be truncated before the assignment. This result in overflows:",
        "Sample Code": " tensorflow as tf\n\ntf.range(start=-1e+38, limit=1)",
        "Bug fix": "",
        "Title": "\n          Overflow/crash in `tf.range`\n        "
    },
    {
        "Bug description": "If  tf.image.resize  is called with a large input argument then the TensorFlow process will crash due to a  CHECK -failure caused by an overflow.",
        "Sample Code": "import numpy as np\n\ntf.keras.layers.UpSampling2D(\n  size=1610637938,\n  data_format='channels_first',\n  ,\n  interpolation='bilinear')(np.ones((5,1,1,1)))",
        "Bug fix": "",
        "Title": "\n          Overflow/crash in `tf.image.resize` when size is large\n        "
    }
]
[
    {
        "Bug description": "If  tf.tile  is called with a large input argument then the TensorFlow process will crash due to a  CHECK -failure caused by an overflow.",
        "Sample Code": "import numpy as np\n\ntf.keras.backend.tile(x=np.ones((1,1,1)), n=[100000000,100000000, 100000000])",
        "Bug fix": "",
        "Title": "\n          Overflow/crash in `tf.tile` when tiling tensor is large\n        "
    },
    {
        "Bug description": "If  tf.summary.create_file_writer  is called with non-scalar arguments code crashes due to a  CHECK -fail.",
        "Sample Code": "import numpy as np\n\ntf.summary.create_file_writer(logdir='', flush_millis=np.ones((1,2)))",
        "Bug fix": "",
        "Title": "\n          Incomplete validation in `tf.summary.create_file_writer`\n        "
    },
    {
        "Bug description": "TensorFlow allows tensor to have a large number of dimensions and each dimension can be as large as desired. However, the total number of elements in a tensor must fit within an  int64_t . If an overflow occurs,  MultiplyWithoutOverflow  would return a negative result. In the majority of TensorFlow codebase this then results in a  CHECK -failure. Newer constructs exist which return a  Status  instead of crashing the binary.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Crashes due to overflow and `CHECK`-fail in ops with large tensor shapes\n        "
    },
    {
        "Bug description": "The Keras pooling layers can trigger a segfault if the size of the pool is 0 or if a dimension is negative:",
        "Sample Code": "pool_size = [2, 2, 0]\nlayer = tf.keras.layers.MaxPooling3D(strides=1, pool_size=pool_size)\ninput_tensor = tf.random.uniform([3, 4, 10, 11, 12], dtype=tf.float32)\n)\nres = layer(input_tensor)",
        "Bug fix": "",
        "Title": "\n          Crash in `max_pool3d` when size argument is 0 or negative\n        "
    },
    {
        "Bug description": "The implementation of  tf.math.segment_*  operations results in a  CHECK -fail related abort (and denial of service) if a segment id in  segment_ids  is large.",
        "Sample Code": "tf.math.segment_max(data=np.ones((1,10,1)), segment_ids=[1676240524292489355])\ntf.math.segment_min(data=np.ones((1,10,1)), segment_ids=[1676240524292489355])\ntf.math.segment_mean(data=np.ones((1,10,1)), segment_ids=[1676240524292489355])    \ntf.math.segment_sum(data=np.ones((1,10,1)), segment_ids=[1676240524292489355])\n])\ntf.math.segment_prod(data=np.ones((1,10,1)), segment_ids=[1676240524292489355])",
        "Bug fix": "",
        "Title": "\n          Crash in `tf.math.segment_*` operations\n        "
    },
    {
        "Bug description": "When running shape functions, some functions (such as  MutableHashTableShape ) produce extra output information in the form of a  ShapeAndType  struct. The shapes embedded in this struct are owned by an inference context that is cleaned up almost immediately; if the upstream code attempts to access this shape information, it can trigger a segfault.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -120,9 +120,26 @@ Status ShapeRefiner::InferShapesForFunctionSubNode(\n     TF_RETURN_IF_ERROR(outer_context->MakeShapeFromShapeProto(proto, &handle));\n     outer_context->set_output(index, handle);\n \n-    auto* resource = node_context->input_handle_shapes_and_types(0);\n+    const std::vector<ShapeAndType>* resource =\n+        node_context->input_handle_shapes_and_types(0);\n     if (resource) {\n-      outer_context->set_output_handle_shapes_and_types(index, *resource);\n+      // `ShapesAndType`s contain `ShapeHandle`s.  These `ShapeHandle`s point\n+      // to `Shape`s that are owned by a different inference context too.  We\n+      // need to copy them to the outer context to prevent them from being\n+      // destroyed before they are used.\n+      std::vector<ShapeAndType> copied_shapes_and_types;\n+      for (auto& shape_and_type : *resource) {\n+        ShapeHandle handle;\n+        TensorShapeProto proto;\n+        node_context->ShapeHandleToProto(shape_and_type.shape, &proto);\n+        TF_RETURN_IF_ERROR(\n+            outer_context->MakeShapeFromShapeProto(proto, &handle));\n+        copied_shapes_and_types.push_back(\n+            ShapeAndType(handle, shape_and_type.dtype, shape_and_type.type));\n+      }\n+\n+      outer_context->set_output_handle_shapes_and_types(\n+          index, copied_shapes_and_types);\n     }\n   }\n \n"
        ],
        "Title": "\n          Use after free and segfault in shape inference functions\n        "
    },
    {
        "Bug description": "Under certain conditions, Go code can trigger a segfault in string deallocation.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Segfault on strings tensors with mistmatched dimensions, due to Go code\n        "
    },
    {
        "Bug description": "An attacker can craft a TFLite model that would trigger a division by zero error in LSH   implementation .",
        "Sample Code": "",
        "Bug fix": [
            "@@ -28,7 +28,7 @@ limitations under the License.\n //\n // Input:\n //   Tensor[0]: Hash functions. Dim.size == 2, DataType: Float.\n-//              Tensor[0].Dim[0]: Num of hash functions.\n+//              Tensor[0].Dim[0]: Num of hash functions. Must be at least 1.\n //              Tensor[0].Dim[1]: Num of projected output bits generated by\n //                                each hash function.\n //   In sparse case, Tensor[0].Dim[1] + ceil( log2(Tensor[0].Dim[0] )) <= 32.\n@@ -82,6 +82,7 @@ TfLiteStatus Resize(TfLiteContext* context, TfLiteNode* node) {\n   const TfLiteTensor* input;\n   TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &input));\n   TF_LITE_ENSURE(context, NumDimensions(input) >= 1);\n+  TF_LITE_ENSURE(context, SizeOfDimension(input, 0) >= 1);\n \n   if (NumInputs(node) == 3) {\n     const TfLiteTensor* weight;\n"
        ],
        "Title": "\n          FPE in LSH in TFLite\n        "
    },
    {
        "Bug description": "An attacker can craft a TFLite model that would trigger a null pointer dereference, which would result in a crash and denial of service:",
        "Sample Code": "",
        "Bug fix": [
            "@@ -68,6 +68,9 @@ constexpr char kRelu6[] = \"RELU6\";\n constexpr char kRelu1[] = \"RELU_N1_TO_1\";\n \n bool L2NormalizeReduceAxis(Value sq_op, DenseElementsAttr axis) {\n+  if (axis.getNumElements() == 0) {\n+    return false;\n+  }\n   if (sq_op.getType().cast<ShapedType>().getRank() - 1 ==\n           *axis.getValues<int>().begin() ||\n       *axis.getValues<int>().begin() == -1) {\n"
        ],
        "Title": "\n          Null pointer dereference in TFLite MLIR optimizations\n        "
    },
    {
        "Bug description": "An attacker can craft a TFLite model that would trigger a null pointer dereference, which would result in a crash and denial of service:",
        "Sample Code": "model = tf.keras.models.Sequential()\nmodel.add(tf.keras.Input(shape=(1, 2, 3)))\nmodel.add(tf.keras.layers.Dense(0, activation='relu'))\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\ninterpreter.allocate_tensors()\n\n()\n\ninterpreter.invoke()",
        "Bug fix": [
            "@@ -265,7 +265,7 @@ inline void BinaryBroadcastFiveFold(const ArithmeticParams& unswitched_params,\n       // We have broadcast y2*y3*y4 of input2 data y1 times, and now move on.\n       input2_data_reset = input2_data_ptr;\n     }\n-  } else {\n+  } else if (input1_data_ptr != nullptr) {\n     // Special case of y4 == 1, in which the innermost loop is a single\n     // element and can be combined with the next (y3) as an inner broadcast.\n     //\n"
        ],
        "Title": "\n          Null pointer dereference in TFLite\n        "
    }
]
[
    {
        "Bug description": "TFLite's  GatherNd  does not support negative indices but there are no checks for this situation.",
        "Sample Code": "import numpy as np\ntf.compat.v1.disable_v2_behavior()\n\nparams = tf.compat.v1.placeholder(name=\"params\", dtype=tf.int64, shape=(1,))\nindices = tf.compat.v1.placeholder(name=\"indices\", dtype=tf.int64, shape=())\n\nout = tf.gather(params, indices, name='out')\n\nwith tf.compat.v1.Session() as sess:\n   converter = tf.compat.v1.lite.TFLiteConverter.from_session(sess, [params, indices], [out])\n   tflite_model = converter.convert()\n\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nparams_data = np.reshape(np.array([1], dtype=np.int64), newshape=(1,))\nindices_data = np.reshape(np.array(-10, dtype=np.int64), newshape=())\ninterpreter.set_tensor(input_details[0]['index'], params_data)\ninterpreter.set_tensor(input_details[1]['index'], indices_data)\n\n)\n\ninterpreter.invoke()",
        "Bug fix": [
            "@@ -123,6 +123,17 @@ TfLiteStatus GatherNdString(const TfLiteTensor* params,\n template <typename IndicesT>\n TfLiteStatus EvalGatherNd(TfLiteContext* context, const TfLiteTensor* params,\n                           const TfLiteTensor* indices, TfLiteTensor* output) {\n+  bool indices_has_only_positive_elements = true;\n+  const auto* indices_values = GetTensorData<IndicesT>(indices);\n+  const size_t num_indices = indices->bytes / sizeof(IndicesT);\n+  for (size_t i = 0; i < num_indices; i++) {\n+    if (indices_values[i] < 0) {\n+      indices_has_only_positive_elements = false;\n+      break;\n+    }\n+  }\n+  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\n+\n   switch (params->type) {\n     case kTfLiteFloat32:\n       return GatherNd<float, IndicesT>(params, indices, output);\n"
        ],
        "Title": "\n          Heap OOB in TFLite's `Gather*` implementations\n        "
    },
    {
        "Bug description": "TFLite's  expand_dims.cc  contains a vulnerability which allows reading one element outside of bounds of heap allocated data:",
        "Sample Code": "",
        "Bug fix": [
            "@@ -37,6 +37,7 @@ TfLiteStatus ExpandTensorDim(TfLiteContext* context, const TfLiteTensor& input,\n     axis = input_dims.size + 1 + axis;\n   }\n   TF_LITE_ENSURE(context, axis <= input_dims.size);\n+  TF_LITE_ENSURE(context, axis >= 0);\n \n   TfLiteIntArray* output_dims = TfLiteIntArrayCreate(input_dims.size + 1);\n   for (int i = 0; i < output_dims->size; ++i) {\n"
        ],
        "Title": "\n          Heap OOB in TFLite\n        "
    },
    {
        "Bug description": "The strided slice implementation in TFLite has a logic bug which can allow an attacker to trigger an infinite loop. This arises from newly introduced support for  ellipsis in axis definition :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -40,12 +40,14 @@ void RunOneAveragePoolTest(const PoolParams& params,\n   std::vector<int8> optimized_averagePool_output(buffer_size);\n   std::vector<int8> reference_averagePool_output(buffer_size);\n \n-  reference_integer_ops::AveragePool(params, input_shape, input_data,\n-                                     output_shape,\n-                                     reference_averagePool_output.data());\n-  optimized_integer_ops::AveragePool(params, input_shape, input_data,\n-                                     output_shape,\n-                                     optimized_averagePool_output.data());\n+  bool reference_success = reference_integer_ops::AveragePool(\n+      params, input_shape, input_data, output_shape,\n+      reference_averagePool_output.data());\n+  bool optimized_success = optimized_integer_ops::AveragePool(\n+      params, input_shape, input_data, output_shape,\n+      optimized_averagePool_output.data());\n+  EXPECT_TRUE(reference_success);\n+  EXPECT_TRUE(optimized_success);\n \n   for (int i = 0; i < buffer_size; i++) {\n     EXPECT_TRUE(reference_averagePool_output[i] ==\n",
            "@@ -144,7 +144,7 @@ inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n   }\n }\n \n-inline void AveragePool(const PoolParams& params,\n+inline bool AveragePool(const PoolParams& params,\n                         const RuntimeShape& input_shape, const int8* input_data,\n                         const RuntimeShape& output_shape, int8* output_data) {\n   ruy::profiler::ScopeLabel label(\"AveragePool/8bitWith32bitAccumulator\");\n@@ -192,6 +192,7 @@ inline void AveragePool(const PoolParams& params,\n               std::min(params.filter_height, input_height - in_y_origin);\n           const int filter_count =\n               (filter_x_end - filter_x_start) * (filter_y_end - filter_y_start);\n+          if (filter_count == 0) return false;\n           memset(acc, 0, tranche_depth * sizeof(acc[0]));\n           const int8* input_ptr =\n               input_data + depth_base +\n@@ -267,6 +268,7 @@ inline void AveragePool(const PoolParams& params,\n       }\n     }\n   }\n+  return true;\n }\n \n }  // namespace optimized_integer_ops\n",
            "@@ -3761,7 +3761,7 @@ inline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n                output_data, output_dims);\n }\n \n-inline void AveragePool(const float* input_data, const Dims<4>& input_dims,\n+inline bool AveragePool(const float* input_data, const Dims<4>& input_dims,\n                         int stride_width, int stride_height, int pad_width,\n                         int pad_height, int kwidth, int kheight,\n                         float output_activation_min,\n@@ -3776,35 +3776,37 @@ inline void AveragePool(const float* input_data, const Dims<4>& input_dims,\n   params.padding_values.width = pad_width;\n   params.float_activation_min = output_activation_min;\n   params.float_activation_max = output_activation_max;\n-  AveragePool(params, DimsToShape(input_dims), input_data,\n-              DimsToShape(output_dims), output_data);\n+  return AveragePool(params, DimsToShape(input_dims), input_data,\n+                     DimsToShape(output_dims), output_data);\n }\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const float* input_data, const Dims<4>& input_dims,\n+bool AveragePool(const float* input_data, const Dims<4>& input_dims,\n                  int stride_width, int stride_height, int pad_width,\n                  int pad_height, int kwidth, int kheight, float* output_data,\n                  const Dims<4>& output_dims) {\n   float output_activation_min, output_activation_max;\n   GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n \n-  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n-              pad_height, kwidth, kheight, output_activation_min,\n-              output_activation_max, output_data, output_dims);\n+  return AveragePool(input_data, input_dims, stride_width, stride_height,\n+                     pad_width, pad_height, kwidth, kheight,\n+                     output_activation_min, output_activation_max, output_data,\n+                     output_dims);\n }\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const float* input_data, const Dims<4>& input_dims, int stride,\n+bool AveragePool(const float* input_data, const Dims<4>& input_dims, int stride,\n                  int pad_width, int pad_height, int filter_width,\n                  int filter_height, float* output_data,\n                  const Dims<4>& output_dims) {\n-  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n-                  filter_width, filter_height, output_data, output_dims);\n+  return AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width,\n+                         pad_height, filter_width, filter_height, output_data,\n+                         output_dims);\n }\n \n-inline void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n+inline bool AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                         int stride_width, int stride_height, int pad_width,\n                         int pad_height, int filter_width, int filter_height,\n                         int32 output_activation_min,\n@@ -3819,13 +3821,13 @@ inline void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n   params.padding_values.width = pad_width;\n   params.quantized_activation_min = output_activation_min;\n   params.quantized_activation_max = output_activation_max;\n-  AveragePool(params, DimsToShape(input_dims), input_data,\n-              DimsToShape(output_dims), output_data);\n+  return AveragePool(params, DimsToShape(input_dims), input_data,\n+                     DimsToShape(output_dims), output_data);\n }\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n+bool AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                  int stride_width, int stride_height, int pad_width,\n                  int pad_height, int filter_width, int filter_height,\n                  int32 output_activation_min, int32 output_activation_max,\n@@ -3839,21 +3841,23 @@ void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n     TFLITE_DCHECK_EQ(output_activation_min, 0);\n     TFLITE_DCHECK_EQ(output_activation_max, 255);\n   }\n-  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n-              pad_height, filter_width, filter_height, output_activation_min,\n-              output_activation_max, output_data, output_dims);\n+  return AveragePool(input_data, input_dims, stride_width, stride_height,\n+                     pad_width, pad_height, filter_width, filter_height,\n+                     output_activation_min, output_activation_max, output_data,\n+                     output_dims);\n }\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n+bool AveragePool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n                  int pad_width, int pad_height, int filter_width,\n                  int filter_height, int32 output_activation_min,\n                  int32 output_activation_max, uint8* output_data,\n                  const Dims<4>& output_dims) {\n-  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n-                  filter_width, filter_height, output_activation_min,\n-                  output_activation_max, output_data, output_dims);\n+  return AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width,\n+                         pad_height, filter_width, filter_height,\n+                         output_activation_min, output_activation_max,\n+                         output_data, output_dims);\n }\n \n inline void MaxPool(const float* input_data, const Dims<4>& input_dims,\n",
            "@@ -3172,7 +3172,7 @@ inline int NodeOffset(int b, int h, int w, int height, int width) {\n   return (b * height + h) * width + w;\n }\n \n-inline void AveragePool(const PoolParams& params,\n+inline bool AveragePool(const PoolParams& params,\n                         const RuntimeShape& input_shape,\n                         const float* input_data,\n                         const RuntimeShape& output_shape, float* output_data) {\n@@ -3187,6 +3187,9 @@ inline void AveragePool(const PoolParams& params,\n   const int stride_height = params.stride_height;\n   const int stride_width = params.stride_width;\n \n+  if (stride_height == 0) return false;\n+  if (stride_width == 0) return false;\n+\n   // TODO(benoitjacob) make this a proper reference impl without Eigen!\n   const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);\n   auto out_mat = MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n@@ -3232,9 +3235,11 @@ inline void AveragePool(const PoolParams& params,\n                                                   params.float_activation_min,\n                                                   params.float_activation_max);\n   }\n+\n+  return true;\n }\n \n-inline void AveragePool(const PoolParams& params,\n+inline bool AveragePool(const PoolParams& params,\n                         const RuntimeShape& input_shape,\n                         const uint8* input_data,\n                         const RuntimeShape& output_shape, uint8* output_data) {\n@@ -3283,6 +3288,7 @@ inline void AveragePool(const PoolParams& params,\n               std::min(params.filter_height, input_height - in_y_origin);\n           const int filter_count =\n               (filter_x_end - filter_x_start) * (filter_y_end - filter_y_start);\n+          if (filter_count == 0) return false;\n           memset(acc, 0, tranche_depth * sizeof(acc[0]));\n           const uint8* input_ptr =\n               input_data + depth_base +\n@@ -3369,6 +3375,7 @@ inline void AveragePool(const PoolParams& params,\n       }\n     }\n   }\n+  return true;\n }\n \n inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n",
            "@@ -21,7 +21,7 @@ limitations under the License.\n namespace tflite {\n namespace reference_integer_ops {\n \n-inline void AveragePool(const PoolParams& params,\n+inline bool AveragePool(const PoolParams& params,\n                         const RuntimeShape& input_shape,\n                         const int8_t* input_data,\n                         const RuntimeShape& output_shape, int8_t* output_data) {\n@@ -66,6 +66,7 @@ inline void AveragePool(const PoolParams& params,\n               filter_count++;\n             }\n           }\n+          if (filter_count == 0) return false;\n           // Round to the closest integer value.\n           acc = acc > 0 ? (acc + filter_count / 2) / filter_count\n                         : (acc - filter_count / 2) / filter_count;\n@@ -77,6 +78,7 @@ inline void AveragePool(const PoolParams& params,\n       }\n     }\n   }\n+  return true;\n }\n \n inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n@@ -136,7 +138,7 @@ inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n   }\n }\n \n-inline void AveragePool(const PoolParams& params,\n+inline bool AveragePool(const PoolParams& params,\n                         const RuntimeShape& input_shape,\n                         const int16_t* input_data,\n                         const RuntimeShape& output_shape,\n@@ -182,6 +184,7 @@ inline void AveragePool(const PoolParams& params,\n               filter_count++;\n             }\n           }\n+          if (filter_count == 0) return false;\n           // Round to the closest integer value.\n           acc = acc > 0 ? (acc + filter_count / 2) / filter_count\n                         : (acc - filter_count / 2) / filter_count;\n@@ -193,6 +196,7 @@ inline void AveragePool(const PoolParams& params,\n       }\n     }\n   }\n+  return true;\n }\n \n inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n",
            "@@ -1487,7 +1487,7 @@ void Sub(const T* input1_data, const Dims<4>& input1_dims, const T* input2_data,\n       output_data);\n }\n \n-inline void AveragePool(const float* input_data, const Dims<4>& input_dims,\n+inline bool AveragePool(const float* input_data, const Dims<4>& input_dims,\n                         int stride_width, int stride_height, int pad_width,\n                         int pad_height, int kwidth, int kheight,\n                         float output_activation_min,\n@@ -1502,8 +1502,8 @@ inline void AveragePool(const float* input_data, const Dims<4>& input_dims,\n   params.padding_values.width = pad_width;\n   params.float_activation_min = output_activation_min;\n   params.float_activation_max = output_activation_max;\n-  AveragePool(params, DimsToShape(input_dims), input_data,\n-              DimsToShape(output_dims), output_data);\n+  return AveragePool(params, DimsToShape(input_dims), input_data,\n+                     DimsToShape(output_dims), output_data);\n }\n \n // Transitional version that will be moved shortly to legacy_reference_ops, as\n@@ -1562,29 +1562,31 @@ inline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const float* input_data, const Dims<4>& input_dims,\n+bool AveragePool(const float* input_data, const Dims<4>& input_dims,\n                  int stride_width, int stride_height, int pad_width,\n                  int pad_height, int kwidth, int kheight, float* output_data,\n                  const Dims<4>& output_dims) {\n   float output_activation_min, output_activation_max;\n   GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n \n-  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n-              pad_height, kwidth, kheight, output_activation_min,\n-              output_activation_max, output_data, output_dims);\n+  return AveragePool(input_data, input_dims, stride_width, stride_height,\n+                     pad_width, pad_height, kwidth, kheight,\n+                     output_activation_min, output_activation_max, output_data,\n+                     output_dims);\n }\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const float* input_data, const Dims<4>& input_dims, int stride,\n+bool AveragePool(const float* input_data, const Dims<4>& input_dims, int stride,\n                  int pad_width, int pad_height, int filter_width,\n                  int filter_height, float* output_data,\n                  const Dims<4>& output_dims) {\n-  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n-                  filter_width, filter_height, output_data, output_dims);\n+  return AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width,\n+                         pad_height, filter_width, filter_height, output_data,\n+                         output_dims);\n }\n \n-inline void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n+inline bool AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                         int stride_width, int stride_height, int pad_width,\n                         int pad_height, int filter_width, int filter_height,\n                         int32 output_activation_min,\n@@ -1599,13 +1601,13 @@ inline void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n   params.padding_values.width = pad_width;\n   params.quantized_activation_min = output_activation_min;\n   params.quantized_activation_max = output_activation_max;\n-  AveragePool(params, DimsToShape(input_dims), input_data,\n-              DimsToShape(output_dims), output_data);\n+  return AveragePool(params, DimsToShape(input_dims), input_data,\n+                     DimsToShape(output_dims), output_data);\n }\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n+bool AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                  int stride_width, int stride_height, int pad_width,\n                  int pad_height, int filter_width, int filter_height,\n                  int32 output_activation_min, int32 output_activation_max,\n@@ -1619,21 +1621,23 @@ void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n     TFLITE_DCHECK_EQ(output_activation_min, 0);\n     TFLITE_DCHECK_EQ(output_activation_max, 255);\n   }\n-  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n-              pad_height, filter_width, filter_height, output_activation_min,\n-              output_activation_max, output_data, output_dims);\n+  return AveragePool(input_data, input_dims, stride_width, stride_height,\n+                     pad_width, pad_height, filter_width, filter_height,\n+                     output_activation_min, output_activation_max, output_data,\n+                     output_dims);\n }\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n+bool AveragePool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n                  int pad_width, int pad_height, int filter_width,\n                  int filter_height, int32 output_activation_min,\n                  int32 output_activation_max, uint8* output_data,\n                  const Dims<4>& output_dims) {\n-  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n-                  filter_width, filter_height, output_activation_min,\n-                  output_activation_max, output_data, output_dims);\n+  return AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width,\n+                         pad_height, filter_width, filter_height,\n+                         output_activation_min, output_activation_max,\n+                         output_data, output_dims);\n }\n \n inline void MaxPool(const float* input_data, const Dims<4>& input_dims,\n",
            "@@ -23,7 +23,7 @@ limitations under the License.\n namespace tflite {\n namespace reference_ops {\n \n-inline void AveragePool(const PoolParams& params,\n+inline bool AveragePool(const PoolParams& params,\n                         const RuntimeShape& input_shape,\n                         const float* input_data,\n                         const RuntimeShape& output_shape, float* output_data) {\n@@ -66,6 +66,7 @@ inline void AveragePool(const PoolParams& params,\n               filter_count++;\n             }\n           }\n+          if (filter_count == 0) return false;\n           const float average = total / filter_count;\n           output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n               ActivationFunctionWithMinMax(average, params.float_activation_min,\n@@ -74,9 +75,10 @@ inline void AveragePool(const PoolParams& params,\n       }\n     }\n   }\n+  return true;\n }\n \n-inline void AveragePool(const PoolParams& params,\n+inline bool AveragePool(const PoolParams& params,\n                         const RuntimeShape& input_shape,\n                         const uint8_t* input_data,\n                         const RuntimeShape& output_shape,\n@@ -122,6 +124,7 @@ inline void AveragePool(const PoolParams& params,\n               filter_count++;\n             }\n           }\n+          if (filter_count == 0) return false;\n           acc = (acc + filter_count / 2) / filter_count;\n           acc = std::max(acc, params.quantized_activation_min);\n           acc = std::min(acc, params.quantized_activation_max);\n@@ -131,6 +134,7 @@ inline void AveragePool(const PoolParams& params,\n       }\n     }\n   }\n+  return true;\n }\n \n inline void L2Pool(const PoolParams& params, const RuntimeShape& input_shape,\n",
            "@@ -117,117 +117,126 @@ TfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n template <KernelType kernel_type>\n-void AverageEvalFloat(TfLiteContext* context, TfLiteNode* node,\n-                      TfLitePoolParams* params, OpData* data,\n-                      const TfLiteTensor* input, TfLiteTensor* output) {\n+TfLiteStatus AverageEvalFloat(TfLiteContext* context, TfLiteNode* node,\n+                              TfLitePoolParams* params, OpData* data,\n+                              const TfLiteTensor* input, TfLiteTensor* output) {\n   float activation_min, activation_max;\n   CalculateActivationRange(params->activation, &activation_min,\n                            &activation_max);\n-#define TF_LITE_AVERAGE_POOL(type)                                       \\\n-  tflite::PoolParams op_params;                                          \\\n-  op_params.stride_height = params->stride_height;                       \\\n-  op_params.stride_width = params->stride_width;                         \\\n-  op_params.filter_height = params->filter_height;                       \\\n-  op_params.filter_width = params->filter_width;                         \\\n-  op_params.padding_values.height = data->padding.height;                \\\n-  op_params.padding_values.width = data->padding.width;                  \\\n-  op_params.float_activation_min = activation_min;                       \\\n-  op_params.float_activation_max = activation_max;                       \\\n-  type::AveragePool(op_params, GetTensorShape(input),                    \\\n-                    GetTensorData<float>(input), GetTensorShape(output), \\\n-                    GetTensorData<float>(output))\n+#define TF_LITE_AVERAGE_POOL(type)                                            \\\n+  tflite::PoolParams op_params;                                               \\\n+  op_params.stride_height = params->stride_height;                            \\\n+  op_params.stride_width = params->stride_width;                              \\\n+  op_params.filter_height = params->filter_height;                            \\\n+  op_params.filter_width = params->filter_width;                              \\\n+  op_params.padding_values.height = data->padding.height;                     \\\n+  op_params.padding_values.width = data->padding.width;                       \\\n+  op_params.float_activation_min = activation_min;                            \\\n+  op_params.float_activation_max = activation_max;                            \\\n+  TF_LITE_ENSURE(context, type::AveragePool(op_params, GetTensorShape(input), \\\n+                                            GetTensorData<float>(input),      \\\n+                                            GetTensorShape(output),           \\\n+                                            GetTensorData<float>(output)))\n   if (kernel_type == kReference) {\n     TF_LITE_AVERAGE_POOL(reference_ops);\n   } else {\n     TF_LITE_AVERAGE_POOL(optimized_ops);\n   }\n #undef TF_LITE_AVERAGE_POOL\n+  return kTfLiteOk;\n }\n \n template <KernelType kernel_type>\n-void AverageEvalQuantizedUint8(TfLiteContext* context, TfLiteNode* node,\n-                               TfLitePoolParams* params, OpData* data,\n-                               const TfLiteTensor* input,\n-                               TfLiteTensor* output) {\n+TfLiteStatus AverageEvalQuantizedUint8(TfLiteContext* context, TfLiteNode* node,\n+                                       TfLitePoolParams* params, OpData* data,\n+                                       const TfLiteTensor* input,\n+                                       TfLiteTensor* output) {\n   int32_t activation_min;\n   int32_t activation_max;\n   (void)CalculateActivationRangeQuantized(context, params->activation, output,\n                                           &activation_min, &activation_max);\n-#define TF_LITE_AVERAGE_POOL(type)                                         \\\n-  tflite::PoolParams op_params;                                            \\\n-  op_params.stride_height = params->stride_height;                         \\\n-  op_params.stride_width = params->stride_width;                           \\\n-  op_params.filter_height = params->filter_height;                         \\\n-  op_params.filter_width = params->filter_width;                           \\\n-  op_params.padding_values.height = data->padding.height;                  \\\n-  op_params.padding_values.width = data->padding.width;                    \\\n-  op_params.quantized_activation_min = activation_min;                     \\\n-  op_params.quantized_activation_max = activation_max;                     \\\n-  type::AveragePool(op_params, GetTensorShape(input),                      \\\n-                    GetTensorData<uint8_t>(input), GetTensorShape(output), \\\n-                    GetTensorData<uint8_t>(output))\n+#define TF_LITE_AVERAGE_POOL(type)                                            \\\n+  tflite::PoolParams op_params;                                               \\\n+  op_params.stride_height = params->stride_height;                            \\\n+  op_params.stride_width = params->stride_width;                              \\\n+  op_params.filter_height = params->filter_height;                            \\\n+  op_params.filter_width = params->filter_width;                              \\\n+  op_params.padding_values.height = data->padding.height;                     \\\n+  op_params.padding_values.width = data->padding.width;                       \\\n+  op_params.quantized_activation_min = activation_min;                        \\\n+  op_params.quantized_activation_max = activation_max;                        \\\n+  TF_LITE_ENSURE(context, type::AveragePool(op_params, GetTensorShape(input), \\\n+                                            GetTensorData<uint8_t>(input),    \\\n+                                            GetTensorShape(output),           \\\n+                                            GetTensorData<uint8_t>(output)))\n   if (kernel_type == kReference) {\n     TF_LITE_AVERAGE_POOL(reference_ops);\n   } else {\n     TF_LITE_AVERAGE_POOL(optimized_ops);\n   }\n #undef TF_LITE_AVERAGE_POOL\n+  return kTfLiteOk;\n }\n \n template <KernelType kernel_type>\n-void AverageEvalQuantizedInt8(TfLiteContext* context, TfLiteNode* node,\n-                              TfLitePoolParams* params, OpData* data,\n-                              const TfLiteTensor* input, TfLiteTensor* output) {\n+TfLiteStatus AverageEvalQuantizedInt8(TfLiteContext* context, TfLiteNode* node,\n+                                      TfLitePoolParams* params, OpData* data,\n+                                      const TfLiteTensor* input,\n+                                      TfLiteTensor* output) {\n   int32_t activation_min;\n   int32_t activation_max;\n \n   (void)CalculateActivationRangeQuantized(context, params->activation, output,\n                                           &activation_min, &activation_max);\n-#define TF_LITE_AVERAGE_POOL(type)                                        \\\n-  tflite::PoolParams op_params;                                           \\\n-  op_params.stride_height = params->stride_height;                        \\\n-  op_params.stride_width = params->stride_width;                          \\\n-  op_params.filter_height = params->filter_height;                        \\\n-  op_params.filter_width = params->filter_width;                          \\\n-  op_params.padding_values.height = data->padding.height;                 \\\n-  op_params.padding_values.width = data->padding.width;                   \\\n-  op_params.quantized_activation_min = activation_min;                    \\\n-  op_params.quantized_activation_max = activation_max;                    \\\n-  type::AveragePool(op_params, GetTensorShape(input),                     \\\n-                    GetTensorData<int8_t>(input), GetTensorShape(output), \\\n-                    GetTensorData<int8_t>(output))\n+#define TF_LITE_AVERAGE_POOL(type)                                            \\\n+  tflite::PoolParams op_params;                                               \\\n+  op_params.stride_height = params->stride_height;                            \\\n+  op_params.stride_width = params->stride_width;                              \\\n+  op_params.filter_height = params->filter_height;                            \\\n+  op_params.filter_width = params->filter_width;                              \\\n+  op_params.padding_values.height = data->padding.height;                     \\\n+  op_params.padding_values.width = data->padding.width;                       \\\n+  op_params.quantized_activation_min = activation_min;                        \\\n+  op_params.quantized_activation_max = activation_max;                        \\\n+  TF_LITE_ENSURE(context, type::AveragePool(op_params, GetTensorShape(input), \\\n+                                            GetTensorData<int8_t>(input),     \\\n+                                            GetTensorShape(output),           \\\n+                                            GetTensorData<int8_t>(output)))\n   if (kernel_type == kReference) {\n     TF_LITE_AVERAGE_POOL(reference_integer_ops);\n   } else {\n     TF_LITE_AVERAGE_POOL(optimized_integer_ops);\n   }\n #undef TF_LITE_AVERAGE_POOL\n+  return kTfLiteOk;\n }\n \n template <KernelType kernel_type>\n-void AverageEvalQuantizedInt16(TfLiteContext* context, TfLiteNode* node,\n-                               TfLitePoolParams* params, OpData* data,\n-                               const TfLiteTensor* input,\n-                               TfLiteTensor* output) {\n+TfLiteStatus AverageEvalQuantizedInt16(TfLiteContext* context, TfLiteNode* node,\n+                                       TfLitePoolParams* params, OpData* data,\n+                                       const TfLiteTensor* input,\n+                                       TfLiteTensor* output) {\n   int32_t activation_min;\n   int32_t activation_max;\n   CalculateActivationRangeQuantized(context, params->activation, output,\n                                     &activation_min, &activation_max);\n-#define TF_LITE_AVERAGE_POOL(type)                                         \\\n-  tflite::PoolParams op_params;                                            \\\n-  op_params.stride_height = params->stride_height;                         \\\n-  op_params.stride_width = params->stride_width;                           \\\n-  op_params.filter_height = params->filter_height;                         \\\n-  op_params.filter_width = params->filter_width;                           \\\n-  op_params.padding_values.height = data->padding.height;                  \\\n-  op_params.padding_values.width = data->padding.width;                    \\\n-  op_params.quantized_activation_min = activation_min;                     \\\n-  op_params.quantized_activation_max = activation_max;                     \\\n-  type::AveragePool(op_params, GetTensorShape(input),                      \\\n-                    GetTensorData<int16_t>(input), GetTensorShape(output), \\\n-                    GetTensorData<int16_t>(output))\n+#define TF_LITE_AVERAGE_POOL(type)                                            \\\n+  tflite::PoolParams op_params;                                               \\\n+  op_params.stride_height = params->stride_height;                            \\\n+  op_params.stride_width = params->stride_width;                              \\\n+  op_params.filter_height = params->filter_height;                            \\\n+  op_params.filter_width = params->filter_width;                              \\\n+  op_params.padding_values.height = data->padding.height;                     \\\n+  op_params.padding_values.width = data->padding.width;                       \\\n+  op_params.quantized_activation_min = activation_min;                        \\\n+  op_params.quantized_activation_max = activation_max;                        \\\n+  TF_LITE_ENSURE(context, type::AveragePool(op_params, GetTensorShape(input), \\\n+                                            GetTensorData<int16_t>(input),    \\\n+                                            GetTensorShape(output),           \\\n+                                            GetTensorData<int16_t>(output)))\n   TF_LITE_AVERAGE_POOL(reference_integer_ops);\n #undef TF_LITE_AVERAGE_POOL\n+  return kTfLiteOk;\n }\n \n template <KernelType kernel_type>\n@@ -380,20 +389,17 @@ TfLiteStatus AverageEval(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n   switch (input->type) {  // Already know in/out types are same.\n     case kTfLiteFloat32:\n-      AverageEvalFloat<kernel_type>(context, node, params, data, input, output);\n-      break;\n+      return AverageEvalFloat<kernel_type>(context, node, params, data, input,\n+                                           output);\n     case kTfLiteUInt8:\n-      AverageEvalQuantizedUint8<kernel_type>(context, node, params, data, input,\n-                                             output);\n-      break;\n+      return AverageEvalQuantizedUint8<kernel_type>(context, node, params, data,\n+                                                    input, output);\n     case kTfLiteInt8:\n-      AverageEvalQuantizedInt8<kernel_type>(context, node, params, data, input,\n-                                            output);\n-      break;\n+      return AverageEvalQuantizedInt8<kernel_type>(context, node, params, data,\n+                                                   input, output);\n     case kTfLiteInt16:\n-      AverageEvalQuantizedInt16<kernel_type>(context, node, params, data, input,\n-                                             output);\n-      break;\n+      return AverageEvalQuantizedInt16<kernel_type>(context, node, params, data,\n+                                                    input, output);\n     default:\n       TF_LITE_KERNEL_LOG(context, \"Type %s not currently supported.\",\n                          TfLiteTypeGetName(input->type));\n"
        ],
        "Title": "\n          Infinite loop in TFLite\n        "
    },
    {
        "Bug description": "The implementations of pooling in TFLite are vulnerable to division by 0 errors as there are no checks for divisors not being 0.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -40,12 +40,14 @@ void RunOneAveragePoolTest(const PoolParams& params,\n   std::vector<int8> optimized_averagePool_output(buffer_size);\n   std::vector<int8> reference_averagePool_output(buffer_size);\n \n-  reference_integer_ops::AveragePool(params, input_shape, input_data,\n-                                     output_shape,\n-                                     reference_averagePool_output.data());\n-  optimized_integer_ops::AveragePool(params, input_shape, input_data,\n-                                     output_shape,\n-                                     optimized_averagePool_output.data());\n+  bool reference_success = reference_integer_ops::AveragePool(\n+      params, input_shape, input_data, output_shape,\n+      reference_averagePool_output.data());\n+  bool optimized_success = optimized_integer_ops::AveragePool(\n+      params, input_shape, input_data, output_shape,\n+      optimized_averagePool_output.data());\n+  EXPECT_TRUE(reference_success);\n+  EXPECT_TRUE(optimized_success);\n \n   for (int i = 0; i < buffer_size; i++) {\n     EXPECT_TRUE(reference_averagePool_output[i] ==\n",
            "@@ -144,7 +144,7 @@ inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n   }\n }\n \n-inline void AveragePool(const PoolParams& params,\n+inline bool AveragePool(const PoolParams& params,\n                         const RuntimeShape& input_shape, const int8* input_data,\n                         const RuntimeShape& output_shape, int8* output_data) {\n   ruy::profiler::ScopeLabel label(\"AveragePool/8bitWith32bitAccumulator\");\n@@ -192,6 +192,7 @@ inline void AveragePool(const PoolParams& params,\n               std::min(params.filter_height, input_height - in_y_origin);\n           const int filter_count =\n               (filter_x_end - filter_x_start) * (filter_y_end - filter_y_start);\n+          if (filter_count == 0) return false;\n           memset(acc, 0, tranche_depth * sizeof(acc[0]));\n           const int8* input_ptr =\n               input_data + depth_base +\n@@ -267,6 +268,7 @@ inline void AveragePool(const PoolParams& params,\n       }\n     }\n   }\n+  return true;\n }\n \n }  // namespace optimized_integer_ops\n",
            "@@ -3761,7 +3761,7 @@ inline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n                output_data, output_dims);\n }\n \n-inline void AveragePool(const float* input_data, const Dims<4>& input_dims,\n+inline bool AveragePool(const float* input_data, const Dims<4>& input_dims,\n                         int stride_width, int stride_height, int pad_width,\n                         int pad_height, int kwidth, int kheight,\n                         float output_activation_min,\n@@ -3776,35 +3776,37 @@ inline void AveragePool(const float* input_data, const Dims<4>& input_dims,\n   params.padding_values.width = pad_width;\n   params.float_activation_min = output_activation_min;\n   params.float_activation_max = output_activation_max;\n-  AveragePool(params, DimsToShape(input_dims), input_data,\n-              DimsToShape(output_dims), output_data);\n+  return AveragePool(params, DimsToShape(input_dims), input_data,\n+                     DimsToShape(output_dims), output_data);\n }\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const float* input_data, const Dims<4>& input_dims,\n+bool AveragePool(const float* input_data, const Dims<4>& input_dims,\n                  int stride_width, int stride_height, int pad_width,\n                  int pad_height, int kwidth, int kheight, float* output_data,\n                  const Dims<4>& output_dims) {\n   float output_activation_min, output_activation_max;\n   GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n \n-  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n-              pad_height, kwidth, kheight, output_activation_min,\n-              output_activation_max, output_data, output_dims);\n+  return AveragePool(input_data, input_dims, stride_width, stride_height,\n+                     pad_width, pad_height, kwidth, kheight,\n+                     output_activation_min, output_activation_max, output_data,\n+                     output_dims);\n }\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const float* input_data, const Dims<4>& input_dims, int stride,\n+bool AveragePool(const float* input_data, const Dims<4>& input_dims, int stride,\n                  int pad_width, int pad_height, int filter_width,\n                  int filter_height, float* output_data,\n                  const Dims<4>& output_dims) {\n-  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n-                  filter_width, filter_height, output_data, output_dims);\n+  return AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width,\n+                         pad_height, filter_width, filter_height, output_data,\n+                         output_dims);\n }\n \n-inline void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n+inline bool AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                         int stride_width, int stride_height, int pad_width,\n                         int pad_height, int filter_width, int filter_height,\n                         int32 output_activation_min,\n@@ -3819,13 +3821,13 @@ inline void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n   params.padding_values.width = pad_width;\n   params.quantized_activation_min = output_activation_min;\n   params.quantized_activation_max = output_activation_max;\n-  AveragePool(params, DimsToShape(input_dims), input_data,\n-              DimsToShape(output_dims), output_data);\n+  return AveragePool(params, DimsToShape(input_dims), input_data,\n+                     DimsToShape(output_dims), output_data);\n }\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n+bool AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                  int stride_width, int stride_height, int pad_width,\n                  int pad_height, int filter_width, int filter_height,\n                  int32 output_activation_min, int32 output_activation_max,\n@@ -3839,21 +3841,23 @@ void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n     TFLITE_DCHECK_EQ(output_activation_min, 0);\n     TFLITE_DCHECK_EQ(output_activation_max, 255);\n   }\n-  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n-              pad_height, filter_width, filter_height, output_activation_min,\n-              output_activation_max, output_data, output_dims);\n+  return AveragePool(input_data, input_dims, stride_width, stride_height,\n+                     pad_width, pad_height, filter_width, filter_height,\n+                     output_activation_min, output_activation_max, output_data,\n+                     output_dims);\n }\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n+bool AveragePool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n                  int pad_width, int pad_height, int filter_width,\n                  int filter_height, int32 output_activation_min,\n                  int32 output_activation_max, uint8* output_data,\n                  const Dims<4>& output_dims) {\n-  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n-                  filter_width, filter_height, output_activation_min,\n-                  output_activation_max, output_data, output_dims);\n+  return AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width,\n+                         pad_height, filter_width, filter_height,\n+                         output_activation_min, output_activation_max,\n+                         output_data, output_dims);\n }\n \n inline void MaxPool(const float* input_data, const Dims<4>& input_dims,\n",
            "@@ -3172,7 +3172,7 @@ inline int NodeOffset(int b, int h, int w, int height, int width) {\n   return (b * height + h) * width + w;\n }\n \n-inline void AveragePool(const PoolParams& params,\n+inline bool AveragePool(const PoolParams& params,\n                         const RuntimeShape& input_shape,\n                         const float* input_data,\n                         const RuntimeShape& output_shape, float* output_data) {\n@@ -3187,6 +3187,9 @@ inline void AveragePool(const PoolParams& params,\n   const int stride_height = params.stride_height;\n   const int stride_width = params.stride_width;\n \n+  if (stride_height == 0) return false;\n+  if (stride_width == 0) return false;\n+\n   // TODO(benoitjacob) make this a proper reference impl without Eigen!\n   const auto in_mat = MapAsMatrixWithLastDimAsRows(input_data, input_shape);\n   auto out_mat = MapAsMatrixWithLastDimAsRows(output_data, output_shape);\n@@ -3232,9 +3235,11 @@ inline void AveragePool(const PoolParams& params,\n                                                   params.float_activation_min,\n                                                   params.float_activation_max);\n   }\n+\n+  return true;\n }\n \n-inline void AveragePool(const PoolParams& params,\n+inline bool AveragePool(const PoolParams& params,\n                         const RuntimeShape& input_shape,\n                         const uint8* input_data,\n                         const RuntimeShape& output_shape, uint8* output_data) {\n@@ -3283,6 +3288,7 @@ inline void AveragePool(const PoolParams& params,\n               std::min(params.filter_height, input_height - in_y_origin);\n           const int filter_count =\n               (filter_x_end - filter_x_start) * (filter_y_end - filter_y_start);\n+          if (filter_count == 0) return false;\n           memset(acc, 0, tranche_depth * sizeof(acc[0]));\n           const uint8* input_ptr =\n               input_data + depth_base +\n@@ -3369,6 +3375,7 @@ inline void AveragePool(const PoolParams& params,\n       }\n     }\n   }\n+  return true;\n }\n \n inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n",
            "@@ -21,7 +21,7 @@ limitations under the License.\n namespace tflite {\n namespace reference_integer_ops {\n \n-inline void AveragePool(const PoolParams& params,\n+inline bool AveragePool(const PoolParams& params,\n                         const RuntimeShape& input_shape,\n                         const int8_t* input_data,\n                         const RuntimeShape& output_shape, int8_t* output_data) {\n@@ -66,6 +66,7 @@ inline void AveragePool(const PoolParams& params,\n               filter_count++;\n             }\n           }\n+          if (filter_count == 0) return false;\n           // Round to the closest integer value.\n           acc = acc > 0 ? (acc + filter_count / 2) / filter_count\n                         : (acc - filter_count / 2) / filter_count;\n@@ -77,6 +78,7 @@ inline void AveragePool(const PoolParams& params,\n       }\n     }\n   }\n+  return true;\n }\n \n inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n@@ -136,7 +138,7 @@ inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n   }\n }\n \n-inline void AveragePool(const PoolParams& params,\n+inline bool AveragePool(const PoolParams& params,\n                         const RuntimeShape& input_shape,\n                         const int16_t* input_data,\n                         const RuntimeShape& output_shape,\n@@ -182,6 +184,7 @@ inline void AveragePool(const PoolParams& params,\n               filter_count++;\n             }\n           }\n+          if (filter_count == 0) return false;\n           // Round to the closest integer value.\n           acc = acc > 0 ? (acc + filter_count / 2) / filter_count\n                         : (acc - filter_count / 2) / filter_count;\n@@ -193,6 +196,7 @@ inline void AveragePool(const PoolParams& params,\n       }\n     }\n   }\n+  return true;\n }\n \n inline void MaxPool(const PoolParams& params, const RuntimeShape& input_shape,\n",
            "@@ -1487,7 +1487,7 @@ void Sub(const T* input1_data, const Dims<4>& input1_dims, const T* input2_data,\n       output_data);\n }\n \n-inline void AveragePool(const float* input_data, const Dims<4>& input_dims,\n+inline bool AveragePool(const float* input_data, const Dims<4>& input_dims,\n                         int stride_width, int stride_height, int pad_width,\n                         int pad_height, int kwidth, int kheight,\n                         float output_activation_min,\n@@ -1502,8 +1502,8 @@ inline void AveragePool(const float* input_data, const Dims<4>& input_dims,\n   params.padding_values.width = pad_width;\n   params.float_activation_min = output_activation_min;\n   params.float_activation_max = output_activation_max;\n-  AveragePool(params, DimsToShape(input_dims), input_data,\n-              DimsToShape(output_dims), output_data);\n+  return AveragePool(params, DimsToShape(input_dims), input_data,\n+                     DimsToShape(output_dims), output_data);\n }\n \n // Transitional version that will be moved shortly to legacy_reference_ops, as\n@@ -1562,29 +1562,31 @@ inline void BroadcastMul(const uint8* input1_data, const Dims<4>& input1_dims,\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const float* input_data, const Dims<4>& input_dims,\n+bool AveragePool(const float* input_data, const Dims<4>& input_dims,\n                  int stride_width, int stride_height, int pad_width,\n                  int pad_height, int kwidth, int kheight, float* output_data,\n                  const Dims<4>& output_dims) {\n   float output_activation_min, output_activation_max;\n   GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n \n-  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n-              pad_height, kwidth, kheight, output_activation_min,\n-              output_activation_max, output_data, output_dims);\n+  return AveragePool(input_data, input_dims, stride_width, stride_height,\n+                     pad_width, pad_height, kwidth, kheight,\n+                     output_activation_min, output_activation_max, output_data,\n+                     output_dims);\n }\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const float* input_data, const Dims<4>& input_dims, int stride,\n+bool AveragePool(const float* input_data, const Dims<4>& input_dims, int stride,\n                  int pad_width, int pad_height, int filter_width,\n                  int filter_height, float* output_data,\n                  const Dims<4>& output_dims) {\n-  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n-                  filter_width, filter_height, output_data, output_dims);\n+  return AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width,\n+                         pad_height, filter_width, filter_height, output_data,\n+                         output_dims);\n }\n \n-inline void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n+inline bool AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                         int stride_width, int stride_height, int pad_width,\n                         int pad_height, int filter_width, int filter_height,\n                         int32 output_activation_min,\n@@ -1599,13 +1601,13 @@ inline void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n   params.padding_values.width = pad_width;\n   params.quantized_activation_min = output_activation_min;\n   params.quantized_activation_max = output_activation_max;\n-  AveragePool(params, DimsToShape(input_dims), input_data,\n-              DimsToShape(output_dims), output_data);\n+  return AveragePool(params, DimsToShape(input_dims), input_data,\n+                     DimsToShape(output_dims), output_data);\n }\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n+bool AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n                  int stride_width, int stride_height, int pad_width,\n                  int pad_height, int filter_width, int filter_height,\n                  int32 output_activation_min, int32 output_activation_max,\n@@ -1619,21 +1621,23 @@ void AveragePool(const uint8* input_data, const Dims<4>& input_dims,\n     TFLITE_DCHECK_EQ(output_activation_min, 0);\n     TFLITE_DCHECK_EQ(output_activation_max, 255);\n   }\n-  AveragePool(input_data, input_dims, stride_width, stride_height, pad_width,\n-              pad_height, filter_width, filter_height, output_activation_min,\n-              output_activation_max, output_data, output_dims);\n+  return AveragePool(input_data, input_dims, stride_width, stride_height,\n+                     pad_width, pad_height, filter_width, filter_height,\n+                     output_activation_min, output_activation_max, output_data,\n+                     output_dims);\n }\n \n // legacy, for compatibility with old checked-in code\n template <FusedActivationFunctionType Ac>\n-void AveragePool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n+bool AveragePool(const uint8* input_data, const Dims<4>& input_dims, int stride,\n                  int pad_width, int pad_height, int filter_width,\n                  int filter_height, int32 output_activation_min,\n                  int32 output_activation_max, uint8* output_data,\n                  const Dims<4>& output_dims) {\n-  AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width, pad_height,\n-                  filter_width, filter_height, output_activation_min,\n-                  output_activation_max, output_data, output_dims);\n+  return AveragePool<Ac>(input_data, input_dims, stride, stride, pad_width,\n+                         pad_height, filter_width, filter_height,\n+                         output_activation_min, output_activation_max,\n+                         output_data, output_dims);\n }\n \n inline void MaxPool(const float* input_data, const Dims<4>& input_dims,\n",
            "@@ -23,7 +23,7 @@ limitations under the License.\n namespace tflite {\n namespace reference_ops {\n \n-inline void AveragePool(const PoolParams& params,\n+inline bool AveragePool(const PoolParams& params,\n                         const RuntimeShape& input_shape,\n                         const float* input_data,\n                         const RuntimeShape& output_shape, float* output_data) {\n@@ -66,6 +66,7 @@ inline void AveragePool(const PoolParams& params,\n               filter_count++;\n             }\n           }\n+          if (filter_count == 0) return false;\n           const float average = total / filter_count;\n           output_data[Offset(output_shape, batch, out_y, out_x, channel)] =\n               ActivationFunctionWithMinMax(average, params.float_activation_min,\n@@ -74,9 +75,10 @@ inline void AveragePool(const PoolParams& params,\n       }\n     }\n   }\n+  return true;\n }\n \n-inline void AveragePool(const PoolParams& params,\n+inline bool AveragePool(const PoolParams& params,\n                         const RuntimeShape& input_shape,\n                         const uint8_t* input_data,\n                         const RuntimeShape& output_shape,\n@@ -122,6 +124,7 @@ inline void AveragePool(const PoolParams& params,\n               filter_count++;\n             }\n           }\n+          if (filter_count == 0) return false;\n           acc = (acc + filter_count / 2) / filter_count;\n           acc = std::max(acc, params.quantized_activation_min);\n           acc = std::min(acc, params.quantized_activation_max);\n@@ -131,6 +134,7 @@ inline void AveragePool(const PoolParams& params,\n       }\n     }\n   }\n+  return true;\n }\n \n inline void L2Pool(const PoolParams& params, const RuntimeShape& input_shape,\n",
            "@@ -117,117 +117,126 @@ TfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n template <KernelType kernel_type>\n-void AverageEvalFloat(TfLiteContext* context, TfLiteNode* node,\n-                      TfLitePoolParams* params, OpData* data,\n-                      const TfLiteTensor* input, TfLiteTensor* output) {\n+TfLiteStatus AverageEvalFloat(TfLiteContext* context, TfLiteNode* node,\n+                              TfLitePoolParams* params, OpData* data,\n+                              const TfLiteTensor* input, TfLiteTensor* output) {\n   float activation_min, activation_max;\n   CalculateActivationRange(params->activation, &activation_min,\n                            &activation_max);\n-#define TF_LITE_AVERAGE_POOL(type)                                       \\\n-  tflite::PoolParams op_params;                                          \\\n-  op_params.stride_height = params->stride_height;                       \\\n-  op_params.stride_width = params->stride_width;                         \\\n-  op_params.filter_height = params->filter_height;                       \\\n-  op_params.filter_width = params->filter_width;                         \\\n-  op_params.padding_values.height = data->padding.height;                \\\n-  op_params.padding_values.width = data->padding.width;                  \\\n-  op_params.float_activation_min = activation_min;                       \\\n-  op_params.float_activation_max = activation_max;                       \\\n-  type::AveragePool(op_params, GetTensorShape(input),                    \\\n-                    GetTensorData<float>(input), GetTensorShape(output), \\\n-                    GetTensorData<float>(output))\n+#define TF_LITE_AVERAGE_POOL(type)                                            \\\n+  tflite::PoolParams op_params;                                               \\\n+  op_params.stride_height = params->stride_height;                            \\\n+  op_params.stride_width = params->stride_width;                              \\\n+  op_params.filter_height = params->filter_height;                            \\\n+  op_params.filter_width = params->filter_width;                              \\\n+  op_params.padding_values.height = data->padding.height;                     \\\n+  op_params.padding_values.width = data->padding.width;                       \\\n+  op_params.float_activation_min = activation_min;                            \\\n+  op_params.float_activation_max = activation_max;                            \\\n+  TF_LITE_ENSURE(context, type::AveragePool(op_params, GetTensorShape(input), \\\n+                                            GetTensorData<float>(input),      \\\n+                                            GetTensorShape(output),           \\\n+                                            GetTensorData<float>(output)))\n   if (kernel_type == kReference) {\n     TF_LITE_AVERAGE_POOL(reference_ops);\n   } else {\n     TF_LITE_AVERAGE_POOL(optimized_ops);\n   }\n #undef TF_LITE_AVERAGE_POOL\n+  return kTfLiteOk;\n }\n \n template <KernelType kernel_type>\n-void AverageEvalQuantizedUint8(TfLiteContext* context, TfLiteNode* node,\n-                               TfLitePoolParams* params, OpData* data,\n-                               const TfLiteTensor* input,\n-                               TfLiteTensor* output) {\n+TfLiteStatus AverageEvalQuantizedUint8(TfLiteContext* context, TfLiteNode* node,\n+                                       TfLitePoolParams* params, OpData* data,\n+                                       const TfLiteTensor* input,\n+                                       TfLiteTensor* output) {\n   int32_t activation_min;\n   int32_t activation_max;\n   (void)CalculateActivationRangeQuantized(context, params->activation, output,\n                                           &activation_min, &activation_max);\n-#define TF_LITE_AVERAGE_POOL(type)                                         \\\n-  tflite::PoolParams op_params;                                            \\\n-  op_params.stride_height = params->stride_height;                         \\\n-  op_params.stride_width = params->stride_width;                           \\\n-  op_params.filter_height = params->filter_height;                         \\\n-  op_params.filter_width = params->filter_width;                           \\\n-  op_params.padding_values.height = data->padding.height;                  \\\n-  op_params.padding_values.width = data->padding.width;                    \\\n-  op_params.quantized_activation_min = activation_min;                     \\\n-  op_params.quantized_activation_max = activation_max;                     \\\n-  type::AveragePool(op_params, GetTensorShape(input),                      \\\n-                    GetTensorData<uint8_t>(input), GetTensorShape(output), \\\n-                    GetTensorData<uint8_t>(output))\n+#define TF_LITE_AVERAGE_POOL(type)                                            \\\n+  tflite::PoolParams op_params;                                               \\\n+  op_params.stride_height = params->stride_height;                            \\\n+  op_params.stride_width = params->stride_width;                              \\\n+  op_params.filter_height = params->filter_height;                            \\\n+  op_params.filter_width = params->filter_width;                              \\\n+  op_params.padding_values.height = data->padding.height;                     \\\n+  op_params.padding_values.width = data->padding.width;                       \\\n+  op_params.quantized_activation_min = activation_min;                        \\\n+  op_params.quantized_activation_max = activation_max;                        \\\n+  TF_LITE_ENSURE(context, type::AveragePool(op_params, GetTensorShape(input), \\\n+                                            GetTensorData<uint8_t>(input),    \\\n+                                            GetTensorShape(output),           \\\n+                                            GetTensorData<uint8_t>(output)))\n   if (kernel_type == kReference) {\n     TF_LITE_AVERAGE_POOL(reference_ops);\n   } else {\n     TF_LITE_AVERAGE_POOL(optimized_ops);\n   }\n #undef TF_LITE_AVERAGE_POOL\n+  return kTfLiteOk;\n }\n \n template <KernelType kernel_type>\n-void AverageEvalQuantizedInt8(TfLiteContext* context, TfLiteNode* node,\n-                              TfLitePoolParams* params, OpData* data,\n-                              const TfLiteTensor* input, TfLiteTensor* output) {\n+TfLiteStatus AverageEvalQuantizedInt8(TfLiteContext* context, TfLiteNode* node,\n+                                      TfLitePoolParams* params, OpData* data,\n+                                      const TfLiteTensor* input,\n+                                      TfLiteTensor* output) {\n   int32_t activation_min;\n   int32_t activation_max;\n \n   (void)CalculateActivationRangeQuantized(context, params->activation, output,\n                                           &activation_min, &activation_max);\n-#define TF_LITE_AVERAGE_POOL(type)                                        \\\n-  tflite::PoolParams op_params;                                           \\\n-  op_params.stride_height = params->stride_height;                        \\\n-  op_params.stride_width = params->stride_width;                          \\\n-  op_params.filter_height = params->filter_height;                        \\\n-  op_params.filter_width = params->filter_width;                          \\\n-  op_params.padding_values.height = data->padding.height;                 \\\n-  op_params.padding_values.width = data->padding.width;                   \\\n-  op_params.quantized_activation_min = activation_min;                    \\\n-  op_params.quantized_activation_max = activation_max;                    \\\n-  type::AveragePool(op_params, GetTensorShape(input),                     \\\n-                    GetTensorData<int8_t>(input), GetTensorShape(output), \\\n-                    GetTensorData<int8_t>(output))\n+#define TF_LITE_AVERAGE_POOL(type)                                            \\\n+  tflite::PoolParams op_params;                                               \\\n+  op_params.stride_height = params->stride_height;                            \\\n+  op_params.stride_width = params->stride_width;                              \\\n+  op_params.filter_height = params->filter_height;                            \\\n+  op_params.filter_width = params->filter_width;                              \\\n+  op_params.padding_values.height = data->padding.height;                     \\\n+  op_params.padding_values.width = data->padding.width;                       \\\n+  op_params.quantized_activation_min = activation_min;                        \\\n+  op_params.quantized_activation_max = activation_max;                        \\\n+  TF_LITE_ENSURE(context, type::AveragePool(op_params, GetTensorShape(input), \\\n+                                            GetTensorData<int8_t>(input),     \\\n+                                            GetTensorShape(output),           \\\n+                                            GetTensorData<int8_t>(output)))\n   if (kernel_type == kReference) {\n     TF_LITE_AVERAGE_POOL(reference_integer_ops);\n   } else {\n     TF_LITE_AVERAGE_POOL(optimized_integer_ops);\n   }\n #undef TF_LITE_AVERAGE_POOL\n+  return kTfLiteOk;\n }\n \n template <KernelType kernel_type>\n-void AverageEvalQuantizedInt16(TfLiteContext* context, TfLiteNode* node,\n-                               TfLitePoolParams* params, OpData* data,\n-                               const TfLiteTensor* input,\n-                               TfLiteTensor* output) {\n+TfLiteStatus AverageEvalQuantizedInt16(TfLiteContext* context, TfLiteNode* node,\n+                                       TfLitePoolParams* params, OpData* data,\n+                                       const TfLiteTensor* input,\n+                                       TfLiteTensor* output) {\n   int32_t activation_min;\n   int32_t activation_max;\n   CalculateActivationRangeQuantized(context, params->activation, output,\n                                     &activation_min, &activation_max);\n-#define TF_LITE_AVERAGE_POOL(type)                                         \\\n-  tflite::PoolParams op_params;                                            \\\n-  op_params.stride_height = params->stride_height;                         \\\n-  op_params.stride_width = params->stride_width;                           \\\n-  op_params.filter_height = params->filter_height;                         \\\n-  op_params.filter_width = params->filter_width;                           \\\n-  op_params.padding_values.height = data->padding.height;                  \\\n-  op_params.padding_values.width = data->padding.width;                    \\\n-  op_params.quantized_activation_min = activation_min;                     \\\n-  op_params.quantized_activation_max = activation_max;                     \\\n-  type::AveragePool(op_params, GetTensorShape(input),                      \\\n-                    GetTensorData<int16_t>(input), GetTensorShape(output), \\\n-                    GetTensorData<int16_t>(output))\n+#define TF_LITE_AVERAGE_POOL(type)                                            \\\n+  tflite::PoolParams op_params;                                               \\\n+  op_params.stride_height = params->stride_height;                            \\\n+  op_params.stride_width = params->stride_width;                              \\\n+  op_params.filter_height = params->filter_height;                            \\\n+  op_params.filter_width = params->filter_width;                              \\\n+  op_params.padding_values.height = data->padding.height;                     \\\n+  op_params.padding_values.width = data->padding.width;                       \\\n+  op_params.quantized_activation_min = activation_min;                        \\\n+  op_params.quantized_activation_max = activation_max;                        \\\n+  TF_LITE_ENSURE(context, type::AveragePool(op_params, GetTensorShape(input), \\\n+                                            GetTensorData<int16_t>(input),    \\\n+                                            GetTensorShape(output),           \\\n+                                            GetTensorData<int16_t>(output)))\n   TF_LITE_AVERAGE_POOL(reference_integer_ops);\n #undef TF_LITE_AVERAGE_POOL\n+  return kTfLiteOk;\n }\n \n template <KernelType kernel_type>\n@@ -380,20 +389,17 @@ TfLiteStatus AverageEval(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n   switch (input->type) {  // Already know in/out types are same.\n     case kTfLiteFloat32:\n-      AverageEvalFloat<kernel_type>(context, node, params, data, input, output);\n-      break;\n+      return AverageEvalFloat<kernel_type>(context, node, params, data, input,\n+                                           output);\n     case kTfLiteUInt8:\n-      AverageEvalQuantizedUint8<kernel_type>(context, node, params, data, input,\n-                                             output);\n-      break;\n+      return AverageEvalQuantizedUint8<kernel_type>(context, node, params, data,\n+                                                    input, output);\n     case kTfLiteInt8:\n-      AverageEvalQuantizedInt8<kernel_type>(context, node, params, data, input,\n-                                            output);\n-      break;\n+      return AverageEvalQuantizedInt8<kernel_type>(context, node, params, data,\n+                                                   input, output);\n     case kTfLiteInt16:\n-      AverageEvalQuantizedInt16<kernel_type>(context, node, params, data, input,\n-                                             output);\n-      break;\n+      return AverageEvalQuantizedInt16<kernel_type>(context, node, params, data,\n+                                                    input, output);\n     default:\n       TF_LITE_KERNEL_LOG(context, \"Type %s not currently supported.\",\n                          TfLiteTypeGetName(input->type));\n"
        ],
        "Title": "\n          FPE in TFLite pooling operations\n        "
    },
    {
        "Bug description": "The implementation of division in TFLite is  vulnerable to a division by 0 error",
        "Sample Code": "",
        "Bug fix": [
            "@@ -216,9 +216,23 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_OK(context,\n                     GetOutputSafe(context, node, kOutputTensor, &output));\n \n-  if (output->type == kTfLiteFloat32 || output->type == kTfLiteInt32) {\n+  // TODO(b/193904910): This can written with C++ templates\n+#define TF_LITE_CHECK_DIV_NON_ZERO(data_type)                       \\\n+  const auto* input2_data = GetTensorData<data_type>(input2);       \\\n+  const size_t input2_elements = input2->bytes / sizeof(data_type); \\\n+  for (size_t i = 0; i < input2_elements; i++) {                    \\\n+    TF_LITE_ENSURE(context, input2_data[i] != 0);                   \\\n+  }\n+\n+  if (output->type == kTfLiteFloat32) {\n+    // Div by zero seems ok in this case, just like in TF case infinities are\n+    // returned. So we don't do a check at this point.\n+    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n+  } else if (output->type == kTfLiteInt32) {\n+    TF_LITE_CHECK_DIV_NON_ZERO(int32_t);\n     EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n   } else if (output->type == kTfLiteUInt8) {\n+    TF_LITE_CHECK_DIV_NON_ZERO(uint8_t);\n     TF_LITE_ENSURE_OK(\n         context, EvalQuantized<kernel_type>(context, node, params, data, input1,\n                                             input2, output));\n@@ -229,6 +243,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n         output->type);\n     return kTfLiteError;\n   }\n+#undef TF_LITE_CHECK_DIV_NON_ZERO\n \n   return kTfLiteOk;\n }\n"
        ],
        "Title": "\n          FPE in TFLite division operations\n        "
    },
    {
        "Bug description": "All TFLite operations that use quantization can be made to use unitialized values.  For example :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -256,14 +256,21 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n                                                      output_temp_size_array));\n \n     // Calculate effective scales.\n+    TF_LITE_ENSURE(context, input->quantization.type != kTfLiteNoQuantization);\n     auto* input_params =\n         reinterpret_cast<TfLiteAffineQuantization*>(input->quantization.params);\n+    TF_LITE_ENSURE(context,\n+                   weights_feature->quantization.type != kTfLiteNoQuantization);\n     auto* weights_feature_params = reinterpret_cast<TfLiteAffineQuantization*>(\n         weights_feature->quantization.params);\n+    TF_LITE_ENSURE(context, state->quantization.type != kTfLiteNoQuantization);\n     auto* state_params =\n         reinterpret_cast<TfLiteAffineQuantization*>(state->quantization.params);\n+    TF_LITE_ENSURE(context,\n+                   weights_time->quantization.type != kTfLiteNoQuantization);\n     auto* weight_time_params = reinterpret_cast<TfLiteAffineQuantization*>(\n         weights_time->quantization.params);\n+    TF_LITE_ENSURE(context, output->quantization.type != kTfLiteNoQuantization);\n     auto* output_params = reinterpret_cast<TfLiteAffineQuantization*>(\n         output->quantization.params);\n     const double effective_scale_1 = input_params->scale->data[0] *\n"
        ],
        "Title": "\n          Use of unitialized value in TFLite\n        "
    },
    {
        "Bug description": "The implementation of SVDF in TFLite is  vulnerable to a null pointer error :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -119,6 +119,7 @@ TfLiteStatus GetInputSafe(const TfLiteContext* context, const TfLiteNode* node,\n TfLiteTensor* GetVariableInput(TfLiteContext* context, const TfLiteNode* node,\n                                int index) {\n   TfLiteTensor* tensor = GetMutableInput(context, node, index);\n+  if (tensor == nullptr) return nullptr;\n   return tensor->is_variable ? tensor : nullptr;\n }\n \n",
            "@@ -299,6 +299,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n                     GetTemporarySafe(context, node, /*index=*/0, &scratch));\n \n   TfLiteTensor* state = GetVariableInput(context, node, kStateTensor);\n+  TF_LITE_ENSURE(context, state != nullptr);\n   TfLiteTensor* output;\n   TF_LITE_ENSURE_OK(context,\n                     GetOutputSafe(context, node, kOutputTensor, &output));\n"
        ],
        "Title": "\n          NPE in TFLite\n        "
    },
    {
        "Bug description": "The implementation of fully connected layers in TFLite is  vulnerable to a division by zero error :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -223,6 +223,7 @@ TfLiteStatus PrepareImpl(TfLiteContext* context, TfLiteNode* node) {\n   }\n \n   TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 2);\n+  TF_LITE_ENSURE(context, filter->dims->data[1] != 0);\n   const int batch_size = input_size / filter->dims->data[1];\n   const int num_units = filter->dims->data[0];\n \n"
        ],
        "Title": "\n          Division by zero in TFLite\n        "
    },
    {
        "Bug description": "It is possible to nest a  tf.map_fn  within another  tf.map_fn  call. However, if the input tensor is a  RaggedTensor  and there is no function signature provided, code assumes the output is a fully specified tensor and fills output buffer with uninitialized contents from the heap:",
        "Sample Code": "x = tf.ragged.constant([[1,2], [3,4,5], [6]])\n]])\nt = tf.map_fn(lambda r: tf.map_fn(lambda y: r, r), x) ",
        "Bug fix": [
            "@@ -174,7 +174,23 @@ Status NestedStackRaggedTensors(\n   auto output_values_flat =\n       output_ragged->mutable_values()->flat_outer_dims<VALUE_TYPE, 2>();\n   int values_index = 0;\n+\n+  TensorShape expected_value_shape = component_values_shape;\n+  expected_value_shape.RemoveDim(0);\n+\n   for (int i = 0; i < ragged_components.size(); i++) {\n+    // Check that the flat_values tensor shape is compatible.\n+    TensorShape value_shape = ragged_components[i].values().shape();\n+    value_shape.RemoveDim(0);\n+    if (value_shape != expected_value_shape) {\n+      return errors::InvalidArgument(\n+          \"All flat_values must have compatible shapes.  Shape at index 0: \",\n+          expected_value_shape, \".  Shape at index \", i, \": \", value_shape,\n+          \".  If you are using tf.map_fn, then you may need to specify an \"\n+          \"explicit fn_output_signature with appropriate ragged_rank, and/or \"\n+          \"convert output tensors to RaggedTensors.\");\n+    }\n+\n     auto component_values_flat =\n         ragged_components[i].values().flat_outer_dims<VALUE_TYPE, 2>();\n     int num_inner_elements = ragged_components[i].values().NumElements();\n",
            "@@ -21,9 +21,11 @@ from absl.testing import parameterized\n import numpy as np\n \n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import sparse_tensor\n from tensorflow.python.framework import test_util\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import map_fn as map_fn_lib\n from tensorflow.python.ops import math_ops as mo\n from tensorflow.python.ops import string_ops\n from tensorflow.python.ops.ragged import ragged_factory_ops\n@@ -309,6 +311,27 @@ class RaggedMapOpTest(test_util.TensorFlowTestCase,\n     )\n     self.assertAllEqual(id_t2, [[0, 5], [0, 4]])\n \n+  def testRaggedMapWithIncorrectFnOutputSignature(self):\n+    x = ragged_factory_ops.constant([[1, 2, 3, 4], [1]])\n+    with self.assertRaisesRegex(errors.InvalidArgumentError,\n+                                'All flat_values must have compatible shapes'):\n+      y = map_fn_lib.map_fn(lambda r: map_fn_lib.map_fn(lambda y: r, r), x)\n+      self.evaluate(y)\n+\n+  def testNestedRaggedMapWithFnOutputSignature(self):\n+    ragged1d = ragged_tensor.RaggedTensorSpec([None], dtypes.int32)\n+    ragged2d = ragged_tensor.RaggedTensorSpec([None, None], dtypes.int32)\n+\n+    x = ragged_factory_ops.constant([[1, 2, 3, 4], [1]])\n+    # pylint: disable=g-long-lambda\n+    y = map_fn_lib.map_fn(\n+        lambda r: map_fn_lib.map_fn(\n+            lambda y: r, r, fn_output_signature=ragged1d),\n+        x,\n+        fn_output_signature=ragged2d)\n+    expected = [[[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]], [[1]]]\n+    self.assertAllEqual(y, expected)\n+\n \n if __name__ == '__main__':\n   googletest.main()\n"
        ],
        "Title": "\n          Heap OOB in nested `tf.map_fn` with `RaggedTensor`s\n        "
    },
    {
        "Bug description": "TensorFlow and Keras can be tricked to perform arbitrary code execution when deserializing a Keras model from YAML format.",
        "Sample Code": "payload = '''\n!!python/object/new:type\nargs: ['z', !!python/tuple [], {'extend': !!python/name:exec }]\nlistitems: \"__import__('os').system('cat /etc/passwd')\"\n'''\n  \n\n  \nmodels.model_from_yaml(payload)",
        "Bug fix": [
            "@@ -15,6 +15,10 @@\n     `if x.shape.rank == 1: x = tf.expand_dims(x, axis=-1)`.\n     Functional models as well as Sequential models built with an explicit\n     input shape are not affected.\n+  * The methods `Model.to_yaml()` and `keras.models.model_from_yaml` have been\n+    replaced to raise a `RuntimeError` as they can be abused to cause arbitrary\n+    code execution. It is recommended to use JSON serialization instead of YAML,\n+    or, a better alternative, serialize to H5.\n \n * `tf.lite`:\n   * Rename fields `SignatureDef` table in schema to maximize the parity with\n",
            "@@ -53,7 +53,7 @@ class Functional(training_lib.Model):\n   than with subclassed `Model`s, specifically:\n \n   - Model cloning (`keras.models.clone`)\n-  - Serialization (`model.get_config()/from_config`, `model.to_json()/to_yaml()`\n+  - Serialization (`model.get_config()/from_config`, `model.to_json()`\n   - Whole-model saving (`model.save()`)\n \n   A `Functional` model can be instantiated by passing two arguments to\n",
            "@@ -47,11 +47,6 @@ from tensorflow.python.ops.ragged import ragged_factory_ops\n from tensorflow.python.platform import test\n from tensorflow.python.training.tracking.util import Checkpoint\n \n-try:\n-  import yaml  # pylint:disable=g-import-not-at-top\n-except ImportError:\n-  yaml = None\n-\n \n class NetworkConstructionTest(keras_parameterized.TestCase):\n \n@@ -627,10 +622,6 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n       json_str = model.to_json()\n       models.model_from_json(json_str)\n \n-      if yaml is not None:\n-        yaml_str = model.to_yaml()\n-        models.model_from_yaml(yaml_str)\n-\n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n   def test_invalid_graphs(self):\n     a = layers.Input(shape=(32,), name='input_a')\n@@ -1361,10 +1352,6 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     json_str = model.to_json()\n     models.model_from_json(json_str)\n \n-    if yaml is not None:\n-      yaml_str = model.to_yaml()\n-      models.model_from_yaml(yaml_str)\n-\n   def test_subclassed_error_if_init_not_called(self):\n \n     class MyNetwork(training_lib.Model):\n",
            "@@ -87,11 +87,6 @@ try:\n   import h5py\n except ImportError:\n   h5py = None\n-\n-try:\n-  import yaml\n-except ImportError:\n-  yaml = None\n # pylint: enable=g-import-not-at-top\n \n \n@@ -2416,6 +2411,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n   def to_yaml(self, **kwargs):\n     \"\"\"Returns a yaml string containing the network configuration.\n \n+    Note: Since TF 2.6, this method is no longer supported and will raise a\n+    RuntimeError.\n+\n     To load a network from a yaml save file, use\n     `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n \n@@ -2431,12 +2429,12 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         A YAML string.\n \n     Raises:\n-        ImportError: if yaml module is not found.\n+        RuntimeError: announces that the method poses a security risk\n     \"\"\"\n-    if yaml is None:\n-      raise ImportError(\n-          'Requires yaml module installed (`pip install pyyaml`).')\n-    return yaml.dump(self._updated_config(), **kwargs)\n+    raise RuntimeError(\n+        'Method `model.to_yaml()` has been removed due to security risk of '\n+        'arbitrary code execution. Please use `model.to_json()` instead.'\n+    )\n \n   def reset_states(self):\n     for layer in self.layers:\n",
            "@@ -18,18 +18,11 @@\n from tensorflow.python.keras.saving.saved_model import json_utils\n from tensorflow.python.util.tf_export import keras_export\n \n-# pylint: disable=g-import-not-at-top\n-try:\n-  import yaml\n-except ImportError:\n-  yaml = None\n-# pylint: enable=g-import-not-at-top\n-\n \n @keras_export('keras.models.model_from_config')\n def model_from_config(config, custom_objects=None):\n   \"\"\"Instantiates a Keras model from its config.\n- \n+\n   Usage:\n   ```\n   # for a Functional API model\n@@ -63,17 +56,8 @@ def model_from_config(config, custom_objects=None):\n def model_from_yaml(yaml_string, custom_objects=None):\n   \"\"\"Parses a yaml model configuration file and returns a model instance.\n \n-  Usage:\n-\n-  >>> model = tf.keras.Sequential([\n-  ...     tf.keras.layers.Dense(5, input_shape=(3,)),\n-  ...     tf.keras.layers.Softmax()])\n-  >>> try:\n-  ...   import yaml\n-  ...   config = model.to_yaml()\n-  ...   loaded_model = tf.keras.models.model_from_yaml(config)\n-  ... except ImportError:\n-  ...   pass\n+  Note: Since TF 2.6, this method is no longer supported and will raise a\n+  RuntimeError.\n \n   Args:\n       yaml_string: YAML string or open file encoding a model configuration.\n@@ -85,19 +69,13 @@ def model_from_yaml(yaml_string, custom_objects=None):\n       A Keras model instance (uncompiled).\n \n   Raises:\n-      ImportError: if yaml module is not found.\n+      RuntimeError: announces that the method poses a security risk\n   \"\"\"\n-  if yaml is None:\n-    raise ImportError('Requires yaml module installed (`pip install pyyaml`).')\n-  # The method unsafe_load only exists in PyYAML 5.x+, so which branch of the\n-  # try block is covered by tests depends on the installed version of PyYAML.\n-  try:\n-    # PyYAML 5.x+\n-    config = yaml.unsafe_load(yaml_string)\n-  except AttributeError:\n-    config = yaml.load(yaml_string)\n-  from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\n-  return deserialize(config, custom_objects=custom_objects)\n+  raise RuntimeError(\n+      'Method `model_from_yaml()` has been removed due to security risk of '\n+      'arbitrary code execution. Please use `Model.to_json()` and '\n+      '`model_from_json()` instead.'\n+  )\n \n \n @keras_export('keras.models.model_from_json')\n"
        ],
        "Title": "\n          Arbitrary code execution due to YAML deserialization\n        "
    }
]
[
    {
        "Bug description": "The shape inference code for  tf.raw_ops.Dequantize  has a vulnerability that could trigger a denial of service via a segfault if an attacker provides invalid arguments:",
        "Sample Code": "tf.compat.v1.disable_v2_behavior()\ntf.raw_ops.Dequantize(\n  input_tensor = tf.constant(-10.0, dtype=tf.float32),\n  input_tensor = tf.cast(input_tensor, dtype=tf.quint8),\n  min_range = tf.constant([], shape=[0], dtype=tf.float32),\n  max_range = tf.constant([], shape=[0], dtype=tf.float32),\n  mode  = 'MIN_COMBINED',\n  narrow_range=False,\n  axis=-10,\n  ,\n  dtype=tf.dtypes.float32)",
        "Bug fix": [
            "@@ -2990,6 +2990,10 @@ REGISTER_OP(\"Dequantize\")\n       if (!s.ok() && s.code() != error::NOT_FOUND) {\n         return s;\n       }\n+      if (axis < -1) {\n+        return errors::InvalidArgument(\"axis should be at least -1, got \",\n+                                       axis);\n+      }\n       const int minmax_rank = (axis == -1) ? 0 : 1;\n       TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n       ShapeHandle minmax;\n"
        ],
        "Title": "\n          Missing validation in shape inference for `Dequantize`\n        "
    },
    {
        "Bug description": "Most implementations of convolution operators in TensorFlow are affected by a division by 0 vulnerability where an attacker can trigger a denial of service via a crash:",
        "Sample Code": "tf.compat.v1.disable_v2_behavior()\ntf.raw_ops.Conv2D(\n  input = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32),\n  filter = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32),\n  strides = [1, 1, 1, 1],\n  ],\n  padding = \"SAME\")",
        "Bug fix": [
            "@@ -672,6 +672,8 @@ Status Conv2DShapeImpl(shape_inference::InferenceContext* c,\n   if (c->ValueKnown(input_depth_dim) && c->ValueKnown(filter_input_depth_dim)) {\n     int64_t input_depth_value = c->Value(input_depth_dim),\n             filter_input_depth_value = c->Value(filter_input_depth_dim);\n+    if (filter_input_depth_value == 0)\n+      return errors::InvalidArgument(\"Depth of filter must not be 0\");\n     if (input_depth_value % filter_input_depth_value != 0)\n       return errors::InvalidArgument(\n           \"Depth of input (\", input_depth_value,\n@@ -681,6 +683,8 @@ Status Conv2DShapeImpl(shape_inference::InferenceContext* c,\n       int64_t num_groups = input_depth_value / filter_input_depth_value;\n       if (c->ValueKnown(output_depth_dim)) {\n         int64_t output_depth_value = c->Value(output_depth_dim);\n+        if (num_groups == 0)\n+          return errors::InvalidArgument(\"Number of groups must not be 0\");\n         if (output_depth_value % num_groups != 0)\n           return errors::InvalidArgument(\n               \"Depth of output (\", output_depth_value,\n@@ -816,6 +820,8 @@ Status Conv3DShape(shape_inference::InferenceContext* c) {\n   if (c->ValueKnown(input_depth_dim) && c->ValueKnown(filter_input_depth_dim)) {\n     int64_t input_depth_value = c->Value(input_depth_dim),\n             filter_input_depth_value = c->Value(filter_input_depth_dim);\n+    if (filter_input_depth_value == 0)\n+      return errors::InvalidArgument(\"Depth of filter must not be 0\");\n     if (input_depth_value % filter_input_depth_value != 0)\n       return errors::InvalidArgument(\n           \"Depth of input (\", input_depth_value,\n@@ -825,6 +831,8 @@ Status Conv3DShape(shape_inference::InferenceContext* c) {\n       int64_t num_groups = input_depth_value / filter_input_depth_value;\n       if (c->ValueKnown(output_depth_dim)) {\n         int64_t output_depth_value = c->Value(output_depth_dim);\n+        if (num_groups == 0)\n+          return errors::InvalidArgument(\"Number of groups must not be 0\");\n         if (output_depth_value % num_groups != 0)\n           return errors::InvalidArgument(\n               \"Depth of output (\", output_depth_value,\n@@ -2456,6 +2464,9 @@ Status SparseReduceShapeFn(InferenceContext* c) {\n \n     int64_t ndims = shape_vec.size();\n     absl::flat_hash_set<int64> axes;\n+    if (ndims == 0)\n+      return errors::InvalidArgument(\n+          \"Number of dims in shape tensor must not be 0\");\n     for (int i = 0; i < axes_vec.size(); i++) {\n       axes.insert((axes_vec(i) + ndims) % ndims);\n     }\n"
        ],
        "Title": "\n          Division by 0 in most convolution operators\n        "
    },
    {
        "Bug description": "An attacker can cause undefined behavior via binding a reference to null pointer in  tf.raw_ops.SparseFillEmptyRows :",
        "Sample Code": "tf.compat.v1.disable_v2_behavior()\ntf.raw_ops.SparseFillEmptyRows(\n  indices = tf.constant([], shape=[0, 0], dtype=tf.int64),\n  values = tf.constant([], shape=[0], dtype=tf.int64),\n  dense_shape = tf.constant([], shape=[0], dtype=tf.int64),\n  ),\n  default_value = 0)",
        "Bug fix": [
            "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/common_shape_fns.h\"\n #include \"tensorflow/core/framework/op.h\"\n #include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n \n namespace tensorflow {\n \n@@ -619,6 +620,8 @@ REGISTER_OP(\"SparseFillEmptyRows\")\n       DimensionHandle unused_dim;\n       TF_RETURN_IF_ERROR(c->Merge(c->Dim(input_indices, 1),\n                                   c->Dim(input_shape, 0), &unused_dim));\n+      if (c->Value(c->NumElements(input_shape)) == 0)\n+        return errors::InvalidArgument(\"dense_shape must not be empty\");\n       ShapeHandle output_indices =\n           c->Matrix(InferenceContext::kUnknownDim, c->NumElements(input_shape));\n       ShapeHandle output_values = c->Vector(InferenceContext::kUnknownDim);\n"
        ],
        "Title": "\n          Reference binding to nullptr in shape inference\n        "
    },
    {
        "Bug description": "An attacker can trigger a denial of service via a segmentation fault in  tf.raw_ops.MaxPoolGrad  caused by missing validation:",
        "Sample Code": "tf.raw_ops.MaxPoolGrad(\n  orig_input = tf.constant([], shape=[3, 0, 0, 2], dtype=tf.float32),\n  orig_output = tf.constant([], shape=[3, 0, 0, 2], dtype=tf.float32),\n  grad = tf.constant([], shape=[3, 0, 0, 2], dtype=tf.float32),\n  ksize = [1, 16, 16, 1],\n  strides = [1, 16, 18, 1],\n  padding = \"EXPLICIT\",\n  ,\n  explicit_paddings = [0, 0, 14, 3, 15, 5, 0, 0])",
        "Bug fix": [
            "@@ -74,6 +74,7 @@ static void SpatialMaxPoolWithArgMaxHelper(\n         errors::Internal(\"SpatialMaxPoolWithArgMaxHelper requires Targmax \"\n                          \"to be int64 when input_backprop != nullptr\"));\n   }\n+  if (tensor_in.NumElements() == 0 || output->NumElements() == 0) return;\n \n   typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n       ConstEigenMatrixMap;\n@@ -949,6 +950,10 @@ class MaxPoolingWithArgmaxOp : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     const Tensor& tensor_in = context->input(0);\n+    OP_REQUIRES(context, tensor_in.dims() == 4,\n+                errors::InvalidArgument(\"tensor_in must be 4-dimensional (2)\"));\n+    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n+                errors::InvalidArgument(\"tensor_in must not be empty (2)\"));\n \n     PoolParameters params{context,\n                           ksize_,\n",
            "@@ -171,6 +171,8 @@ PoolParameters::PoolParameters(OpKernelContext* context,\n     pad_depth = 0;\n     out_depth = depth;\n   } else {\n+    OP_REQUIRES(context, depth_window > 0,\n+                errors::InvalidArgument(\"depth_window must not be 0\"));\n     // Our current version of depthwise max pooling does not support\n     // any padding, and expects the depth_window to equal the\n     // depth_stride (no overlapping).\n"
        ],
        "Title": "\n          Incomplete validation in `MaxPoolGrad`\n        "
    },
    {
        "Bug description": "An attacker can trigger a denial of service via a  CHECK -fail in  tf.raw_ops.MapStage :",
        "Sample Code": "tf.raw_ops.MapStage(\n  key=tf.constant([], shape=[0, 0, 0, 0], dtype=tf.int64),\n  indices=tf.constant((0), dtype=tf.int32),\n  values=[tf.constant((0), dtype=tf.int32)],\n  dtypes=[tf.int32,\n  tf.int64],\n  capacity=0,\n  memory_limit=0,\n  container='',\n  ,\n  shared_name='')",
        "Bug fix": [
            "@@ -527,6 +527,8 @@ class MapStageOp : public OpKernel {\n     OP_REQUIRES_OK(ctx, ctx->input(\"key\", &key_tensor));\n     OP_REQUIRES_OK(ctx, ctx->input(\"indices\", &indices_tensor));\n     OP_REQUIRES_OK(ctx, ctx->input_list(\"values\", &values_tensor));\n+    OP_REQUIRES(ctx, key_tensor->NumElements() > 0,\n+                errors::InvalidArgument(\"key must not be empty\"));\n \n     // Create copy for insertion into Staging Area\n     Tensor key(*key_tensor);\n"
        ],
        "Title": "\n          `CHECK`-fail in `MapStage`\n        "
    },
    {
        "Bug description": "An attacker can read from outside of bounds of heap allocated data by sending specially crafted illegal arguments to  tf.raw_ops.SdcaOptimizerV2 :",
        "Sample Code": "tf.raw_ops.SdcaOptimizerV2(\n  sparse_example_indices=[[1]],\n  sparse_feature_indices=[[1]],\n  sparse_feature_values=[[1.0,2.0]],\n  dense_features=[[1.0]],\n  example_weights=[1.0],\n  example_labels=[],\n  sparse_indices=[1],\n  sparse_weights=[1.0],\n  dense_weights=[[1.0]],\n  example_state_data=[[100.0,100.0,100.0,100.0]],\n  loss_type='logistic_loss',\n  l1=100.0,\n  l2=100.0,\n  num_loss_partitions=1,\n  num_inner_iterations=1,\n  ,\n  adaptive=True)       ",
        "Bug fix": [
            "@@ -380,6 +380,11 @@ Status Examples::Initialize(OpKernelContext* const context,\n   const Tensor* example_labels_t;\n   TF_RETURN_IF_ERROR(context->input(\"example_labels\", &example_labels_t));\n   auto example_labels = example_labels_t->flat<float>();\n+  if (example_labels.size() != num_examples) {\n+    return errors::InvalidArgument(\"Expected \", num_examples,\n+                                   \" example labels but got \",\n+                                   example_labels.size());\n+  }\n \n   OpInputList dense_features_inputs;\n   TF_RETURN_IF_ERROR(\n"
        ],
        "Title": "\n          Heap OOB in `SdcaOptimizerV2`\n        "
    },
    {
        "Bug description": "An attacker can cause undefined behavior via binding a reference to null pointer in  tf.raw_ops.Map*  and  tf.raw_ops.OrderedMap*  operations:",
        "Sample Code": "tf.raw_ops.MapPeek(\n  key=tf.constant([8],dtype=tf.int64),\n  indices=[],\n  dtypes=[tf.int32],\n  capacity=8,\n  ,\n  memory_limit=128)",
        "Bug fix": [
            "@@ -210,9 +210,9 @@ class StagingMap : public ResourceBase {\n                                    const OptionalTuple& tuple)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n     if (tuple[index].has_value()) {\n-      return Status(errors::InvalidArgument(\n+      return errors::InvalidArgument(\n           \"The tensor for index '\", index, \"' for key '\", key.scalar<int64>()(),\n-          \"' was already initialized '\", dtypes_.size(), \"'.\"));\n+          \"' was already initialized '\", dtypes_.size(), \"'.\");\n     }\n \n     return Status::OK();\n@@ -220,6 +220,10 @@ class StagingMap : public ResourceBase {\n \n   // Check that the indices are strictly ordered\n   Status check_index_ordering(const Tensor& indices) {\n+    if (indices.NumElements() == 0) {\n+      return errors::InvalidArgument(\"Indices are empty\");\n+    }\n+\n     auto findices = indices.flat<int>();\n \n     for (std::size_t i = 0; i < findices.dimension(0) - 1; ++i) {\n@@ -227,8 +231,7 @@ class StagingMap : public ResourceBase {\n         continue;\n       }\n \n-      return Status(\n-          errors::InvalidArgument(\"Indices are not strictly ordered\"));\n+      return errors::InvalidArgument(\"Indices are not strictly ordered\");\n     }\n \n     return Status::OK();\n@@ -238,10 +241,10 @@ class StagingMap : public ResourceBase {\n   Status check_memory_limit(std::size_t bytes)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n     if (has_memory_limit() && bytes > memory_limit_) {\n-      return Status(errors::ResourceExhausted(\n+      return errors::ResourceExhausted(\n           \"Attempted to insert tensors with combined size of '\", bytes,\n           \"' bytes into Staging Area with a memory limit of '\", memory_limit_,\n-          \"'.\"));\n+          \"'.\");\n     }\n \n     return Status::OK();\n"
        ],
        "Title": "\n          Reference binding to nullptr in map operations\n        "
    },
    {
        "Bug description": "An attacker can read from outside of bounds of heap allocated data by sending specially crafted illegal arguments to  tf.raw_ops.UpperBound :",
        "Sample Code": "tf.raw_ops.UpperBound(\n  sorted_input=[1,2,3],\n  values=tf.constant(value=[[0,0,0],[1,1,1],[2,2,2]],dtype=tf.int64),\n  ),\n  out_type=tf.int64)",
        "Bug fix": [
            "@@ -86,6 +86,10 @@ class UpperBoundOp : public OpKernel {\n     const Tensor& sorted_inputs_t = ctx->input(0);\n     const Tensor& values_t = ctx->input(1);\n \n+    // inputs must be at least a matrix\n+    OP_REQUIRES(\n+        ctx, sorted_inputs_t.shape().dims() >= 2,\n+        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n     // must have same batch dim_size for both\n     OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                 Status(error::INVALID_ARGUMENT,\n@@ -127,6 +131,10 @@ class LowerBoundOp : public OpKernel {\n     const Tensor& sorted_inputs_t = ctx->input(0);\n     const Tensor& values_t = ctx->input(1);\n \n+    // inputs must be at least a matrix\n+    OP_REQUIRES(\n+        ctx, sorted_inputs_t.shape().dims() >= 2,\n+        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n     // must have same batch dim_size for both\n     OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                 Status(error::INVALID_ARGUMENT,\n"
        ],
        "Title": "\n          Heap OOB in `UpperBound` and `LowerBound`\n        "
    },
    {
        "Bug description": "An attacker can cause denial of service in applications serving models using  tf.raw_ops.NonMaxSuppressionV5  by triggering a division by 0:",
        "Sample Code": "tf.raw_ops.NonMaxSuppressionV5(\n  boxes=[[[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]],[[0.1,0.1,0.1,0.1],[0.2,0.2,0.2,0.2],[0.3,0.3,0.3,0.3]]]],\n  scores=[[[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]]],\n  max_output_size_per_class=-1,\n  max_total_size=10,\n  iou_threshold=score_threshold=0.5,\n  pad_per_class=True,\n  ,\n  clip_boxes=True)",
        "Bug fix": [
            "@@ -169,6 +169,8 @@ void DoNonMaxSuppressionOp(OpKernelContext* context, const Tensor& scores,\n                            bool pad_to_max_output_size = false,\n                            int* ptr_num_valid_outputs = nullptr) {\n   const int output_size = max_output_size.scalar<int>()();\n+  OP_REQUIRES(context, output_size >= 0,\n+              errors::InvalidArgument(\"output size must be non-negative\"));\n \n   std::vector<T> scores_data(num_boxes);\n   std::copy_n(scores.flat<T>().data(), num_boxes, scores_data.begin());\n@@ -768,6 +770,9 @@ class NonMaxSuppressionV4Op : public OpKernel {\n         context, scores, num_boxes, max_output_size, iou_threshold_val,\n         score_threshold_val, dummy_soft_nms_sigma, similarity_fn,\n         return_scores_tensor_, pad_to_max_output_size_, &num_valid_outputs);\n+    if (!context->status().ok()) {\n+      return;\n+    }\n \n     // Allocate scalar output tensor for number of indices computed.\n     Tensor* num_outputs_t = nullptr;\n@@ -845,6 +850,9 @@ class NonMaxSuppressionV5Op : public OpKernel {\n         context, scores, num_boxes, max_output_size, iou_threshold_val,\n         score_threshold_val, soft_nms_sigma_val, similarity_fn,\n         return_scores_tensor_, pad_to_max_output_size_, &num_valid_outputs);\n+    if (!context->status().ok()) {\n+      return;\n+    }\n \n     // Allocate scalar output tensor for number of indices computed.\n     Tensor* num_outputs_t = nullptr;\n"
        ],
        "Title": "\n          Crash in NMS ops caused by integer conversion to unsigned\n        "
    },
    {
        "Bug description": "An attacker can cause denial of service in applications serving models using  tf.raw_ops.UnravelIndex  by triggering a division by 0:",
        "Sample Code": " tensorflow as tf\n\ntf.raw_ops.UnravelIndex(indices=-1, dims=[1,0,2])",
        "Bug fix": [
            "@@ -53,6 +53,14 @@ class UnravelIndexOp : public OpKernel {\n                                 dims_tensor.shape().DebugString(), \"\\\"\"));\n \n     auto dims = dims_tensor.vec<Tidx>();\n+    // Make sure dims does not contain a zero\n+    for (int i = 0; i < dims.size(); i++) {\n+      OP_REQUIRES(\n+          ctx, dims(i) != 0,\n+          errors::InvalidArgument(\"Input dims cannot contain a dim of zero, \"\n+                                  \"but dims contains zero at index \",\n+                                  i));\n+    }\n \n     // Chek to make sure indices is not out of boundary\n     Eigen::Tensor<Tidx, 0, Eigen::RowMajor> dims_prod_eigen = dims.prod();\n",
            "@@ -1575,7 +1575,7 @@ class UnravelIndexTest(test_util.TensorFlowTestCase):\n     with self.cached_session():\n       for dtype in [dtypes.int32, dtypes.int64]:\n         with self.assertRaisesRegex(errors.InvalidArgumentError,\n-                                    \"index is out of bound as with dims\"):\n+                                    \"dims cannot contain a dim of zero\"):\n           indices = constant_op.constant([2, 5, 7], dtype=dtype)\n           dims = constant_op.constant([3, 0], dtype=dtype)\n           self.evaluate(array_ops.unravel_index(indices=indices, dims=dims))\n"
        ],
        "Title": "\n          FPE in `tf.raw_ops.UnravelIndex`\n        "
    }
]
[
    {
        "Bug description": "An attacker can cause undefined behavior via binding a reference to null pointer in  tf.raw_ops.UnicodeEncode :",
        "Sample Code": "from tensorflow.python.ops import gen_string_ops\n\ngen_string_ops.unicode_encode(\n  input_values=[],\n  input_splits=[],\n  output_encoding='UTF-8',\n  errors='ignore',\n  ,\n  replacement_char='a')",
        "Bug fix": [
            "@@ -533,6 +533,10 @@ class UnicodeEncodeOp : public OpKernel {\n     const Tensor& input_splits = context->input(1);\n     const auto input_splits_flat = input_splits.flat<SPLITS_TYPE>();\n \n+    OP_REQUIRES(\n+        context, input_splits.NumElements() > 0,\n+        errors::InvalidArgument(\"Input_splits should contain elements, but \"\n+                                \"given input_values has 0 elements\"));\n     // Operation will treat first argument in input_splits as if it were zero\n     // regardless of its actual value since splits should begin with zero and\n     // end with the length of the input values vector.\n"
        ],
        "Title": "\n          Reference binding to nullptr in unicode encoding\n        "
    },
    {
        "Bug description": "An attacker can cause undefined behavior via binding a reference to null pointer in  tf.raw_ops.RaggedTensorToVariant :",
        "Sample Code": "tf.raw_ops.RaggedTensorToVariant(\n  rt_nested_splits=[],\n  rt_dense_values=[1,2,3],\n  ],\n  batched_input=True)",
        "Bug fix": [
            "@@ -157,6 +157,12 @@ class RaggedTensorToVariantOp : public OpKernel {\n       return;\n     }\n \n+    // Checked here instead of at input in case batched_input_ is false\n+    OP_REQUIRES(context, ragged_nested_splits_len > 0,\n+                errors::InvalidArgument(\n+                    \"rt_nested_splits must be a list of one or more, but \"\n+                    \"received rt_nested_splits of length 0.\"));\n+\n     // Unbatch the Ragged Tensor and encode the components.\n     std::vector<RaggedTensorVariant> unbatched_ragged_input;\n     auto batched_splits_top_vec =\n"
        ],
        "Title": "\n          Reference binding to nullptr in `RaggedTensorToVariant`\n        "
    },
    {
        "Bug description": "Due to incomplete validation in MKL implementation of requantization, an  attacker can trigger undefined behavior via binding a reference to a null pointer or can access data outside the bounds of heap allocated arrays:",
        "Sample Code": "from tensorflow.python.ops import gen_math_ops\n\ngen_math_ops.requantize_per_channel(\n  input=[],\n  input_min=[-100,-100,-100,-100,-100],\n  input_max=[-100,-100,-100],\n  requested_output_min=[-100,-100,-100,-100,-100],\n  requested_output_max=[],\n  [],\n  out_type=tf.int)",
        "Bug fix": [
            "@@ -57,6 +57,20 @@ class MklRequantizationRangePerChannelOp : public OpKernel {\n         ctx, input_max.dim_size(0) == depth,\n         errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                 depth, \" was \", input_max.dim_size(0)));\n+    OP_REQUIRES(\n+        ctx, input_min.NumElements() == depth,\n+        errors::InvalidArgument(\"input_min must have the same number of \"\n+                                \"elements as input_max, got \",\n+                                input_min.NumElements(), \" and \", depth));\n+    OP_REQUIRES(ctx, input.NumElements() > 0,\n+                errors::InvalidArgument(\"input must not be empty\"));\n+    OP_REQUIRES(ctx, input.dims() == 4,\n+                errors::InvalidArgument(\"input must be in NHWC format\"));\n+    OP_REQUIRES(\n+        ctx, input.dim_size(3) == depth,\n+        errors::InvalidArgument(\n+            \"input must have same number of channels as length of input_min: \",\n+            input.dim_size(3), \" vs \", depth));\n \n     const float* input_min_data = input_min.flat<float>().data();\n     const float* input_max_data = input_max.flat<float>().data();\n"
        ],
        "Title": "\n          Incomplete validation in MKL requantization\n        "
    },
    {
        "Bug description": "Due to incomplete validation in  tf.raw_ops.QuantizeV2 , an attacker can trigger undefined behavior via binding a reference to a null pointer or can access data outside the bounds of heap allocated arrays:",
        "Sample Code": "tf.raw_ops.QuantizeV2(\n  input=[1,2,3],\n  min_range=[1,2],\n  max_range=[],\n  T=tf.qint32,\n  mode='SCALED',\n  round_mode='HALF_AWAY_FROM_ZERO',\n  narrow_range=False,\n  axis=1,\n  ,\n  ensure_minimum_range=3)",
        "Bug fix": [
            "@@ -113,7 +113,50 @@ class QuantizeV2Op : public OpKernel {\n \n     int num_slices = 1;\n     if (axis_ > -1) {\n+      OP_REQUIRES(\n+          ctx, input.dims() > axis_,\n+          errors::InvalidArgument(\n+              \"Axis is on a zero-based index, so its value must always be less \"\n+              \"than number of input's dims, but given axis value was \",\n+              axis_, \" and input's dims was \", input.dims()));\n       num_slices = input.dim_size(axis_);\n+      OP_REQUIRES(ctx, input_min_range.dims() == 1,\n+                  errors::InvalidArgument(\n+                      \"If axis is specified, min_range must be a 1-D tensor \"\n+                      \"whose size matches the axis dimension of the input and \"\n+                      \"output tensors, but min_range dims are \",\n+                      input_min_range.dims()));\n+      OP_REQUIRES(ctx, input_min_range.dim_size(0) == num_slices,\n+                  errors::InvalidArgument(\n+                      \"If axis is specified, min_range must be a 1-D tensor \"\n+                      \"whose size matches the axis dimension of the input and \"\n+                      \"output tensors, but min_range is a 1-D tensor of size \",\n+                      input_min_range.dim_size(0),\n+                      \" and input's axis dimension is of size \", num_slices));\n+      OP_REQUIRES(ctx, input_max_range.dims() == 1,\n+                  errors::InvalidArgument(\n+                      \"If axis is specified, max_range must be a 1-D tensor \"\n+                      \"whose size matches the axis dimension of the input and \"\n+                      \"output tensors, but max_range dims are \",\n+                      input_max_range.dims()));\n+      OP_REQUIRES(ctx, input_max_range.dim_size(0) == num_slices,\n+                  errors::InvalidArgument(\n+                      \"If axis is specified, max_range must be a 1-D tensor \"\n+                      \"whose size matches the axis dimension of the input and \"\n+                      \"output tensors, but max_range is a 1-D tensor of size \",\n+                      input_max_range.dim_size(0),\n+                      \" and input's axis dimension is of size \", num_slices));\n+    } else {\n+      OP_REQUIRES(ctx, input_min_range.NumElements() == 1,\n+                  errors::InvalidArgument(\n+                      \"If axis is not specified, min_range must contain a \"\n+                      \"single float element, but it contains \",\n+                      input_min_range.NumElements(), \" elements\"));\n+      OP_REQUIRES(ctx, input_max_range.NumElements() == 1,\n+                  errors::InvalidArgument(\n+                      \"If axis is not specified, max_range must contain a \"\n+                      \"single float element, but it contains \",\n+                      input_max_range.NumElements(), \" elements\"));\n     }\n \n     const TensorShape& minmax_shape = ctx->input(1).shape();\n"
        ],
        "Title": "\n          Incomplete validation in `QuantizeV2`\n        "
    },
    {
        "Bug description": "An attacker can read from outside of bounds of heap allocated data by sending specially crafted illegal arguments to  BoostedTreesSparseCalculateBestFeatureSplit :",
        "Sample Code": "tf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit(\n  node_id_range=[0,10],\n  stats_summary_indices=[[1, 2, 3, 0x1000000]],\n  stats_summary_values=[1.0],\n  stats_summary_shape=[1,1,1,1],\n  l1=l2=[1.0],\n  tree_complexity=[0.5],\n  min_node_weight=[1.0],\n  logits_dimension=3,\n  ,\n  split_type='inequality')                                                                                                                                                                                                                                                                ",
        "Bug fix": [
            "@@ -1050,6 +1050,13 @@ class BoostedTreesSparseCalculateBestFeatureSplitOp : public OpKernel {\n       const int32_t feature_dim = stats_summary_indices(idx, 1);\n       const int32_t bucket_id = stats_summary_indices(idx, 2);\n       const int32_t stat_dim = stats_summary_indices(idx, 3);\n+      OP_REQUIRES(context, stat_dim < stats_dims,\n+                  errors::InvalidArgument(\n+                      \"Stat dim, the sum of logits dim and hessian dim in \"\n+                      \"stats_summary_indices, cannot be greater than stats \"\n+                      \"dims, the last value in stats_summary_shape, which was \",\n+                      stats_dims, \". At index (\", idx,\n+                      \", 4), stats_summary_indices contains value \", stat_dim));\n       std::pair<FeatureMapIterator, bool> const& f_insert_result = f_map.insert(\n           FeatureMapIterator::value_type(feature_dim, BucketMap()));\n       auto& b_map = f_insert_result.first->second;\n"
        ],
        "Title": "\n          Heap OOB in boosted trees\n        "
    },
    {
        "Bug description": "An attacker can generate undefined behavior via a reference binding to nullptr in  BoostedTreesCalculateBestGainsPerFeature :",
        "Sample Code": "tf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2(\n  node_id_range=[],\n  stats_summaries_list=[[1,2,3]],\n  split_types=[''],\n  candidate_feature_ids=[1,2,3,4],\n  l1=[1],     \n  l2=[1],\n  tree_complexity=[1.0],\n  min_node_weight=[1.17],\n  ],\n  logits_dimension=5)",
        "Bug fix": [
            "@@ -51,6 +51,16 @@ class BoostedTreesCalculateBestGainsPerFeatureOp : public OpKernel {\n     // node_id_range\n     const Tensor* node_id_range_t;\n     OP_REQUIRES_OK(context, context->input(\"node_id_range\", &node_id_range_t));\n+    OP_REQUIRES(\n+        context, node_id_range_t->dims() == 1,\n+        errors::InvalidArgument(\"node_id_range must be a rank 1 tensor, but \"\n+                                \"given node_id_range has dims of \",\n+                                node_id_range_t->dims()));\n+    OP_REQUIRES(context, node_id_range_t->dim_size(0) == 2,\n+                errors::InvalidArgument(\n+                    \"node_id_range must be a rank 1 tensor with shape=[2], but \"\n+                    \"given node_id_range has shape \",\n+                    node_id_range_t->dim_size(0), \" on its first dim\"));\n     const auto node_id_range = node_id_range_t->vec<int32>();\n     const int32_t node_id_first = node_id_range(0);  // inclusive\n     const int32_t node_id_last = node_id_range(1);   // exclusive\n@@ -570,6 +580,16 @@ class BoostedTreesCalculateBestFeatureSplitV2 : public OpKernel {\n     const Tensor* node_id_range_t;\n     OP_REQUIRES_OK(context, context->input(\"node_id_range\", &node_id_range_t));\n     const auto node_id_range = node_id_range_t->vec<int32>();\n+    OP_REQUIRES(\n+        context, node_id_range_t->dims() == 1,\n+        errors::InvalidArgument(\"node_id_range must be a rank 1 tensor, but \"\n+                                \"given node_id_range has dims of \",\n+                                node_id_range_t->dims()));\n+    OP_REQUIRES(context, node_id_range_t->dim_size(0) == 2,\n+                errors::InvalidArgument(\n+                    \"node_id_range must be a rank 1 tensor with shape=[2], but \"\n+                    \"given node_id_range has shape \",\n+                    node_id_range_t->dim_size(0), \" on its first dim\"));\n     const int32_t node_id_first = node_id_range(0);  // Inclusive.\n     const int32_t node_id_last = node_id_range(1);   // Exclusive.\n \n"
        ],
        "Title": "\n          Reference binding to nullptr in boosted trees\n        "
    },
    {
        "Bug description": "An attacker can cause a denial of service in  boosted_trees_create_quantile_stream_resource  by using negative arguments:",
        "Sample Code": "from tensorflow.python.ops import gen_boosted_trees_ops\nimport numpy as np\n\nv= tf.Variable([0.0, 0.0, 0.0, 0.0, 0.0])\ngen_boosted_trees_ops.boosted_trees_create_quantile_stream_resource(\n  quantile_stream_resource_handle = v.handle,\n  epsilon = [74.82224],\n  num_streams = [-49], \n  ], \n  max_elements = np.int32(586))",
        "Bug fix": [
            "@@ -116,6 +116,9 @@ class BoostedTreesCreateQuantileStreamResourceOp : public OpKernel {\n     const Tensor* num_streams_t;\n     OP_REQUIRES_OK(context, context->input(kNumStreamsName, &num_streams_t));\n     int64_t num_streams = num_streams_t->scalar<int64>()();\n+    OP_REQUIRES(context, num_streams >= 0,\n+                errors::InvalidArgument(\n+                    \"Num_streams input cannot be a negative integer\"));\n \n     auto result =\n         new QuantileStreamResource(epsilon, max_elements_, num_streams);\n"
        ],
        "Title": "\n          Crash caused by integer conversion to unsigned\n        "
    },
    {
        "Bug description": "An attacker can cause a floating point exception by calling inplace operations with crafted arguments that would result in a division by 0:",
        "Sample Code": " tensorflow as tf\n\ntf.raw_ops.InplaceSub(x=[],i=[-99,-1,-1],v=[1,1,1])",
        "Bug fix": [
            "@@ -225,7 +225,7 @@ class InplaceOpBase : public OpKernel {\n \n     Tensor y = x;  // This creates an alias intentionally.\n     // Skip processing if tensors are empty.\n-    if (x.NumElements() > 0 || v.NumElements() > 0) {\n+    if (x.NumElements() > 0 && v.NumElements() > 0) {\n       OP_REQUIRES_OK(ctx, DoCompute(ctx, i, v, &y));\n     }\n     ctx->set_output(0, y);\n"
        ],
        "Title": "\n          Division by 0 in inplace operations\n        "
    },
    {
        "Bug description": "An attacker can cause undefined behavior via binding a reference to null pointer in all binary cwise operations that don't require broadcasting (e.g., gradients of binary cwise operations):",
        "Sample Code": " tensorflow as tf\n\ntf.raw_ops.SqrtGrad(y=[4, 16],dy=[])",
        "Bug fix": [
            "@@ -265,6 +265,11 @@ class SimpleBinaryOp : public OpKernel {\n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& in0 = ctx->input(0);\n     const Tensor& in1 = ctx->input(1);\n+    OP_REQUIRES(\n+        ctx, in0.NumElements() == in1.NumElements(),\n+        errors::InvalidArgument(\"The two arguments to a cwise op must have \"\n+                                \"same number of elements, got \",\n+                                in0.NumElements(), \" and \", in1.NumElements()));\n     auto in0_flat = in0.flat<Tin>();\n     auto in1_flat = in1.flat<Tin>();\n     const Device& eigen_device = ctx->eigen_device<Device>();\n"
        ],
        "Title": "\n          Reference binding to nullptr and heap OOB in binary cwise ops\n        "
    },
    {
        "Bug description": "An attacker can cause undefined behavior via binding a reference to null pointer in all operations of type  tf.raw_ops.MatrixSetDiagV* :",
        "Sample Code": "tf.raw_ops.MatrixSetDiagV3(\n  input=[1,2,3],\n  diagonal=[1,1],\n  k=[],\n  [],\n  align='RIGHT_LEFT')",
        "Bug fix": [
            "@@ -70,6 +70,9 @@ class MatrixSetDiagOp : public OpKernel {\n                   errors::InvalidArgument(\n                       \"diag_index must be a scalar or vector, received shape: \",\n                       diag_index.shape().DebugString()));\n+      OP_REQUIRES(\n+          context, diag_index.NumElements() > 0,\n+          errors::InvalidArgument(\"diag_index must have at least one element\"));\n       lower_diag_index = diag_index.flat<int32>()(0);\n       upper_diag_index = lower_diag_index;\n       if (TensorShapeUtils::IsVector(diag_index.shape())) {\n"
        ],
        "Title": "\n          Reference binding to nullptr in `MatrixSetDiagV*` ops\n        "
    }
]
[
    {
        "Bug description": "An attacker can cause undefined behavior via binding a reference to null pointer in all operations of type  tf.raw_ops.MatrixDiagV* :",
        "Sample Code": "tf.raw_ops.MatrixDiagV3(\n  diagonal=[1,0],\n  k=[],\n  num_rows=[1,2,3],\n  num_cols=[4,5],\n  padding_value=[],\n  [],\n  align='RIGHT_RIGHT')",
        "Bug fix": [
            "@@ -73,6 +73,9 @@ class MatrixDiagPartOp : public OpKernel {\n                   errors::InvalidArgument(\n                       \"diag_index must be a scalar or vector, received shape: \",\n                       diag_index.shape().DebugString()));\n+      OP_REQUIRES(context, diag_index.NumElements() > 0,\n+                  errors::InvalidArgument(\n+                      \"Expected diag_index to have at least 1 element\"));\n       lower_diag_index = diag_index.flat<int32>()(0);\n       upper_diag_index = lower_diag_index;\n       if (TensorShapeUtils::IsVector(diag_index.shape())) {\n@@ -179,6 +182,9 @@ class MatrixDiagOp : public OpKernel {\n                   errors::InvalidArgument(\n                       \"diag_index must be a scalar or vector, received shape: \",\n                       diag_index.shape().DebugString()));\n+      OP_REQUIRES(context, diag_index.NumElements() > 0,\n+                  errors::InvalidArgument(\n+                      \"Expected diag_index to have at least 1 element\"));\n       lower_diag_index = diag_index.flat<int32>()(0);\n       upper_diag_index = lower_diag_index;\n       if (TensorShapeUtils::IsVector(diag_index.shape())) {\n"
        ],
        "Title": "\n          Reference binding to nullptr in `MatrixDiagV*` ops\n        "
    },
    {
        "Bug description": "An attacker can cause undefined behavior via binding a reference to null pointer in  tf.raw_ops.RaggedTensorToSparse :",
        "Sample Code": "tf.raw_ops.RaggedTensorToSparse(\n  rt_nested_splits=[[0, 38, 0]],\n  ]],\n  rt_dense_values=[])",
        "Bug fix": [
            "@@ -21,6 +21,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n \n namespace tensorflow {\n \n@@ -38,7 +39,8 @@ class RaggedTensorToSparseOp : public OpKernel {\n     OP_REQUIRES_OK(\n         context, context->input_list(\"rt_nested_splits\", &rt_nested_splits_in));\n     const int rt_nested_splits_len = rt_nested_splits_in.size();\n-    DCHECK_GT(rt_nested_splits_len, 0);  // Enforced by REGISTER_OP.\n+    OP_REQUIRES(context, rt_nested_splits_len > 0,\n+                errors::InvalidArgument(\"rt_nested_splits must be non empty\"));\n     std::vector<ConstFlatSplits> rt_nested_splits;\n     rt_nested_splits.reserve(rt_nested_splits_len);\n     for (int i = 0; i < rt_nested_splits_len; ++i) {\n@@ -162,6 +164,14 @@ class RaggedTensorToSparseOp : public OpKernel {\n       if (rt_nested_splits[i](0) != 0) {\n         return InvalidArgument(\"First value of ragged splits must be 0.\");\n       }\n+      for (int j = 1; j < rt_nested_splits[i].size(); ++j) {\n+        if (rt_nested_splits[i](j) < rt_nested_splits[i](j - 1)) {\n+          return InvalidArgument(\n+              \"Ragged splits should be non decreasing, but we got \",\n+              rt_nested_splits[i](j - 1), \" followed by \",\n+              rt_nested_splits[i](j));\n+        }\n+      }\n       if (i > 0) {\n         SPLITS_TYPE last_split =\n             rt_nested_splits[i - 1](rt_nested_splits[i - 1].size() - 1);\n"
        ],
        "Title": "\n          Reference binding to nullptr in `RaggedTensorToSparse`\n        "
    },
    {
        "Bug description": "An attacker can trigger a read from outside of bounds of heap allocated data by sending invalid arguments to  tf.raw_ops.ResourceScatterUpdate :",
        "Sample Code": "v = tf.Variable([b'vvv'])\ntf.raw_ops.ResourceScatterUpdate(\n  resource=v.handle,\n  indices=[0],\n  ],\n  updates=['1', '2', '3', '4', '5'])",
        "Bug fix": [
            "@@ -955,11 +955,12 @@ class ResourceScatterUpdateOp : public OpKernel {\n                         params->dim_size(0), \")\"));\n       } else {\n         int64_t num_updates = updates.NumElements();\n-        OP_REQUIRES(c, num_updates % N == 0,\n-                    errors::InvalidArgument(\n-                        \"shape of indices (\", indices.shape().DebugString(),\n-                        \") is not compatible with the shape of updates (\",\n-                        updates.shape().DebugString(), \")\"));\n+        OP_REQUIRES(\n+            c, TensorShapeUtils::StartsWith(updates.shape(), indices.shape()),\n+            errors::InvalidArgument(\n+                \"The shape of indices (\", indices.shape().DebugString(),\n+                \") must be a prefix of the shape of updates (\",\n+                updates.shape().DebugString(), \")\"));\n         auto updates_flat = updates.shaped<T, 2>({N, num_updates / N});\n \n         functor::ScatterFunctor<Device, T, Index, op> functor;\n"
        ],
        "Title": "\n          Heap OOB in `ResourceScatterUpdate`\n        "
    },
    {
        "Bug description": "An attacker can trigger a crash via a  CHECK -fail in debug builds of TensorFlow using  tf.raw_ops.ResourceGather  or a read from outside the bounds of heap allocated data in the same API in a release build:",
        "Sample Code": "tensor = tf.constant(value=[[1,2],[3,4],[5,6]],shape=(3,2),dtype=tf.uint32)\nv = tf.Variable(tensor)\ntf.raw_ops.ResourceGather(\n  resource=v.handle,\n  indices=[0],\n  dtype=tf.uint32,\n  batch_dims=10,\n  ,\n  validate_indices=False)",
        "Bug fix": [
            "@@ -660,6 +660,11 @@ class ResourceGatherOp : public OpKernel {\n     OP_REQUIRES(\n         c, TensorShapeUtils::IsVectorOrHigher(params.shape()),\n         errors::InvalidArgument(\"params must be at least 1 dimensional\"));\n+    OP_REQUIRES(\n+        c, params.shape().dims() >= batch_dims_,\n+        errors::InvalidArgument(\"params must have at least \", batch_dims_,\n+                                \" (batch_dims) dimensions but it has shape \",\n+                                params.shape().DebugString()));\n \n     // Check that we have enough index space\n     const int64_t N = indices.NumElements();\n"
        ],
        "Title": "\n          Heap OOB and CHECK fail in `ResourceGather`\n        "
    },
    {
        "Bug description": "An attacker can trigger a crash via a floating point exception in  tf.raw_ops.ResourceGather :",
        "Sample Code": "tensor = tf.constant(value=[[]],shape=(0,1),dtype=tf.uint32)\nv = tf.Variable(tensor)\ntf.raw_ops.ResourceGather(\n  resource=v.handle,\n  indices=[0],\n  dtype=tf.uint32,\n  batch_dims=1,\n  ,\n  validate_indices=False)",
        "Bug fix": [
            "@@ -710,7 +710,8 @@ class ResourceGatherOp : public OpKernel {\n         copy_functor(c->eigen_device<Device>(), tmp_indices.flat<Index>(),\n                      indices.flat<Index>());\n \n-        AddBatchOffsets(&tmp_indices, params);\n+        AddBatchOffsets(c, &tmp_indices, params);\n+        if (!c->status().ok()) return;\n         op_indices = &tmp_indices;\n       }\n \n@@ -742,11 +743,17 @@ class ResourceGatherOp : public OpKernel {\n   // Example: batch_dims = 1, indices = [[0, 1, 2], [0, 1, 2]]\n   // If indexing into a params dimension of size 4, then the indices will become\n   // [0, 1, 2, 4, 5, 6]\n-  void AddBatchOffsets(Tensor* indices, const Tensor& params) {\n+  void AddBatchOffsets(OpKernelContext* ctx, Tensor* indices,\n+                       const Tensor& params) {\n     int64_t batch_size = 1;  // The size of all batch dimensions.\n     for (int idx = 0; idx < batch_dims_; ++idx) {\n       batch_size *= params.dim_size(idx);\n     }\n+    OP_REQUIRES(\n+        ctx, batch_size != 0,\n+        errors::InvalidArgument(\n+            \"Inner size of indices would result in batch_size of 0 and a \",\n+            \"division by 0 in the implementation. This is illegal\"));\n \n     auto indices_flat = indices->flat<Index>();\n     int64_t const index_inner_size = indices->NumElements() / batch_size;\n"
        ],
        "Title": "\n          Division by 0 in `ResourceGather`\n        "
    },
    {
        "Bug description": "The implementation for  tf.raw_ops.BoostedTreesCreateEnsemble  can result in a use after free error if an attacker supplies specially crafted arguments:",
        "Sample Code": "v= tf.Variable([0.0])\ntf.raw_ops.BoostedTreesCreateEnsemble(\n  tree_ensemble_handle=v.handle,\n  stamp_token=[0],\n  ],\n  tree_ensemble_serialized=['0']) ",
        "Bug fix": [
            "@@ -53,6 +53,7 @@ class BoostedTreesCreateEnsembleOp : public OpKernel {\n     if (!result->InitFromSerialized(\n             tree_ensemble_serialized_t->scalar<tstring>()(), stamp_token)) {\n       result->Unref();\n+      result.release();  // Needed due to the `->Unref` above, to prevent UAF\n       OP_REQUIRES(\n           context, false,\n           errors::InvalidArgument(\"Unable to parse tree ensemble proto.\"));\n"
        ],
        "Title": "\n          Use after free in boosted trees creation\n        "
    },
    {
        "Bug description": "The implementation for  tf.raw_ops.FractionalAvgPoolGrad  can be tricked into accessing data outside of bounds of heap allocated buffers:",
        "Sample Code": "tf.raw_ops.FractionalAvgPoolGrad(\n  orig_input_tensor_shape=[0,1,2,3],\n  out_backprop = np.array([[[[541],[541]],[[541],[541]]]]),\n  row_pooling_sequence=[0, 0, 0, 0, 0],\n  col_pooling_sequence=[-2, 0, 0, 2, 0],\n  ],\n  overlapping=True)",
        "Bug fix": [
            "@@ -271,6 +271,18 @@ class FractionalAvgPoolGradOp : public OpKernel {\n     const int64_t in_rows = orig_input_tensor_shape_flat(1);\n     const int64_t in_cols = orig_input_tensor_shape_flat(2);\n     const int64_t in_depth = orig_input_tensor_shape_flat(3);\n+    OP_REQUIRES(\n+        context, in_batch != 0,\n+        errors::InvalidArgument(\"Batch dimension of input must not be 0\"));\n+    OP_REQUIRES(\n+        context, in_rows != 0,\n+        errors::InvalidArgument(\"Rows dimension of input must not be 0\"));\n+    OP_REQUIRES(\n+        context, in_cols != 0,\n+        errors::InvalidArgument(\"Columns dimension of input must not be 0\"));\n+    OP_REQUIRES(\n+        context, in_depth != 0,\n+        errors::InvalidArgument(\"Depth dimension of input must not be 0\"));\n \n     constexpr int tensor_in_and_out_dims = 4;\n     // Transform orig_input_tensor_shape into TensorShape\n"
        ],
        "Title": "\n          Heap buffer overflow in `FractionalAvgPoolGrad`\n        "
    },
    {
        "Bug description": "The implementation for  tf.raw_ops.ExperimentalDatasetToTFRecord  and  tf.raw_ops.DatasetToTFRecord  can trigger heap buffer overflow and segmentation fault:",
        "Sample Code": "dataset = tf.data.Dataset.range(3)\ndataset = tf.data.experimental.to_variant(dataset)\ntf.raw_ops.ExperimentalDatasetToTFRecord(\n  input_dataset=dataset,\n  filename='/tmp/output',\n  ,\n  compression_type='')",
        "Bug fix": [
            "@@ -18,6 +18,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/function_handle_cache.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/resource_mgr.h\"\n+#include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/ops_util.h\"\n #include \"tensorflow/core/lib/core/threadpool.h\"\n #include \"tensorflow/core/lib/io/record_writer.h\"\n@@ -91,8 +92,20 @@ class ToTFRecordOp : public AsyncOpKernel {\n     TF_RETURN_IF_ERROR(finalized_dataset->MakeIterator(\n         &iter_ctx, /*parent=*/nullptr, \"ToTFRecordOpIterator\", &iterator));\n \n+    const int num_output_dtypes = finalized_dataset->output_dtypes().size();\n+    if (num_output_dtypes != 1) {\n+      return errors::InvalidArgument(\n+          \"ToTFRecordOp currently only support datasets of 1 single column, \",\n+          \"but got \", num_output_dtypes);\n+    }\n+    const DataType dt = finalized_dataset->output_dtypes()[0];\n+    if (dt != DT_STRING) {\n+      return errors::InvalidArgument(\n+          \"ToTFRecordOp currently only supports DT_STRING dataypes, but got \",\n+          DataTypeString(dt));\n+    }\n     std::vector<Tensor> components;\n-    components.reserve(finalized_dataset->output_dtypes().size());\n+    components.reserve(num_output_dtypes);\n     bool end_of_sequence;\n     do {\n       TF_RETURN_IF_ERROR(\n"
        ],
        "Title": "\n          Segfault and heap buffer overflow in `{Experimental,}DatasetToTFRecord`\n        "
    },
    {
        "Bug description": "The code for  tf.raw_ops.UncompressElement  can be made to trigger a null pointer dereference:",
        "Sample Code": "data = tf.data.Dataset.from_tensors([0.0])\ntf.raw_ops.UncompressElement(\n  compressed=tf.data.experimental.to_variant(data),\n  output_types=[tf.int64],\n  ],\n  output_shapes=[2])",
        "Bug fix": [
            "@@ -48,6 +48,11 @@ void UncompressElementOp::Compute(OpKernelContext* ctx) {\n   Tensor tensor = ctx->input(0);\n   const Variant& variant = tensor.scalar<Variant>()();\n   const CompressedElement* compressed = variant.get<CompressedElement>();\n+  OP_REQUIRES(\n+      ctx, compressed != nullptr,\n+      errors::InvalidArgument(\n+          \"Input does not contain a compressed element. Instead got tensor \",\n+          tensor.DebugString()));\n \n   std::vector<Tensor> components;\n   OP_REQUIRES_OK(ctx, UncompressElement(*compressed, &components));\n"
        ],
        "Title": "\n          Null pointer dereference in `UncompressElement`\n        "
    },
    {
        "Bug description": "The code for  tf.raw_ops.SaveV2  does not properly validate the inputs and an attacker can trigger a null pointer dereference:",
        "Sample Code": "tf.raw_ops.SaveV2(\n  prefix=['tensorflow'],\n  tensor_name=['v'],\n  shape_and_slices=[],\n  [],\n  tensors=[1,2,3])",
        "Bug fix": [
            "@@ -98,6 +98,7 @@ class SaveV2 : public OpKernel {\n     const Tensor& shape_and_slices = context->input(2);\n     ValidateInputs(true /* is save op */, context, prefix, tensor_names,\n                    shape_and_slices);\n+    if (!context->status().ok()) return;\n \n     const int kFixedInputs = 3;  // Prefix, tensor names, shape_and_slices.\n     const int num_tensors = static_cast<int>(tensor_names.NumElements());\n@@ -177,6 +178,7 @@ class RestoreV2 : public OpKernel {\n                                         \" expected dtypes.\"));\n     ValidateInputs(false /* not save op */, context, prefix, tensor_names,\n                    shape_and_slices);\n+    if (!context->status().ok()) return;\n \n     const string& prefix_string = prefix.scalar<tstring>()();\n \n"
        ],
        "Title": "\n          Incorrect validation of `SaveV2` inputs\n        "
    }
]
[
    {
        "Bug description": "When a user does not supply arguments that determine a valid sparse tensor,  tf.raw_ops.SparseTensorSliceDataset  implementation can be made to dereference a null pointer:",
        "Sample Code": "tf.raw_ops.SparseTensorSliceDataset(\n  indices=[[],[],[]],\n  values=[1,2,3],\n  ],\n  dense_shape=[3,3])",
        "Bug fix": [
            "@@ -241,6 +241,17 @@ class SparseTensorSliceDatasetOp : public DatasetOpKernel {\n                 errors::InvalidArgument(\n                     \"Input indices should be a matrix but received shape \",\n                     indices->shape().DebugString()));\n+\n+    const auto num_indices = indices->NumElements();\n+    const auto num_values = values->NumElements();\n+    if (num_indices == 0 || num_values == 0) {\n+      OP_REQUIRES(ctx, num_indices == num_values,\n+                  errors::InvalidArgument(\n+                      \"If indices or values are empty, the other one must also \"\n+                      \"be. Got indices of shape \",\n+                      indices->shape().DebugString(), \" and values of shape \",\n+                      values->shape().DebugString()));\n+    }\n     OP_REQUIRES(ctx, TensorShapeUtils::IsVector(values->shape()),\n                 errors::InvalidArgument(\n                     \"Input values should be a vector but received shape \",\n",
            "@@ -118,6 +118,26 @@ class FromSparseTensorSlicesTest(test_base.DatasetTestBase,\n       with self.assertRaises(errors.OutOfRangeError):\n         sess.run(get_next)\n \n+  @combinations.generate(combinations.combine(tf_api_version=1, mode=[\"graph\"]))\n+  def testEmptySparseTensorSlicesInvalid(self):\n+    \"\"\"Test a dataset based on invalid `tf.sparse.SparseTensor`.\"\"\"\n+    st = array_ops.sparse_placeholder(dtypes.float64)\n+    iterator = dataset_ops.make_initializable_iterator(\n+        dataset_ops.Dataset.from_sparse_tensor_slices(st))\n+    init_op = iterator.initializer\n+\n+    with self.cached_session() as sess:\n+      # Test with an empty sparse tensor but with non empty values.\n+      empty_indices = np.empty((0, 4), dtype=np.int64)\n+      non_empty_values = [1, 2, 3, 4]\n+      empty_dense_shape = [0, 4, 37, 9]\n+      sparse_feed = sparse_tensor.SparseTensorValue(empty_indices,\n+                                                    non_empty_values,\n+                                                    empty_dense_shape)\n+      # Here, we expect the test to fail when running the feed.\n+      with self.assertRaises(errors.InvalidArgumentError):\n+        sess.run(init_op, feed_dict={st: sparse_feed})\n+\n   @combinations.generate(combinations.combine(tf_api_version=2, mode=[\"eager\"]))\n   def testFromSparseTensorSlicesError(self):\n     with self.assertRaises(AttributeError):\n"
        ],
        "Title": "\n          Null pointer dereference in `SparseTensorSliceDataset`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.StringNGrams  is vulnerable to an integer overflow issue caused by converting a signed integer value to an unsigned one and then allocating memory based on this value.",
        "Sample Code": "tf.raw_ops.StringNGrams(\n  data=['',''],\n  data_splits=[0,2],\n  separator=' '*100,\n  ngram_widths=[-80,0,0,-60],\n  left_pad=' ',\n  right_pad=' ',\n  pad_width=100,\n  ,\n  preserve_short_sequences=False)",
        "Bug fix": [
            "@@ -53,6 +53,12 @@ class StringNGramsOp : public tensorflow::OpKernel {\n   }\n \n   void Compute(tensorflow::OpKernelContext* context) override {\n+    for (int ngram_width : ngram_widths_) {\n+      OP_REQUIRES(\n+          context, ngram_width > 0,\n+          errors::InvalidArgument(\"ngram_widths must contain positive values\"));\n+    }\n+\n     const tensorflow::Tensor* data;\n     OP_REQUIRES_OK(context, context->input(\"data\", &data));\n     const auto& input_data = data->flat<tstring>().data();\n"
        ],
        "Title": "\n          Bad alloc in `StringNGrams` caused by integer conversion\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.QuantizeAndDequantizeV4Grad  is vulnerable to an integer overflow issue caused by converting a signed integer value to an unsigned one and then allocating memory based on this value.",
        "Sample Code": "tf.raw_ops.QuantizeAndDequantizeV4Grad(\n  gradients=[1.0,2.0],\n  input=[1.0,1.0],\n  input_min=[0.0],\n  input_max=[10.0],\n  ],\n  axis=-100)",
        "Bug fix": [
            "@@ -158,6 +158,13 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\n     Tensor* input_backprop = nullptr;\n     OP_REQUIRES_OK(ctx,\n                    ctx->allocate_output(0, input.shape(), &input_backprop));\n+    OP_REQUIRES(\n+        ctx, axis_ >= -1,\n+        errors::InvalidArgument(\"Axis must be at least -1. Found \", axis_));\n+    OP_REQUIRES(ctx, (axis_ == -1 || axis_ < input.shape().dims()),\n+                errors::InvalidArgument(\n+                    \"Axis should be -1 or 0 or a positive value less than \",\n+                    input.shape().dims(), \"but given axis value was \", axis_));\n \n     OP_REQUIRES(\n         ctx, input.IsSameSize(gradient),\n"
        ],
        "Title": "\n          Integer overflow due to conversion to unsigned\n        "
    },
    {
        "Bug description": "If a user does not provide a valid padding value to  tf.raw_ops.MatrixDiagPartOp , then the code triggers a null pointer dereference (if input is empty) or produces invalid behavior, ignoring all values after the first:",
        "Sample Code": "tf.raw_ops.MatrixDiagPartV2(\n  input=tf.ones(2,dtype=tf.int32),\n  k=tf.ones(2,dtype=tf.int32),\n  ),\n  padding_value=[])",
        "Bug fix": [
            "@@ -89,7 +89,10 @@ class MatrixDiagPartOp : public OpKernel {\n           upper_diag_index = diag_index.flat<int32>()(1);\n         }\n       }\n-      padding_value = context->input(2).flat<T>()(0);\n+      const Tensor& padding_in = context->input(2);\n+      OP_REQUIRES(context, padding_in.NumElements() == 1,\n+                  errors::InvalidArgument(\"Padding must be scalar.\"));\n+      padding_value = padding_in.flat<T>()(0);\n     }\n     const TensorShape& input_shape = input.shape();\n \n"
        ],
        "Title": "\n          Null pointer dereference in `MatrixDiagPartOp`\n        "
    },
    {
        "Bug description": "Providing a negative element to  num_elements  list argument of   tf.raw_ops.TensorListReserve  causes the runtime to abort the process due to reallocating a  std::vector  to have a negative number of elements:",
        "Sample Code": "tf.raw_ops.TensorListReserve(\n  element_shape = tf.constant([1]),\n  num_elements=tf.constant([-1]),\n  ]),\n  element_dtype = tf.int32)",
        "Bug fix": [
            "@@ -302,6 +302,10 @@ class TensorListReserve : public OpKernel {\n     PartialTensorShape element_shape;\n     OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(0), &element_shape));\n     int32 num_elements = c->input(1).scalar<int32>()();\n+    OP_REQUIRES(c, num_elements >= 0,\n+                errors::InvalidArgument(\"The num_elements to reserve must be a \"\n+                                        \"non negative number, but got \",\n+                                        num_elements));\n     TensorList output;\n     output.element_shape = element_shape;\n     output.element_dtype = element_dtype_;\n"
        ],
        "Title": "\n          `std::abort` raised from `TensorListReserve`\n        "
    },
    {
        "Bug description": "If the arguments to  tf.raw_ops.RaggedGather  don't determine a valid ragged tensor code can trigger a read from outside of bounds of heap allocated buffers.",
        "Sample Code": "tf.raw_ops.RaggedGather(\n  params_nested_splits = [0,0,0],\n  params_dense_values = [1,1],\n  indices = [0,0,9,0,0],\n  ],\n  OUTPUT_RAGGED_RANK=0)",
        "Bug fix": [
            "@@ -58,15 +58,21 @@ class RaggedGatherOpBase : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     // Get the input Tensors.\n+\n     OpInputList params_nested_splits_in;\n     OP_REQUIRES_OK(context, context->input_list(\"params_nested_splits\",\n                                                 &params_nested_splits_in));\n+    OP_REQUIRES(\n+        context, params_nested_splits_in.size() > 0,\n+        errors::InvalidArgument(\"params_nested_splits must be non empty\"));\n+\n     const Tensor& params_dense_values_in =\n         context->input(params_nested_splits_in.size());\n     const Tensor& indices_in =\n         context->input(params_nested_splits_in.size() + 1);\n \n-    DCHECK_GT(params_nested_splits_in.size(), 0);  // Enforced by REGISTER_OP.\n+    OP_REQUIRES(context, params_nested_splits_in[0].dims() > 0,\n+                errors::InvalidArgument(\"Split tensors must not be scalars\"));\n     SPLITS_TYPE num_params = params_nested_splits_in[0].dim_size(0) - 1;\n     OP_REQUIRES_OK(context, ValidateIndices(indices_in, num_params));\n \n"
        ],
        "Title": "\n          Heap OOB in `RaggedGather`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.ResourceScatterDiv  is vulnerable to a division by 0 error:",
        "Sample Code": "v= tf.Variable([1,2,3])\ntf.raw_ops.ResourceScatterDiv(\n  resource=v.handle,\n  indices=[1],\n  ],\n  updates=[0])",
        "Bug fix": [
            "@@ -873,6 +873,35 @@ TF_CALL_GPU_NUMBER_TYPES(REGISTER_GATHER_ND_GPU);\n #undef REGISTER_GATHER_ND_ALL_INDICES\n #undef REGISTER_GATHER_ND_FULL\n \n+namespace {\n+\n+template <typename Device>\n+bool isCPUDevice() {\n+  return false;\n+}\n+\n+template <>\n+bool isCPUDevice<CPUDevice>() {\n+  return true;\n+}\n+\n+template <typename T>\n+bool ValidateInput(const Tensor& updates) {\n+  const auto updates_flat = updates.flat<T>();\n+  const T zero(0);\n+  for (int i = 0; i < updates.NumElements(); i++) {\n+    if (updates_flat(i) == zero) return false;\n+  }\n+  return true;\n+}\n+\n+template <>\n+bool ValidateInput<Variant>(const Tensor& updates) {\n+  return true;\n+}\n+\n+}  // namespace\n+\n template <typename Device, typename T, typename Index, scatter_op::UpdateOp op>\n class ResourceScatterUpdateOp : public OpKernel {\n  public:\n@@ -939,6 +968,12 @@ class ResourceScatterUpdateOp : public OpKernel {\n                                 \" indexing: \", params->dim_size(0), \" > \",\n                                 std::numeric_limits<Index>::max()));\n \n+    // Prevent division by 0\n+    if (isCPUDevice<Device>() && op == tensorflow::scatter_op::UpdateOp::DIV) {\n+      OP_REQUIRES(c, ValidateInput<T>(updates),\n+                  errors::InvalidArgument(\"updates must not contain 0\"));\n+    }\n+\n     if (N > 0) {\n       auto indices_flat = indices.flat<Index>();\n       auto params_flat = params->flat_outer_dims<T>();\n",
            "@@ -175,8 +175,9 @@ class ShardedVariableTest(test.TestCase, parameterized.TestCase):\n                             'scatter_update')\n   def test_scatter_ops_even_partition(self, op):\n     v = variables_lib.Variable(array_ops.zeros((30, 1)))\n+    # Make sure values does not contain 0 due to testing `scatter_div`!\n     sparse_delta = ops.IndexedSlices(\n-        values=constant_op.constant([[0.], [1.], [2.], [3.], [4.]]),\n+        values=constant_op.constant([[1.], [2.], [3.], [4.], [5.]]),\n         indices=constant_op.constant([0, 10, 12, 21, 22]))\n \n     v0 = variables_lib.Variable(array_ops.zeros((10, 1)))\n"
        ],
        "Title": "\n          Division by 0 in `ResourceScatterDiv`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.SparseReshape  can be made to trigger an integral division by 0 exception:",
        "Sample Code": "tf.raw_ops.SparseReshape(\n  input_indices = np.ones((1,3)),\n  input_shape = np.array([1,1,0]),\n  ]),\n  new_shape = np.array([1,0]))",
        "Bug fix": [
            "@@ -174,6 +174,12 @@ void ReshapeSparseTensor(OpKernelContext *context,\n                                           TensorShape({nnz, output_rank}),\n                                           &result_indices));\n   if (nnz > 0) {\n+    OP_REQUIRES(\n+        context, dense_size > 0 && product > 0,\n+        errors::InvalidArgument(\n+            \"Input tensor has \", nnz, \" non zero elements but input shape (\",\n+            input_shape.DebugString(), \") or output shape (\",\n+            output_shape.DebugString(), \") is empty\"));\n     OP_REQUIRES_OK(context, functor::ReshapeSparseTensorFunctor<Device>()(\n                                 context, input_shape, output_shape,\n                                 input_indices_in.matrix<int64>(),\n"
        ],
        "Title": "\n          Integer division by 0 in sparse reshaping\n        "
    },
    {
        "Bug description": "When restoring tensors via raw APIs, if the tensor name is not provided, TensorFlow can be tricked into dereferencing a null pointer:",
        "Sample Code": "tf.raw_ops.Restore(\n  file_pattern=['/tmp'],\n  tensor_name=['x'], \n  default_value=21,\n  dt=tf.int,\n  ,\n  preferred_shard=42)",
        "Bug fix": [
            "@@ -151,11 +151,18 @@ void RestoreTensor(OpKernelContext* context,\n         context, size == 1,\n         errors::InvalidArgument(\n             \"Input 0 (file_pattern) must be a string scalar; got a tensor of \",\n-            size, \"elements\"));\n+            size, \" elements\"));\n   }\n   const string& file_pattern = file_pattern_t.flat<tstring>()(0);\n \n   const Tensor& tensor_name_t = context->input(1);\n+  {\n+    const int64_t size = tensor_name_t.NumElements();\n+    OP_REQUIRES(context, size > restore_index,\n+                errors::InvalidArgument(\n+                    \"Input 1 (file_pattern) must be a have at least \",\n+                    restore_index + 1, \" elements\"));\n+  }\n   const string& tensor_name = tensor_name_t.flat<tstring>()(restore_index);\n \n   // If we cannot find a cached reader we will allocate our own.\n"
        ],
        "Title": "\n          Null pointer dereference and heap OOB read in operations restoring tensors\n        "
    },
    {
        "Bug description": "Sending invalid argument for  row_partition_types  of  tf.raw_ops.RaggedTensorToTensor  API results in a null pointer dereference and undefined behavior:",
        "Sample Code": "tf.raw_ops.RaggedTensorToTensor(\n  shape=1,\n  values=10,\n  default_value=21,\n  row_partition_tensors=tf.constant([0,0,0,0]),\n  ]),\n  row_partition_types=[])",
        "Bug fix": [
            "@@ -348,6 +348,9 @@ class RaggedTensorToTensorBaseOp : public OpKernel {\n   Status GetFirstDimensionSize(OpKernelContext* context, INDEX_TYPE* result) {\n     const Tensor first_partition_tensor =\n         context->input(kFirstPartitionInputIndex);\n+    if (row_partition_types_.empty()) {\n+      return errors::InvalidArgument(\"No row_partition_types given.\");\n+    }\n     const RowPartitionType first_partition_type = row_partition_types_[0];\n     switch (first_partition_type) {\n       case RowPartitionType::FIRST_DIM_SIZE:\n"
        ],
        "Title": "\n          Null pointer dereference in `RaggedTensorToTensor`\n        "
    }
]
[
    {
        "Bug description": "It is possible to trigger a null pointer dereference in TensorFlow by passing an invalid input to  tf.raw_ops.CompressElement :",
        "Sample Code": " tensorflow as tf\n\ntf.raw_ops.CompressElement(components=[[]])",
        "Bug fix": [
            "@@ -29,9 +29,10 @@ Status CompressElement(const std::vector<Tensor>& element,\n   int64 total_size = 0;\n   for (auto& component : element) {\n     if (DataTypeCanUseMemcpy(component.dtype())) {\n-      // Some datatypes can be memcopied, allowing us to save two copies\n-      // (AsProtoTensorContent and SerializeToArray).\n-      total_size += DMAHelper::buffer(&component)->size();\n+      const TensorBuffer* buffer = DMAHelper::buffer(&component);\n+      if (buffer) {\n+        total_size += buffer->size();\n+      }\n     } else {\n       non_memcpy_components.emplace_back();\n       component.AsProtoTensorContent(&non_memcpy_components.back());\n@@ -53,8 +54,10 @@ Status CompressElement(const std::vector<Tensor>& element,\n     component.shape().AsProto(metadata->mutable_tensor_shape());\n     if (DataTypeCanUseMemcpy(component.dtype())) {\n       const TensorBuffer* buffer = DMAHelper::buffer(&component);\n-      memcpy(position, buffer->data(), buffer->size());\n-      metadata->set_tensor_size_bytes(buffer->size());\n+      if (buffer) {\n+        memcpy(position, buffer->data(), buffer->size());\n+        metadata->set_tensor_size_bytes(buffer->size());\n+      }\n     } else {\n       TensorProto& proto = non_memcpy_components[non_memcpy_component_index++];\n       proto.SerializeToArray(position, proto.ByteSizeLong());\n@@ -94,8 +97,13 @@ Status UncompressElement(const CompressedElement& compressed,\n     if (DataTypeCanUseMemcpy(metadata.dtype())) {\n       out->emplace_back(metadata.dtype(), metadata.tensor_shape());\n       TensorBuffer* buffer = DMAHelper::buffer(&out->back());\n-      iov[i].iov_base = buffer->data();\n-      iov[i].iov_len = buffer->size();\n+      if (buffer) {\n+        iov[i].iov_base = buffer->data();\n+        iov[i].iov_len = buffer->size();\n+      } else {\n+        iov[i].iov_base = nullptr;\n+        iov[i].iov_len = 0;\n+      }\n     } else {\n       // Allocate an empty Tensor. We will fill it out later after\n       // uncompressing into the tensor_proto_str.\n"
        ],
        "Title": "\n          Null pointer dereference in `CompressElement`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.SparseDenseCwiseDiv  is vulnerable to a division by 0 error:",
        "Sample Code": "import numpy as np\n\ntf.raw_ops.SparseDenseCwiseDiv( \n  sp_indices=np.array([[4]]),\n  sp_values=np.array([-400]),\n  sp_shape=np.array([647.]),\n  ]),\n  dense=np.array([0]))",
        "Bug fix": [
            "@@ -114,7 +114,10 @@ class SparseDenseBinaryOpShared : public OpKernel {\n     OP_REQUIRES_OK(\n         ctx, ctx->allocate_temp(DataTypeToEnum<T>::value, TensorShape({nnz}),\n                                 &dense_gathered));\n-\n+    bool op_is_div = false;\n+    if (absl::StrContains(ctx->op_kernel().type_string_view(), \"Div\")) {\n+      op_is_div = true;\n+    }\n     // Pulls relevant entries from the dense side, with reshape and broadcasting\n     // *of the dense side* taken into account.  Use a TensorRef to avoid blowing\n     // up memory.\n@@ -143,6 +146,12 @@ class SparseDenseBinaryOpShared : public OpKernel {\n           errors::InvalidArgument(\"Provided indices are out-of-bounds w.r.t. \" \\\n                                   \"dense side with broadcasted shape\"));       \\\n       dense_gathered_flat(i) = rhs_ref.coeff(idx);                             \\\n+      if (op_is_div) {                                                         \\\n+        OP_REQUIRES(ctx, dense_gathered_flat(i) != 0,                          \\\n+                    errors::InvalidArgument(                                   \\\n+                        \"SparseDenseCwiseDiv cannot divide by zero,\"           \\\n+                        \"but input dense tensor contains zero \"));             \\\n+      }                                                                        \\\n     }                                                                          \\\n     break;                                                                     \\\n   }\n"
        ],
        "Title": "\n          Floating point exception in `SparseDenseCwiseDiv`\n        "
    },
    {
        "Bug description": "The implementation of sparse reduction operations in TensorFlow can trigger accesses outside of bounds of heap allocated data:",
        "Sample Code": "x = tf.SparseTensor(\n      indices=[[773, 773, 773], [773, 773, 773]],\n      values=[1, 1],\n      dense_shape=[337, 337, 337])\n])\ntf.sparse.reduce_sum(x, 1)",
        "Bug fix": [
            "@@ -219,7 +219,20 @@ class SparseReduceOp : public OpKernel {\n     sp.Reorder<T>(reduction.reorder_dims);\n     for (const auto &g : sp.group(reduction.group_by_dims)) {\n       Op::template Run<T>(ctx, reduced_val, g.template values<T>());\n+      OP_REQUIRES(ctx,\n+                  output_strides.empty() ||\n+                  (g.group().size() == output_strides.size()),\n+                  errors::Internal(\n+                      \"Expected group size and output_strides size to match\",\n+                      \", but got \", g.group().size(), \" and \",\n+                      output_strides.size()));\n       const int64_t idx = CoordinatesToFlatIndex(g.group(), output_strides);\n+      OP_REQUIRES(ctx,\n+                  idx >= 0 && idx < out_flat.size(),\n+                  errors::Internal(\n+                      \"Obtained a write index of \", idx,\n+                      \" which is outside of bounds of [0, \",\n+                      out_flat.size(), \")\"));\n       out_flat(idx) = reduced_val();\n       VLOG(2) << \"coords: \" << absl::StrJoin(g.group(), \",\")\n               << \"; idx: \" << idx << \"; group \" << Op::Name() << \": \"\n"
        ],
        "Title": "\n          Heap out of bounds access in sparse reduction operations\n        "
    },
    {
        "Bug description": "Passing invalid arguments (e.g., discovered via fuzzing) to  tf.raw_ops.SparseCountSparseOutput  results in segfault.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -192,6 +192,10 @@ class SparseCount : public OpKernel {\n               \"; values shape: \", values.shape().DebugString()));\n     }\n \n+    OP_REQUIRES(context, shape.NumElements() != 0,\n+                errors::InvalidArgument(\n+                    \"The shape argument requires at least one element.\"));\n+\n     bool is_1d = shape.NumElements() == 1;\n     int num_batches = is_1d ? 1 : shape.flat<int64>()(0);\n     int num_values = values.NumElements();\n@@ -212,6 +216,14 @@ class SparseCount : public OpKernel {\n \n     for (int idx = 0; idx < num_values; ++idx) {\n       int batch = is_1d ? 0 : indices_values(idx, 0);\n+      if (batch >= num_batches) {\n+        OP_REQUIRES(context, batch < num_batches,\n+                    errors::InvalidArgument(\n+                        \"Indices value along the first dimension must be \",\n+                        \"lower than the first index of the shape.\", \"Got \",\n+                        batch, \" as batch and \", num_batches,\n+                        \" as the first dimension of the shape.\"));\n+      }\n       const auto& value = values_values(idx);\n       if (value >= 0 && (maxlength_ <= 0 || value < maxlength_)) {\n         if (binary_output_) {\n"
        ],
        "Title": "\n          Segfault in `tf.raw_ops.SparseCountSparseOutput`\n        "
    },
    {
        "Bug description": "An attacker can cause a denial of service via  CHECK -fail in   tf.strings.substr  with invalid arguments:",
        "Sample Code": " tensorflow as tf\ntf.strings.substr(input='abc', len=1, pos=[1,2])",
        "Bug fix": "",
        "Title": "\n          Crash in `tf.strings.substr` due to `CHECK`-fail\n        "
    },
    {
        "Bug description": "Passing a complex argument to  tf.transpose  at the same time as passing  conjugate=True  argument results in a crash:",
        "Sample Code": " tensorflow as tf\ntf.transpose(conjugate=True, a=complex(1))",
        "Bug fix": "",
        "Title": "\n          Crash in `tf.transpose` with complex inputs\n        "
    },
    {
        "Bug description": "The implementation of  TrySimplify  has undefined behavior due to dereferencing a null pointer in corner cases that \nresult in optimizing a node with no inputs.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -2047,6 +2047,12 @@ class ReorderCastLikeAndValuePreserving : public ArithmeticOptimizerStage {\n \n   Status TrySimplify(NodeDef* consumer, string* simplified_node_name) override {\n     NodeDef* producer;\n+\n+    if (consumer->input_size() < 1) {\n+      return errors::FailedPrecondition(\"Node \", simplified_node_name,\n+                                        \" lacks inputs\");\n+    }\n+\n     TF_RETURN_IF_ERROR(GetInputNode(consumer->input(0), &producer));\n     const bool producer_is_cast = IsCastLike(*producer);\n     const bool can_optimize =\n@@ -2538,6 +2544,11 @@ class ReplaceMulWithSquare : public ArithmeticOptimizerStage {\n   ~ReplaceMulWithSquare() override = default;\n \n   bool IsSupported(const NodeDef* node) const override {\n+    if (!node || node->input_size() < 2) {\n+      // Invalid node\n+      return false;\n+    }\n+\n     return IsAnyMul(*node) && node->input(0) == node->input(1);\n   }\n \n",
            "@@ -68,6 +68,12 @@ bool DependencyOptimizer::SafeToRemoveIdentity(const NodeDef& node) const {\n     // The output values of this node may be needed.\n     return false;\n   }\n+\n+  if (node.input_size() < 1) {\n+    // Node lacks input, is invalid\n+    return false;\n+  }\n+\n   const NodeDef* input = node_map_->GetNode(NodeName(node.input(0)));\n   CHECK(input != nullptr) << \"node = \" << node.name()\n                           << \" input = \" << node.input(0);\n"
        ],
        "Title": "\n          Null dereference in Grappler's `TrySimplify`\n        "
    },
    {
        "Bug description": "The implementation of  ParseAttrValue  can be tricked into stack overflow due to recursion by giving in a specially crafted input.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -38,6 +38,9 @@ namespace {\n // Do not construct large tensors to compute their hash or compare for equality.\n constexpr int kMaxAttrValueTensorByteSize = 32 * 1024 * 1024;  // 32mb\n \n+// Limit nesting of tensors to 100 deep to prevent memory overflow.\n+constexpr int kMaxTensorNestDepth = 100;\n+\n // Return the size of the tensor represented by this TensorProto. If shape is\n // not fully defined return -1.\n int64 TensorByteSize(const TensorProto& t) {\n@@ -224,6 +227,54 @@ string SummarizeFunc(const NameAttrList& func) {\n   return strings::StrCat(func.name(), \"[\", absl::StrJoin(entries, \", \"), \"]\");\n }\n \n+bool ParseAttrValueHelper_TensorNestsUnderLimit(int limit, string to_parse) {\n+  int nests = 0;\n+  int maxed_out = to_parse.length();\n+  int open_curly = to_parse.find('{');\n+  int open_bracket = to_parse.find('<');\n+  int close_curly = to_parse.find('}');\n+  int close_bracket = to_parse.find('>');\n+  if (open_curly == -1) {\n+    open_curly = maxed_out;\n+  }\n+  if (open_bracket == -1) {\n+    open_bracket = maxed_out;\n+  }\n+  int min = std::min(open_curly, open_bracket);\n+  do {\n+    if (open_curly == maxed_out && open_bracket == maxed_out) {\n+      return true;\n+    }\n+    if (min == open_curly) {\n+      nests += 1;\n+      open_curly = to_parse.find('{', open_curly + 1);\n+      if (open_curly == -1) {\n+        open_curly = maxed_out;\n+      }\n+    } else if (min == open_bracket) {\n+      nests += 1;\n+      open_bracket = to_parse.find('<', open_bracket + 1);\n+      if (open_bracket == -1) {\n+        open_bracket = maxed_out;\n+      }\n+    } else if (min == close_curly) {\n+      nests -= 1;\n+      close_curly = to_parse.find('}', close_curly + 1);\n+      if (close_curly == -1) {\n+        close_curly = maxed_out;\n+      }\n+    } else if (min == close_bracket) {\n+      nests -= 1;\n+      close_bracket = to_parse.find('>', close_bracket + 1);\n+      if (close_bracket == -1) {\n+        close_bracket = maxed_out;\n+      }\n+    }\n+    min = std::min({open_curly, open_bracket, close_curly, close_bracket});\n+  } while (nests < 100);\n+  return false;\n+}\n+\n }  // namespace\n \n string SummarizeAttrValue(const AttrValue& attr_value) {\n@@ -448,7 +499,12 @@ bool ParseAttrValue(StringPiece type, StringPiece text, AttrValue* out) {\n   } else {\n     to_parse = strings::StrCat(field_name, \": \", text);\n   }\n-\n+  if (field_name == \"tensor\") {\n+    if (!ParseAttrValueHelper_TensorNestsUnderLimit(kMaxTensorNestDepth,\n+                                                    to_parse)) {\n+      return false;\n+    }\n+  }\n   return ProtoParseFromString(to_parse, out);\n }\n \n"
        ],
        "Title": "\n          Stack overflow in `ParseAttrValue` with nested tensors\n        "
    },
    {
        "Bug description": "The implementation of  tf.io.decode_raw  produces incorrect results and crashes the Python interpreter when combining  fixed_length  and wider datatypes.",
        "Sample Code": " tensorflow as tf\n\ntf.io.decode_raw(tf.constant([\"1\",\"2\",\"3\",\"4\"]), tf.uint16, fixed_length=4)",
        "Bug fix": [
            "@@ -19,6 +19,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/common_shape_fns.h\"\n #include \"tensorflow/core/framework/op.h\"\n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/op_requires.h\"\n #include \"tensorflow/core/framework/shape_inference.h\"\n \n namespace tensorflow {\n@@ -83,14 +84,13 @@ class DecodePaddedRawOp : public OpKernel {\n     // can copy the memory directly.\n     if (!convert_data_endianness_ || sizeof(T) == 1) {\n       for (int64 i = 0; i < flat_in.size(); ++i) {\n-        const T* in_data = reinterpret_cast<const T*>(flat_in(i).data());\n-\n-        if (flat_in(i).size() > fixed_length) {\n-          memcpy(out_data, in_data, fixed_length);\n-        } else {\n-          memcpy(out_data, in_data, flat_in(i).size());\n-        }\n-        out_data += fixed_length;\n+        const auto to_copy =\n+            std::min(flat_in(i).size(), static_cast<size_t>(fixed_length));\n+        memcpy(out_data, flat_in(i).data(), to_copy);\n+        // Note: increase out_data by width since it's already of type T* so\n+        // each shift amount is implicitly multiplied by sizeof(T) according to\n+        // pointer arithmetic rules.\n+        out_data += width;\n       }\n     } else {\n       // Otherwise, the data is not in the host's byte order, and rather than a\n@@ -105,7 +105,10 @@ class DecodePaddedRawOp : public OpKernel {\n              p_in += sizeof(T), p_out += sizeof(T)) {\n           std::reverse_copy(p_in, p_in + sizeof(T), p_out);\n         }\n-        out_data += fixed_length;\n+        // Note: increase out_data by width since it's already of type T* so\n+        // each shift amount is implicitly multiplied by sizeof(T) according to\n+        // pointer arithmetic rules.\n+        out_data += width;\n       }\n     }\n   }\n",
            "@@ -850,8 +850,8 @@ def decode_raw(input_bytes,\n                name=None):\n   r\"\"\"Convert raw bytes from input tensor into numeric tensors.\n \n-  The input tensor is interpreted as a sequence of bytes. These bytes are then\n-  decoded as numbers in the format specified by `out_type`.\n+  Every component of the input tensor is interpreted as a sequence of bytes.\n+  These bytes are then decoded as numbers in the format specified by `out_type`.\n \n   >>> tf.io.decode_raw(tf.constant(\"1\"), tf.uint8)\n   <tf.Tensor: shape=(1,), dtype=uint8, numpy=array([49], dtype=uint8)>\n@@ -909,22 +909,35 @@ def decode_raw(input_bytes,\n   >>> tf.io.decode_raw(tf.constant([\"1212\"]), tf.uint16, fixed_length=4)\n   <tf.Tensor: shape=(1, 2), dtype=uint16, numpy=array([[12849, 12849]], ...\n \n-  Note: There is currently a bug in `fixed_length` that can result in data loss:\n-\n-  >>> # truncated to length of type as it matches fixed_length\n-  >>> tf.io.decode_raw(tf.constant([\"1212\"]), tf.uint16, fixed_length=2)\n-  <tf.Tensor: shape=(1, 1), dtype=uint16, numpy=array([[12849]], dtype=uint16)>\n-  >>> # ignores the second component\n-  >>> tf.io.decode_raw(tf.constant([\"12\",\"34\"]), tf.uint16, fixed_length=2)\n-  <tf.Tensor: shape=(2, 1), dtype=uint16, numpy=\n-  array([[12849],\n-         [    0]], dtype=uint16)>\n-  >>> tf.io.decode_raw(tf.constant([\"12\",\"34\"]), tf.uint16, fixed_length=4)\n-  <tf.Tensor: shape=(2, 2), dtype=uint16, numpy=\n-  array([[12849,     0],\n-         [    0,     0]], dtype=uint16)>\n-\n-  This will be fixed on a future release of TensorFlow.\n+  If the input value is larger than `fixed_length`, it is truncated:\n+\n+  >>> x=''.join([chr(1), chr(2), chr(3), chr(4)])\n+  >>> tf.io.decode_raw(x, tf.uint16, fixed_length=2)\n+  <tf.Tensor: shape=(1,), dtype=uint16, numpy=array([513], dtype=uint16)>\n+  >>> hex(513)\n+  '0x201'\n+\n+  If `little_endian` and `fixed_length` are specified, truncation to the fixed\n+  length occurs before endianness conversion:\n+\n+  >>> x=''.join([chr(1), chr(2), chr(3), chr(4)])\n+  >>> tf.io.decode_raw(x, tf.uint16, fixed_length=2, little_endian=False)\n+  <tf.Tensor: shape=(1,), dtype=uint16, numpy=array([258], dtype=uint16)>\n+  >>> hex(258)\n+  '0x102'\n+\n+  If input values all have the same length, then specifying `fixed_length`\n+  equal to the size of the strings should not change output:\n+\n+  >>> x = [\"12345678\", \"87654321\"]\n+  >>> tf.io.decode_raw(x, tf.int16)\n+  <tf.Tensor: shape=(2, 4), dtype=int16, numpy=\n+  array([[12849, 13363, 13877, 14391],\n+         [14136, 13622, 13108, 12594]], dtype=int16)>\n+  >>> tf.io.decode_raw(x, tf.int16, fixed_length=len(x[0]))\n+  <tf.Tensor: shape=(2, 4), dtype=int16, numpy=\n+  array([[12849, 13363, 13877, 14391],\n+         [14136, 13622, 13108, 12594]], dtype=int16)>\n \n   Args:\n     input_bytes:\n"
        ],
        "Title": "\n          Interpreter crash from `tf.io.decode_raw`\n        "
    },
    {
        "Bug description": "Incomplete validation in  tf.raw_ops.CTCLoss  allows an attacker to trigger an OOB read from heap:",
        "Sample Code": "inputs = tf.constant([], shape=[0, 2, 11], dtype=tf.float32)\nlabels_indices = tf.constant([], shape=[0, 2], dtype=tf.int64)\nlabels_values = tf.constant([], shape=[0], dtype=tf.int32)\nsequence_length = tf.constant([-100, -100], shape=[2], dtype=tf.int32)\n\ntf.raw_ops.CTCLoss(inputs=inputs, labels_indices=labels_indices,\n                   labels_values=labels_values, sequence_length=sequence_length,\n                   preprocess_collapse_repeated=False, ctc_merge_repeated=False,\n                   ,\n                   ignore_longer_outputs_than_inputs=False)",
        "Bug fix": [
            "@@ -109,6 +109,9 @@ class CTCLossOp : public OpKernel {\n \n     const TensorShape& inputs_shape = inputs->shape();\n     const int64 max_time = inputs_shape.dim_size(0);\n+    OP_REQUIRES(ctx, max_time != 0,\n+                errors::InvalidArgument(\n+                    \"Max time or first dimension of input cannot be 0.\"));\n     const int64 batch_size = inputs_shape.dim_size(1);\n     const int64 num_classes_raw = inputs_shape.dim_size(2);\n     OP_REQUIRES(\n"
        ],
        "Title": "\n          Incomplete validation in `tf.raw_ops.CTCLoss`\n        "
    }
]
[
    {
        "Bug description": "An attacker can trigger a heap buffer overflow in Eigen implementation of  tf.raw_ops.BandedTriangularSolve :",
        "Sample Code": "import numpy as np\n  \nmatrix_array = np.array([])\nmatrix_tensor = tf.convert_to_tensor(np.reshape(matrix_array,(0,1)),dtype=tf.float32)\nrhs_array = np.array([1,1])\nrhs_tensor = tf.convert_to_tensor(np.reshape(rhs_array,(1,2)),dtype=tf.float32)\n)\ntf.raw_ops.BandedTriangularSolve(matrix=matrix_tensor,rhs=rhs_tensor)",
        "Bug fix": [
            "@@ -180,6 +180,11 @@ class SparseSparseBinaryOpShared : public OpKernel {\n                                           \" for dimension \", i));\n     }\n \n+    OP_REQUIRES(\n+        ctx, a_indices_t->dim_size(1) == b_indices_t->dim_size(1),\n+        errors::InvalidArgument(\n+            \"Indices' dimensions do not match: got \", a_indices_t->dim_size(1),\n+            \" and \", b_indices_t->dim_size(1), \" for the second dimension.\"));\n     const int num_dims = a_indices_t->dim_size(1);\n     const auto a_indices_mat = a_indices_t->matrix<int64>();\n     const auto b_indices_mat = b_indices_t->matrix<int64>();\n"
        ],
        "Title": "\n          Heap buffer overflow in `BandedTriangularSolve`\n        "
    },
    {
        "Bug description": "The validation in  tf.raw_ops.QuantizeAndDequantizeV2  allows invalid values for  axis  argument:",
        "Sample Code": "input_tensor = tf.constant([0.0], shape=[1], dtype=float)\ninput_min = tf.constant(-10.0)\ninput_max = tf.constant(-10.0)\n\ntf.raw_ops.QuantizeAndDequantizeV2(\n  input=input_tensor, input_min=input_min, input_max=input_max,\n  signed_input=False, num_bits=1, range_given=False, round_mode='HALF_TO_EVEN',\n  ,\n  narrow_range=False, axis=-2)",
        "Bug fix": [
            "@@ -72,6 +72,9 @@ class QuantizeAndDequantizeV2Op : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input = ctx->input(0);\n+    OP_REQUIRES(\n+        ctx, axis_ >= -1,\n+        errors::InvalidArgument(\"Axis must be at least -1. Found \", axis_));\n     OP_REQUIRES(\n         ctx, (axis_ == -1 || axis_ < input.shape().dims()),\n         errors::InvalidArgument(\"Shape must be at least rank \", axis_ + 1,\n"
        ],
        "Title": "\n          Invalid validation in `QuantizeAndDequantizeV2`\n        "
    },
    {
        "Bug description": "Incomplete validation in  SparseReshape  results in a denial of service based on a  CHECK -failure.",
        "Sample Code": "input_indices = tf.constant(41, shape=[1, 1], dtype=tf.int64)\ninput_shape = tf.zeros([11], dtype=tf.int64)\nnew_shape = tf.zeros([1], dtype=tf.int64)\n\ntf.raw_ops.SparseReshape(input_indices=input_indices,\n    input_shape=input_shape,\n    ,\n    new_shape=new_shape)",
        "Bug fix": [
            "@@ -26,6 +26,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/reshape_util.h\"\n #include \"tensorflow/core/lib/gtl/inlined_vector.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n \n namespace tensorflow {\n \n@@ -38,6 +39,17 @@ class SparseReshapeOp : public OpKernel {\n   explicit SparseReshapeOp(OpKernelConstruction* context) : OpKernel(context) {}\n \n   void Compute(OpKernelContext* context) override {\n+    const Tensor& input_indices_in = context->input(0);\n+    const Tensor& input_shape_in = context->input(1);\n+\n+    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_indices_in.shape()),\n+                errors::InvalidArgument(\"Input must be a matrix.\"));\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape_in.shape()),\n+                errors::InvalidArgument(\"Input shape must be a vector.\"));\n+    OP_REQUIRES(context,\n+                input_indices_in.dim_size(1) == input_shape_in.dim_size(0),\n+                errors::InvalidArgument(\n+                    \"Input tensor rank must match input shape length.\"));\n     ReshapeSparseTensor<Device>(context, context->input(0), context->input(1),\n                                 context->input(2), 0 /* output indices index */,\n                                 1 /* output shape index */);\n"
        ],
        "Title": "\n          Incomplete validation in `SparseReshape`\n        "
    },
    {
        "Bug description": "Incomplete validation in  SparseAdd  results in allowing attackers to exploit undefined behavior (dereferencing null pointers) as well as write outside of bounds of heap allocated data:",
        "Sample Code": "a_indices = tf.ones([45, 92], dtype=tf.int64)\na_values = tf.ones([45], dtype=tf.int64)\na_shape = tf.ones([1], dtype=tf.int64)\nb_indices = tf.ones([1, 1], dtype=tf.int64)\nb_values = tf.ones([1], dtype=tf.int64)\nb_shape = tf.ones([1], dtype=tf.int64)\n                    \ntf.raw_ops.SparseSparseMinimum(a_indices=a_indices,\n    a_values=a_values,\n    a_shape=a_shape,\n    b_indices=b_indices,\n    b_values=b_values,\n    ,\n    b_shape=b_shape)",
        "Bug fix": [
            "@@ -180,6 +180,11 @@ class SparseSparseBinaryOpShared : public OpKernel {\n                                           \" for dimension \", i));\n     }\n \n+    OP_REQUIRES(\n+        ctx, a_indices_t->dim_size(1) == b_indices_t->dim_size(1),\n+        errors::InvalidArgument(\n+            \"Indices' dimensions do not match: got \", a_indices_t->dim_size(1),\n+            \" and \", b_indices_t->dim_size(1), \" for the second dimension.\"));\n     const int num_dims = a_indices_t->dim_size(1);\n     const auto a_indices_mat = a_indices_t->matrix<int64>();\n     const auto b_indices_mat = b_indices_t->matrix<int64>();\n"
        ],
        "Title": "\n          Incomplete validation in `SparseSparseMinimum`\n        "
    },
    {
        "Bug description": "Incomplete validation in  SparseAdd  results in allowing attackers to exploit undefined behavior (dereferencing null pointers) as well as write outside of bounds of heap allocated data:",
        "Sample Code": "a_indices = tf.zeros([10, 97], dtype=tf.int64)\na_values = tf.zeros([10], dtype=tf.int64)\na_shape = tf.zeros([0], dtype=tf.int64)\n\nb_indices = tf.zeros([0, 0], dtype=tf.int64)\nb_values = tf.zeros([0], dtype=tf.int64)\nb_shape = tf.zeros([0], dtype=tf.int64)\n  \nthresh = 0\n\ntf.raw_ops.SparseAdd(a_indices=a_indices,\n                    a_values=a_values,\n                    a_shape=a_shape,\n                    b_indices=b_indices,\n                    b_values=b_values,\n                    b_shape=b_shape,\n                    ,\n                    thresh=thresh)",
        "Bug fix": [
            "@@ -14,6 +14,7 @@ limitations under the License.\n ==============================================================================*/\n \n #include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/op_requires.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor_util.h\"\n@@ -101,6 +102,10 @@ class SparseAddOp : public OpKernel {\n     std::vector<T> out_values;\n     const int num_dims = a_shape->dim_size(0);\n \n+    OP_REQUIRES(ctx, num_dims > 0,\n+                errors::InvalidArgument(\"Invalid input_a shape. Received: \",\n+                                        a_shape->DebugString()));\n+\n     // The input and output sparse tensors are assumed to be ordered along\n     // increasing dimension number.\n     int64 i = 0, j = 0;\n"
        ],
        "Title": "\n          Incomplete validation in `SparseAdd`\n        "
    },
    {
        "Bug description": "Due to lack of validation in  tf.raw_ops.RaggedTensorToTensor , an attacker can exploit an undefined behavior if input arguments are empty:",
        "Sample Code": "shape = tf.constant([-1, -1], shape=[2], dtype=tf.int64)\nvalues = tf.constant([], shape=[0], dtype=tf.int64)\ndefault_value = tf.constant(404, dtype=tf.int64)\nrow = tf.constant([269, 404, 0, 0, 0, 0, 0], shape=[7], dtype=tf.int64)\nrows = [row]\ntypes = ['ROW_SPLITS']\n\ntf.raw_ops.RaggedTensorToTensor(\n  shape=shape, values=values, default_value=default_value, \n  , \n  row_partition_tensors=rows, row_partition_types=types)",
        "Bug fix": [
            "@@ -208,7 +208,7 @@ class RaggedTensorToTensorBaseOp : public OpKernel {\n   }\n \n   void CalculateOutputIndexRowSplit(\n-      const RowPartitionTensor& row_split,\n+      OpKernelContext* context, const RowPartitionTensor& row_split,\n       const vector<INDEX_TYPE>& parent_output_index,\n       INDEX_TYPE output_index_multiplier, INDEX_TYPE output_size,\n       vector<INDEX_TYPE>* result) {\n@@ -233,7 +233,8 @@ class RaggedTensorToTensorBaseOp : public OpKernel {\n       }\n     }\n     if (row_split_size > 0) {\n-      DCHECK_EQ(result->size(), row_split(row_split_size - 1));\n+      OP_REQUIRES(context, result->size() == row_split(row_split_size - 1),\n+                  errors::InvalidArgument(\"Invalid row split size.\"));\n     }\n   }\n \n@@ -259,7 +260,7 @@ class RaggedTensorToTensorBaseOp : public OpKernel {\n   // result[7] = -1 because parent_output_index[value_rowids[6]] == -1\n   // result[8] = parent_output_index[value_rowids[7]]\n   void CalculateOutputIndexValueRowID(\n-      const RowPartitionTensor& value_rowids,\n+      OpKernelContext* context, const RowPartitionTensor& value_rowids,\n       const vector<INDEX_TYPE>& parent_output_index,\n       INDEX_TYPE output_index_multiplier, INDEX_TYPE output_size,\n       vector<INDEX_TYPE>* result) {\n@@ -293,7 +294,8 @@ class RaggedTensorToTensorBaseOp : public OpKernel {\n       }\n       result->push_back(current_output_index);\n     }\n-    DCHECK_EQ(result->size(), value_rowids.size());\n+    OP_REQUIRES(context, result->size() == value_rowids.size(),\n+                errors::InvalidArgument(\"Invalid row ids.\"));\n   }\n \n   Status CalculateOutputIndex(OpKernelContext* context, int dimension,\n@@ -307,13 +309,13 @@ class RaggedTensorToTensorBaseOp : public OpKernel {\n     switch (partition_type) {\n       case RowPartitionType::VALUE_ROWIDS:\n         CalculateOutputIndexValueRowID(\n-            row_partition_tensor, parent_output_index, output_index_multiplier,\n-            output_size, result);\n+            context, row_partition_tensor, parent_output_index,\n+            output_index_multiplier, output_size, result);\n         return tensorflow::Status::OK();\n       case RowPartitionType::ROW_SPLITS:\n-        CalculateOutputIndexRowSplit(row_partition_tensor, parent_output_index,\n-                                     output_index_multiplier, output_size,\n-                                     result);\n+        CalculateOutputIndexRowSplit(\n+            context, row_partition_tensor, parent_output_index,\n+            output_index_multiplier, output_size, result);\n         return tensorflow::Status::OK();\n       default:\n         return errors::InvalidArgument(\n"
        ],
        "Title": "\n          Heap OOB and null pointer dereference in `RaggedTensorToTensor`\n        "
    },
    {
        "Bug description": "A specially crafted TFLite model could trigger an OOB read on heap in the TFLite implementation of  Split_V :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -96,6 +96,8 @@ TfLiteStatus ResizeOutputTensors(TfLiteContext* context, TfLiteNode* node,\n     }\n   }\n \n+  TF_LITE_ENSURE(context, axis_value >= 0);\n+  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\n   const int input_size = SizeOfDimension(input, axis_value);\n \n   if (minus_one_index != -1) {\n"
        ],
        "Title": "\n          Heap OOB read in TFLite\n        "
    },
    {
        "Bug description": "A specially crafted TFLite model could trigger an OOB write on heap in the TFLite implementation of  ArgMin :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -48,6 +48,9 @@ TfLiteStatus ResizeOutput(TfLiteContext* context, const TfLiteTensor* input,\n     axis_value += NumDimensions(input);\n   }\n \n+  TF_LITE_ENSURE(context, axis_value >= 0);\n+  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\n+\n   // Copy the input dimensions to output except the axis dimension.\n   TfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\n   int j = 0;\n"
        ],
        "Title": "\n          Heap OOB write in TFLite\n        "
    },
    {
        "Bug description": "The TFLite code for allocating  TFLiteIntArray s is  vulnerable to an integer overflow issue :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -45,8 +45,10 @@ int TfLiteIntArrayEqualsArray(const TfLiteIntArray* a, int b_size,\n #ifndef TF_LITE_STATIC_MEMORY\n \n TfLiteIntArray* TfLiteIntArrayCreate(int size) {\n-  TfLiteIntArray* ret =\n-      (TfLiteIntArray*)malloc(TfLiteIntArrayGetSizeInBytes(size));\n+  int alloc_size = TfLiteIntArrayGetSizeInBytes(size);\n+  if (alloc_size <= 0) return NULL;\n+  TfLiteIntArray* ret = (TfLiteIntArray*)malloc(alloc_size);\n+  if (!ret) return ret;\n   ret->size = size;\n   return ret;\n }\n",
            "@@ -173,6 +173,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n \n   // Resize output tensor.\n   TfLiteIntArray* output_shape = TfLiteIntArrayCreate(output_rank);\n+  TF_LITE_ENSURE(context, output_shape != nullptr);\n   int k = 0;\n   int embedding_size = 1;\n   int lookup_size = 1;\n"
        ],
        "Title": "\n          Integer overflow in TFLite memory allocation\n        "
    },
    {
        "Bug description": "The TFLite implementation of concatenation is  vulnerable to an integer overflow issue :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -16,6 +16,8 @@ limitations under the License.\n \n #include <stdint.h>\n \n+#include <limits>\n+\n #include \"tensorflow/lite/c/builtin_op_data.h\"\n #include \"tensorflow/lite/c/common.h\"\n #include \"tensorflow/lite/kernels/internal/compatibility.h\"\n@@ -69,6 +71,10 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n     TF_LITE_ENSURE_EQ(context, t->type, input_type);\n     for (int d = 0; d < t0->dims->size; ++d) {\n       if (d == axis) {\n+        // Avoid integer overflow in sum_axis below\n+        TF_LITE_ENSURE(context, t->dims->data[axis] >= 0);\n+        TF_LITE_ENSURE(context, t->dims->data[axis] <=\n+                                    std::numeric_limits<int>::max() - sum_axis);\n         sum_axis += t->dims->data[axis];\n       } else {\n         TF_LITE_ENSURE_EQ(context, t->dims->data[d], t0->dims->data[d]);\n"
        ],
        "Title": "\n          Integer overflow in TFLite concatentation\n        "
    }
]
[
    {
        "Bug description": "The TFLite implementation of hashtable lookup is  vulnerable to a division by zero error :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -112,6 +112,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 2, &value));\n \n   const int num_rows = SizeOfDimension(value, 0);\n+  TF_LITE_ENSURE(context, num_rows != 0);\n   const int row_bytes = value->bytes / num_rows;\n   void* pointer = nullptr;\n   DynamicBuffer buf;\n"
        ],
        "Title": "\n          Division by zero in TFLite's implementation of hashtable lookup\n        "
    },
    {
        "Bug description": "The implementation of the  DepthwiseConv  TFLite operator is  vulnerable to a division by zero error :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -285,8 +285,8 @@ TfLiteStatus ComputeDepthMultiplier(TfLiteContext* context,\n                                     int16* depth_multiplier) {\n   int num_filter_channels = SizeOfDimension(filter, 3);\n   int num_input_channels = SizeOfDimension(input, 3);\n+  TF_LITE_ENSURE(context, num_input_channels != 0);\n   TF_LITE_ENSURE_EQ(context, num_filter_channels % num_input_channels, 0);\n-\n   *depth_multiplier = num_filter_channels / num_input_channels;\n   return kTfLiteOk;\n }\n@@ -455,8 +455,9 @@ TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,\n   float output_activation_min, output_activation_max;\n   CalculateActivationRange(params->activation, &output_activation_min,\n                            &output_activation_max);\n-  const int input_size = NumElements(input) / SizeOfDimension(input, 0);\n   const int batch_size = SizeOfDimension(input, 0);\n+  TF_LITE_ENSURE(context, batch_size != 0);\n+  const int input_size = NumElements(input) / batch_size;\n   TfLiteTensor* input_quantized;\n   TF_LITE_ENSURE_OK(context,\n                     GetTemporarySafe(context, node, data->input_quantized_index,\n"
        ],
        "Title": "\n          Division by zero in TFLite's implementation of `DepthwiseConv`\n        "
    },
    {
        "Bug description": "The implementation of the  OneHot  TFLite operator is  vulnerable to a division by zero error :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -69,6 +69,11 @@ void OneHotComputeImpl(const OneHotContext& op_context) {\n   for (int i = 0; i < op_context.axis; ++i) {\n     prefix_dim_size *= op_context.indices->dims->data[i];\n   }\n+  if (prefix_dim_size == 0) {\n+    // If indices tensor is degenerate, return a degenerate tensor, just like\n+    // TensorFlow does.\n+    return;\n+  }\n   const int suffix_dim_size = NumElements(op_context.indices) / prefix_dim_size;\n   const int depth = *op_context.depth->data.i32;\n \n"
        ],
        "Title": "\n          Division by zero in TFLite's implementation of `OneHot`\n        "
    },
    {
        "Bug description": "The implementation of the  Split  TFLite operator is  vulnerable to a division by zero error :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -60,6 +60,7 @@ TfLiteStatus ResizeOutputTensors(TfLiteContext* context, TfLiteNode* node,\n   TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\n \n   const int input_size = SizeOfDimension(input, axis_value);\n+  TF_LITE_ENSURE(context, num_splits != 0);\n   TF_LITE_ENSURE_MSG(context, input_size % num_splits == 0,\n                      \"Not an even split\");\n   const int slice_size = input_size / num_splits;\n"
        ],
        "Title": "\n          Division by zero in TFLite's implementation of `Split`\n        "
    },
    {
        "Bug description": "The implementation of the  SVDF  TFLite operator is  vulnerable to a division by zero error :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -99,6 +99,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   const int rank = params->rank;\n   const int batch_size = input->dims->data[0];\n   const int num_filters = weights_feature->dims->data[0];\n+  TF_LITE_ENSURE(context, rank != 0);\n   TF_LITE_ENSURE_EQ(context, num_filters % rank, 0);\n   const int num_units = num_filters / rank;\n   const int memory_size = weights_time->dims->data[1];\n"
        ],
        "Title": "\n          Division by zero in TFLite's implementation of `SVDF`\n        "
    },
    {
        "Bug description": "The implementation of the  SpaceToBatchNd  TFLite operator is  vulnerable to a division by zero error :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -79,6 +79,7 @@ TfLiteStatus ResizeOutputTensor(TfLiteContext* context,\n   for (int dim = 0; dim < spatial_dims_num; ++dim) {\n     int final_dim_size = (input_size->data[dim + 1] + paddings_data[dim * 2] +\n                           paddings_data[dim * 2 + 1]);\n+    TF_LITE_ENSURE(context, block_shape[dim] != 0);\n     TF_LITE_ENSURE_EQ(context, final_dim_size % block_shape[dim], 0);\n     output_size->data[dim + 1] = final_dim_size / block_shape[dim];\n     output_batch_size *= block_shape[dim];\n"
        ],
        "Title": "\n          Division by zero in TFLite's implementation of `SpaceToBatchNd`\n        "
    },
    {
        "Bug description": "The implementation of the  BatchToSpaceNd  TFLite operator is  vulnerable to a division by zero error :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -78,6 +78,7 @@ TfLiteStatus ResizeOutputTensor(TfLiteContext* context,\n   int output_batch_size = input_size->data[0];\n   for (int dim = 0; dim < spatial_dims_num; ++dim) {\n     // Number of batch must be multiple of (block_shape[dim]).\n+    TF_LITE_ENSURE(context, block_shape[dim] != 0);\n     TF_LITE_ENSURE_EQ(context, output_batch_size % block_shape[dim], 0);\n     output_batch_size = output_batch_size / block_shape[dim];\n     output_size->data[dim + 1] = input_size->data[dim + 1] * block_shape[dim] -\n"
        ],
        "Title": "\n          Division by zero in TFLite's implementation of `BatchToSpaceNd`\n        "
    },
    {
        "Bug description": "const",
        "Sample Code": "",
        "Bug fix": [
            "@@ -71,6 +71,10 @@ TfLiteStatus EvalSimple(TfLiteContext* context, TfLiteNode* node,\n                         const TfLiteTensor* lookup, const TfLiteTensor* value,\n                         TfLiteTensor* output) {\n   const int row_size = SizeOfDimension(value, 0);\n+  if (row_size == 0) {\n+    // Propagate empty tensor if input is empty\n+    return kTfLiteOk;\n+  }\n   const int row_bytes = value->bytes / row_size;\n \n   char* output_raw = GetTensorData<char>(output);\n"
        ],
        "Title": "\n          Division by zero in TFLite's implementation of `EmbeddingLookup`\n        "
    },
    {
        "Bug description": "TFLite's  convolution code  has multiple division where the divisor is controlled by the user and not checked to be non-zero. For example:",
        "Sample Code": "",
        "Bug fix": [
            "@@ -545,6 +545,7 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n     // Only one scale factor per batch is typically necessary. See optimized\n     // implementation for why we need to allocate for the height of the inputs\n     // flattened to 2D.\n+    TF_LITE_ENSURE(context, channels_in != 0);\n     const int height = NumElements(input) / channels_in;\n     int scaling_dims[1] = {height};\n     if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {\n@@ -587,6 +588,7 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n       input_offsets->type = kTfLiteInt32;\n       input_offsets->allocation_type = kTfLiteArenaRw;\n       // See above comment for the need to allocate for height of inputs.\n+      TF_LITE_ENSURE(context, channels_in != 0);\n       const int height = NumElements(input) / channels_in;\n       const int input_offset_dims[1] = {height};\n       if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1,\n@@ -886,8 +888,9 @@ TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,\n   CalculateActivationRange(params->activation, &output_activation_min,\n                            &output_activation_max);\n \n-  const int input_size = NumElements(input) / SizeOfDimension(input, 0);\n   const int batch_size = SizeOfDimension(input, 0);\n+  TF_LITE_ENSURE(context, batch_size != 0);\n+  const int input_size = NumElements(input) / batch_size;\n   TfLiteTensor* quantized_input_tensor;\n   TF_LITE_ENSURE_OK(context,\n                     GetTemporarySafe(context, node, data->input_quantized_index,\n@@ -989,8 +992,9 @@ TfLiteStatus EvalHybrid(TfLiteContext* context, TfLiteNode* node,\n   CalculateActivationRange(params->activation, &output_activation_min,\n                            &output_activation_max);\n \n-  const int input_size = NumElements(input) / SizeOfDimension(input, 0);\n   const int batch_size = SizeOfDimension(input, 0);\n+  TF_LITE_ENSURE(context, batch_size != 0);\n+  const int input_size = NumElements(input) / batch_size;\n \n   const float* input_ptr = GetTensorData<float>(input);\n   TfLiteTensor* quantized_input_tensor;\n"
        ],
        "Title": "\n          Division by zero in TFLite's convolution code\n        "
    },
    {
        "Bug description": "The implementation of the  DepthToSpace  TFLite operator is  vulnerable to a division by zero error :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -61,6 +61,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n \n   const int block_size = params->block_size;\n+  TF_LITE_ENSURE(context, block_size > 0);\n   const int input_height = input->dims->data[1];\n   const int input_width = input->dims->data[2];\n   const int input_channels = input->dims->data[3];\n",
            "@@ -60,6 +60,11 @@ TEST(DepthToSpaceOpModel, BadBlockSize) {\n   EXPECT_DEATH(DepthToSpaceOpModel({TensorType_FLOAT32, {1, 1, 1, 4}}, 4),\n                \"Cannot allocate tensors\");\n }\n+\n+TEST(DepthToSpaceOpModel, NoBlockSize) {\n+  EXPECT_DEATH(DepthToSpaceOpModel({TensorType_FLOAT32, {1, 1, 1, 4}}, 0),\n+               \"Cannot allocate tensors\");\n+}\n #endif\n \n TEST(DepthToSpaceOpModel, Float32) {\n",
            "@@ -54,6 +54,7 @@ TfLiteStatus CalculateOpData(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n \n   const int block_size = params->block_size;\n+  TF_LITE_ENSURE(context, block_size > 0);\n   const int input_height = input->dims->data[kHeightRank];\n   const int input_width = input->dims->data[kWidthRank];\n   const int input_channels = input->dims->data[kDepthRank];\n"
        ],
        "Title": "\n          Division by zero in TFLite's implementation of `DepthToSpace`\n        "
    }
]
[
    {
        "Bug description": "TFlite graphs must not have loops between nodes. However, this condition was not checked and an attacker could craft models that would result in infinite loop during evaluation. In certain cases, the infinite loop would be replaced by stack overflow due to too many recursive calls.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Stack overflow due to looping TFLite subgraph\n        "
    },
    {
        "Bug description": "The fix for  CVE-2020-15209  missed the case when the target shape of  Reshape  operator is given by the elements of a 1-D tensor. As such, the  fix for the vulnerability  allowed passing a null-buffer-backed tensor with a 1D shape:",
        "Sample Code": "",
        "Bug fix": [
            "@@ -1060,10 +1060,17 @@ TfLiteStatus Subgraph::Invoke() {\n         TF_LITE_ENSURE_STATUS(EnsureTensorDataIsReadable(tensor_index));\n       }\n       if (tensor->data.raw == nullptr && tensor->bytes > 0) {\n-        if (registration.builtin_code == kTfLiteBuiltinReshape && i == 1) {\n+        if (registration.builtin_code == kTfLiteBuiltinReshape && i == 1 &&\n+            tensor->dims->size != 1) {\n           // In general, having a tensor here with no buffer will be an error.\n-          // However, for the reshape operator, the second input tensor is only\n-          // used for the shape, not for the data. Thus, null buffer is ok.\n+          // However, for the reshape operator, the second input tensor is\n+          // sometimes only used for the shape, not for the data. Thus, null\n+          // buffer is ok in this situation.\n+          // The situation where null buffer is not ok for reshape operator is\n+          // only when there are 2 inputs given to the node and the one\n+          // corresponding to the shape (i == 1) is a vector that contains all\n+          // dimensions. See `GetOutputShape()` function in\n+          // `tensorflow/lite/kernels/reshape.cc`\n           continue;\n         } else {\n           // In all other cases, we need to return an error as otherwise we will\n"
        ],
        "Title": "\n          Null pointer dereference in TFLite's `Reshape` operator\n        "
    },
    {
        "Bug description": "The implementations of the  Minimum  and  Maximum  TFLite operators can be used to read data outside of bounds of heap allocated objects, if any of the two input tensor arguments are empty.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -157,35 +157,37 @@ template <KernelType kernel_type, typename OpType>\n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   OpContext op_context(context, node);\n \n-    switch (op_context.output->type) {\n-      case kTfLiteFloat32:\n-        TFLiteOperation<kernel_type, float, OpType>(context, node, op_context);\n-        break;\n-      case kTfLiteUInt8:\n-        TFLiteOperation<kernel_type, uint8_t, OpType>(context, node,\n-                                                      op_context);\n-        break;\n-      case kTfLiteInt8:\n-        TFLiteOperation<kernel_type, int8_t, OpType>(context, node, op_context);\n-        break;\n-      case kTfLiteInt32:\n-        TFLiteOperation<kernel_type, int32_t, OpType>(context, node,\n-                                                      op_context);\n-        break;\n-      case kTfLiteInt64:\n-        TFLiteOperation<kernel_type, int64_t, OpType>(context, node,\n-                                                      op_context);\n-        break;\n-      case kTfLiteInt16:\n-        TFLiteOperation<kernel_type, int16_t, OpType>(context, node,\n-                                                      op_context);\n-        break;\n-      default:\n-        context->ReportError(context,\n-                             \"Type %d is currently not supported by Maximum.\",\n-                             op_context.output->type);\n-        return kTfLiteError;\n-    }\n+  // If inputs have no element, shortcircuit.\n+  if (NumElements(op_context.input1) == 0 ||\n+      NumElements(op_context.input2) == 0) {\n+    return kTfLiteOk;\n+  }\n+\n+  switch (op_context.output->type) {\n+    case kTfLiteFloat32:\n+      TFLiteOperation<kernel_type, float, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteUInt8:\n+      TFLiteOperation<kernel_type, uint8_t, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteInt8:\n+      TFLiteOperation<kernel_type, int8_t, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteInt32:\n+      TFLiteOperation<kernel_type, int32_t, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteInt64:\n+      TFLiteOperation<kernel_type, int64_t, OpType>(context, node, op_context);\n+      break;\n+    case kTfLiteInt16:\n+      TFLiteOperation<kernel_type, int16_t, OpType>(context, node, op_context);\n+      break;\n+    default:\n+      context->ReportError(context,\n+                           \"Type %d is currently not supported by Maximum.\",\n+                           op_context.output->type);\n+      return kTfLiteError;\n+  }\n   return kTfLiteOk;\n }\n \n"
        ],
        "Title": "\n          Heap OOB read in TFLite's implementation of `Minimum` or `Maximum`\n        "
    },
    {
        "Bug description": "The optimized implementation of the  TransposeConv  TFLite operator is  vulnerable to a division by zero error :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -591,6 +591,10 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   const auto* params =\n       reinterpret_cast<TfLiteTransposeConvParams*>(node->builtin_data);\n \n+  // Prevent divisions by 0\n+  TF_LITE_ENSURE(context, params->stride_height > 0);\n+  TF_LITE_ENSURE(context, params->stride_width > 0);\n+\n   // Resize any deferred dynamic tensors\n   if (IsDynamicTensor(output)) {\n     TF_LITE_ENSURE_OK(context, ResizeTensor(context, output_shape, output));\n"
        ],
        "Title": "\n          Division by zero in TFLite's implementation of `TransposeConv`\n        "
    },
    {
        "Bug description": "The reference implementation of the  GatherNd  TFLite operator is  vulnerable to a division by zero error :",
        "Sample Code": "",
        "Bug fix": [
            "@@ -155,6 +155,9 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_OK(context,\n                     GetOutputSafe(context, node, kOutputTensor, &output));\n \n+  // Prevent division by 0 in the helper\n+  TF_LITE_ENSURE(context, NumElements(params) > 0);\n+\n   switch (indices->type) {\n     case kTfLiteInt32:\n       return EvalGatherNd<int32_t>(context, params, indices, output);\n"
        ],
        "Title": "\n          Division by zero in TFLite's implementation of `GatherNd`\n        "
    },
    {
        "Bug description": "The  Prepare  step of the  SpaceToDepth  TFLite operator  does not check for 0 before division .",
        "Sample Code": "",
        "Bug fix": [
            "@@ -61,6 +61,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n \n   const int block_size = params->block_size;\n+  TF_LITE_ENSURE(context, block_size > 0);\n   const int input_height = input->dims->data[1];\n   const int input_width = input->dims->data[2];\n   int output_height = input_height / block_size;\n"
        ],
        "Title": "\n          Division by zero in TFLite's implementation of `SpaceToDepth`\n        "
    },
    {
        "Bug description": "Optimized pooling implementations in TFLite fail to check that the stride arguments are not 0 before calling  ComputePaddingHeightWidth .",
        "Sample Code": "",
        "Bug fix": [
            "@@ -87,6 +87,10 @@ TfLiteStatus GenericPrepare(TfLiteContext* context, TfLiteNode* node) {\n   auto padding = params->padding;\n   int out_width, out_height;\n \n+  // Prevent division by 0 in optimized pooling implementations\n+  TF_LITE_ENSURE(context, params->stride_height > 0);\n+  TF_LITE_ENSURE(context, params->stride_width > 0);\n+\n   data->padding = ComputePaddingHeightWidth(\n       params->stride_height, params->stride_width, 1, 1, height, width,\n       params->filter_height, params->filter_width, padding, &out_height,\n",
            "@@ -1151,5 +1151,18 @@ TEST(FloatPoolingOpTest, L2PoolPaddingValidSlide1) {\n   EXPECT_THAT(m.GetOutput(), ElementsAreArray({3.5, 6.0, 6.5}));\n }\n \n+#ifdef GTEST_HAS_DEATH_TEST\n+TEST(FloatPoolingOpTest, MaxPoolWithZeroStride) {\n+  EXPECT_DEATH(\n+      FloatPoolingOpModel m(BuiltinOperator_MAX_POOL_2D,\n+                            /*input=*/{TensorType_FLOAT32, {1, 2, 4, 1}},\n+                            /*filter_width=*/2, /*filter_height=*/2,\n+                            /*output=*/{TensorType_FLOAT32, {}},\n+                            /*padding=*/Padding_VALID,\n+                            /*stride_w=*/0, /*stride_h=*/0),\n+      \"Cannot allocate tensors\");\n+}\n+#endif\n+\n }  // namespace\n }  // namespace tflite\n"
        ],
        "Title": "\n          Division by zero in optimized pooling implementations in TFLite\n        "
    },
    {
        "Bug description": "The TFLite computation for size of output after padding,  ComputeOutSize , does not check that the  stride  argument is not 0 before doing the division.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -44,6 +44,11 @@ inline int ComputePaddingWithOffset(int stride, int dilation_rate, int in_size,\n inline int ComputeOutSize(TfLitePadding padding, int image_size,\n                           int filter_size, int stride, int dilation_rate = 1) {\n   int effective_filter_size = (filter_size - 1) * dilation_rate + 1;\n+\n+  // TODO(b/186448822): This uses 0 since the function has no other way to\n+  // report error case\n+  if (stride == 0) return 0;\n+\n   switch (padding) {\n     case kTfLitePaddingSame:\n       return (image_size + stride - 1) / stride;\n"
        ],
        "Title": "\n          Division by zero in padding computation in TFLite\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.FusedBatchNorm  is vulnerable to a heap buffer overflow:",
        "Sample Code": "import numpy as np\n\nx = tf.zeros([10, 10, 10, 1], dtype=tf.float32)\nscale = tf.constant([], shape=[0], dtype=tf.float32)\noffset = tf.constant([], shape=[0], dtype=tf.float32)\nmean = tf.constant([], shape=[0], dtype=tf.float32)\nvariance = tf.constant([], shape=[0], dtype=tf.float32)\nepsilon = 0.0\nexponential_avg_factor = 0.0\ndata_format = \"NHWC\"\nis_training = False\n\ntf.raw_ops.FusedBatchNorm(\n  x=x, scale=scale, offset=offset, mean=mean, variance=variance, \n  epsilon=epsilon, exponential_avg_factor=exponential_avg_factor,\n  ,\n  data_format=data_format, is_training=is_training)",
        "Bug fix": [
            "@@ -1282,6 +1282,32 @@ class FusedBatchNormOpBase : public OpKernel {\n                   errors::InvalidArgument(\"Error during tensor copy.\"));\n     }\n \n+    const auto num_channels = GetTensorDim(x, tensor_format_, 'C');\n+    OP_REQUIRES(\n+        context, scale.NumElements() == num_channels,\n+        errors::InvalidArgument(\"scale must have the same number of elements \"\n+                                \"as the channels of x, got \",\n+                                scale.NumElements(), \" and \", num_channels));\n+    OP_REQUIRES(\n+        context, offset.NumElements() == num_channels,\n+        errors::InvalidArgument(\"offset must have the same number of elements \"\n+                                \"as the channels of x, got \",\n+                                offset.NumElements(), \" and \", num_channels));\n+    if (estimated_mean.NumElements() != 0) {\n+      OP_REQUIRES(context, estimated_mean.NumElements() == num_channels,\n+                  errors::InvalidArgument(\n+                      \"mean must be empty or have the same number of \"\n+                      \"elements as the channels of x, got \",\n+                      estimated_mean.NumElements(), \" and \", num_channels));\n+    }\n+    if (estimated_variance.NumElements() != 0) {\n+      OP_REQUIRES(context, estimated_variance.NumElements() == num_channels,\n+                  errors::InvalidArgument(\n+                      \"variance must be empty or have the same number of \"\n+                      \"elements as the channels of x, got \",\n+                      estimated_variance.NumElements(), \" and \", num_channels));\n+    }\n+\n     if (has_side_input_) {\n       OP_REQUIRES(context, side_input->shape() == x.shape(),\n                   errors::InvalidArgument(\n@@ -1294,7 +1320,7 @@ class FusedBatchNormOpBase : public OpKernel {\n       // NOTE(ezhulenev): This requirement is coming from implementation\n       // details of cudnnBatchNormalizationForwardTrainingEx.\n       OP_REQUIRES(\n-          context, !is_training_ || x.dim_size(3) % 4 == 0,\n+          context, !is_training_ || num_channels % 4 == 0,\n           errors::InvalidArgument(\"FusedBatchNorm with activation requires \"\n                                   \"channel dimension to be a multiple of 4.\"));\n     }\n"
        ],
        "Title": "\n          Heap buffer overflow and undefined behavior in `FusedBatchNorm`\n        "
    },
    {
        "Bug description": "An attacker can trigger a denial of service via a  CHECK -fail in  caused by an integer overflow in constructing a new tensor shape:",
        "Sample Code": "input_layer = 2**60-1\nsparse_data = tf.raw_ops.SparseSplit(\n    split_dim=1, \n    indices=[(0, 0), (0, 1), (0, 2), \n    (4, 3), (5, 0), (5, 1)],\n    values=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n    shape=(input_layer, input_layer),\n    num_split=2,\n    name=None\n    )\n    )",
        "Bug fix": [
            "@@ -63,11 +63,18 @@ class SparseSplitOp : public OpKernel {\n                                         input_shape.vec<int64>()(axis),\n                                         \"), got \", num_split_));\n \n+    // Prevent overflow by constructing the dense shape separately\n+    TensorShape dense_shape;\n+    const auto input_shape_flat = input_shape.flat<int64>();\n+    for (int i = 0; i < input_shape.NumElements(); i++) {\n+      OP_REQUIRES_OK(context,\n+                     dense_shape.AddDimWithStatus(input_shape_flat(i)));\n+    }\n+\n     sparse::SparseTensor sparse_tensor;\n     OP_REQUIRES_OK(context,\n-                   sparse::SparseTensor::Create(\n-                       input_indices, input_values,\n-                       TensorShape(input_shape.vec<int64>()), &sparse_tensor));\n+                   sparse::SparseTensor::Create(input_indices, input_values,\n+                                                dense_shape, &sparse_tensor));\n \n     std::vector<sparse::SparseTensor> outputs;\n     OP_REQUIRES_OK(context, sparse::SparseTensor::Split<T>(\n"
        ],
        "Title": "\n          `CHECK`-fail due to integer overflow\n        "
    }
]
[
    {
        "Bug description": "Due to lack of validation in  tf.raw_ops.Dequantize , an attacker can trigger a read from outside of bounds of heap allocated data:",
        "Sample Code": "input_tensor=tf.constant(\n  [75, 75, 75, 75, -6, -9, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10,\\\n  -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10,\\\n  -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10,\\\n  -10, -10, -10, -10], shape=[5, 10], dtype=tf.int32)\ninput_tensor=tf.cast(input_tensor, dtype=tf.quint8)\nmin_range = tf.constant([-10], shape=[1], dtype=tf.float32)\nmax_range = tf.constant([24, 758, 758, 758, 758], shape=[5], dtype=tf.float32)\n  \ntf.raw_ops.Dequantize( \n  input=input_tensor, min_range=min_range, max_range=max_range, mode='SCALED',\n  ,\n  narrow_range=True, axis=0, dtype=tf.dtypes.float32)",
        "Bug fix": [
            "@@ -98,6 +98,18 @@ class DequantizeOp : public OpKernel {\n     if (axis_ > -1) {\n       num_slices = input.dim_size(axis_);\n     }\n+    OP_REQUIRES(ctx, input_min_tensor.NumElements() == num_slices,\n+                errors::InvalidArgument(\n+                    \"input_min_tensor must have as many elements as input on \"\n+                    \"the dequantization axis (\",\n+                    axis_, \"), got \", input_min_tensor.NumElements(),\n+                    \", expected \", num_slices));\n+    OP_REQUIRES(ctx, input_max_tensor.NumElements() == num_slices,\n+                errors::InvalidArgument(\n+                    \"input_max_tensor must have as many elements as input on \"\n+                    \"the dequantization axis (\",\n+                    axis_, \"), got \", input_max_tensor.NumElements(),\n+                    \", expected \", num_slices));\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n"
        ],
        "Title": "\n          Heap OOB read in `tf.raw_ops.Dequantize`\n        "
    },
    {
        "Bug description": "Due to lack of validation in  tf.raw_ops.CTCBeamSearchDecoder , an attacker can trigger denial of service via segmentation faults:",
        "Sample Code": "inputs = tf.constant([], shape=[18, 8, 0], dtype=tf.float32)\nsequence_length = tf.constant([11, -43, -92, 11, -89, -83, -35, -100],\nshape=[8], dtype=tf.int32)\nbeam_width = 10\ntop_paths = 3\nmerge_repeated = True\n\ntf.raw_ops.CTCBeamSearchDecoder(\n  inputs=inputs, sequence_length=sequence_length, beam_width=beam_width,\n  ,\n  top_paths=top_paths, merge_repeated=merge_repeated)",
        "Bug fix": [
            "@@ -70,6 +70,9 @@ class CTCDecodeHelper {\n     if (inputs_shape.dims() != 3) {\n       return errors::InvalidArgument(\"inputs is not a 3-Tensor\");\n     }\n+    if (inputs_shape.num_elements() == 0) {\n+      return errors::InvalidArgument(\"inputs must not be empty\");\n+    }\n \n     const int64 max_time = inputs_shape.dim_size(0);\n     const int64 batch_size = inputs_shape.dim_size(1);\n"
        ],
        "Title": "\n          Segfault in `CTCBeamSearchDecoder`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.MaxPoolGrad  is vulnerable to a heap buffer overflow:",
        "Sample Code": "orig_input = tf.constant([0.0], shape=[1, 1, 1, 1], dtype=tf.float32)\norig_output = tf.constant([0.0], shape=[1, 1, 1, 1], dtype=tf.float32)\ngrad = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)\nksize = [1, 1, 1, 1] \nstrides = [1, 1, 1, 1]\npadding = \"SAME\"\n\ntf.raw_ops.MaxPoolGrad(\n  orig_input=orig_input, orig_output=orig_output, grad=grad, ksize=ksize,\n  ,\n  strides=strides, padding=padding, explicit_paddings=[])",
        "Bug fix": [
            "@@ -199,7 +199,9 @@ static void SpatialMaxPoolWithArgMaxHelper(\n         // CHECK(input_backprop_index >= in_start && input_backprop_index <\n         // in_end)\n         FastBoundsCheck(input_backprop_index - in_start, in_end - in_start);\n-        input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n+        if (index < out_backprop.NumElements()) {\n+          input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n+        }\n       }\n     }\n   };\n"
        ],
        "Title": "\n          Heap buffer overflow in `MaxPoolGrad`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.FractionalAvgPoolGrad  is vulnerable to a heap buffer overflow:",
        "Sample Code": "orig_input_tensor_shape = tf.constant([1, 3, 2, 3], shape=[4], dtype=tf.int64)\nout_backprop = tf.constant([2], shape=[1, 1, 1, 1], dtype=tf.int64)\nrow_pooling_sequence = tf.constant([1], shape=[1], dtype=tf.int64)\ncol_pooling_sequence = tf.constant([1], shape=[1], dtype=tf.int64)\n\n\ntf.raw_ops.FractionalAvgPoolGrad(\n  orig_input_tensor_shape=orig_input_tensor_shape, out_backprop=out_backprop,\n  row_pooling_sequence=row_pooling_sequence,\n  ,\n  col_pooling_sequence=col_pooling_sequence, overlapping=False)",
        "Bug fix": [
            "@@ -250,6 +250,19 @@ class FractionalAvgPoolGradOp : public OpKernel {\n     const int64 out_cols = out_backprop.dim_size(2);\n     const int64 out_depth = out_backprop.dim_size(3);\n \n+    OP_REQUIRES(context, row_seq_tensor.NumElements() > out_rows,\n+                errors::InvalidArgument(\"Given out_backprop shape \",\n+                                        out_backprop.shape().DebugString(),\n+                                        \", row_seq_tensor must have at least \",\n+                                        out_rows + 1, \" elements, but got \",\n+                                        row_seq_tensor.NumElements()));\n+    OP_REQUIRES(context, col_seq_tensor.NumElements() > out_cols,\n+                errors::InvalidArgument(\"Given out_backprop shape \",\n+                                        out_backprop.shape().DebugString(),\n+                                        \", col_seq_tensor must have at least \",\n+                                        out_cols + 1, \" elements, but got \",\n+                                        col_seq_tensor.NumElements()));\n+\n     auto row_seq_tensor_flat = row_seq_tensor.flat<int64>();\n     auto col_seq_tensor_flat = col_seq_tensor.flat<int64>();\n     auto orig_input_tensor_shape_flat = orig_input_tensor_shape.flat<int64>();\n"
        ],
        "Title": "\n          Heap buffer overflow in `FractionalAvgPoolGrad`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.FractionalMaxPoolGrad  triggers an undefined behavior if one of the input tensors is empty:",
        "Sample Code": "orig_input = tf.constant([1], shape=[1], dtype=tf.int64)\norig_output = tf.constant([1], shape=[1], dtype=tf.int64)\nout_backprop = tf.constant([1, 1], shape=[2, 1, 1, 1], dtype=tf.int64)\nrow_pooling_sequence = tf.constant([1], shape=[1], dtype=tf.int64) \ncol_pooling_sequence = tf.constant([1], shape=[1], dtype=tf.int64)\n\ntf.raw_ops.FractionalMaxPoolGrad(\n  orig_input=orig_input, orig_output=orig_output, out_backprop=out_backprop,\n  row_pooling_sequence=row_pooling_sequence,\n  ,\n  col_pooling_sequence=col_pooling_sequence, overlapping=False)",
        "Bug fix": [
            "@@ -235,6 +235,20 @@ class FractionalMaxPoolGradOp : public OpKernel {\n \n     // Just to make it similar to FractionalMaxPoolOp.\n     constexpr int tensor_in_and_out_dims = 4;\n+    OP_REQUIRES(\n+        context, tensor_in.dims() == tensor_in_and_out_dims,\n+        errors::InvalidArgument(\"orig_input should be a tensor of rank 4, got \",\n+                                tensor_in.DebugString()));\n+    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n+                errors::InvalidArgument(\"orig_input must not be empty, got \",\n+                                        tensor_in.DebugString()));\n+    OP_REQUIRES(context, tensor_out.dims() == tensor_in_and_out_dims,\n+                errors::InvalidArgument(\n+                    \"orig_output should be a tensor of rank 4, got \",\n+                    tensor_out.DebugString()));\n+    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n+                errors::InvalidArgument(\"orig_output must not be empty, got \",\n+                                        tensor_out.DebugString()));\n     std::vector<int64> input_size(tensor_in_and_out_dims);\n     std::vector<int64> output_size(tensor_in_and_out_dims);\n     for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n"
        ],
        "Title": "\n          Undefined behavior and `CHECK`-fail in `FractionalMaxPoolGrad`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.AvgPool3DGrad  is vulnerable to a heap buffer overflow:",
        "Sample Code": "orig_input_shape = tf.constant([10, 6, 3, 7, 7], shape=[5], dtype=tf.int32)\ngrad = tf.constant([0.01, 0, 0], shape=[3, 1, 1, 1, 1], dtype=tf.float32)\nksize = [1, 1, 1, 1, 1]\nstrides = [1, 1, 1, 1, 1]\npadding = \"SAME\"\n\ntf.raw_ops.AvgPool3DGrad(\n  orig_input_shape=orig_input_shape, grad=grad, ksize=ksize, strides=strides,\n  ,\n  padding=padding)",
        "Bug fix": [
            "@@ -383,6 +383,19 @@ struct LaunchAvgPooling3dGradOp<CPUDevice, T> {\n                      const std::array<int64, 3>& output_shape,\n                      const std::array<int64, 3>& padding,\n                      TensorFormat data_format, Tensor* output) {\n+    OP_REQUIRES(\n+        context, tensor_in_shape.dim_size(0) == out_backprop.dim_size(0),\n+        errors::InvalidArgument(\n+            \"Expected first dimension of tensor_in_shape and \"\n+            \"out_backprop to match, got \",\n+            tensor_in_shape.dim_size(0), \" and \", out_backprop.dim_size(0)));\n+    OP_REQUIRES(\n+        context, tensor_in_shape.dim_size(4) == out_backprop.dim_size(4),\n+        errors::InvalidArgument(\n+            \"Expected last dimension of tensor_in_shape and \"\n+            \"out_backprop to match, got \",\n+            tensor_in_shape.dim_size(4), \" and \", out_backprop.dim_size(4)));\n+\n     output->flat<T>().setZero();\n     std::array<int64, 3> input_size = {{tensor_in_shape.dim_size(3),\n                                         tensor_in_shape.dim_size(2),\n"
        ],
        "Title": "\n          Heap buffer overflow in `AvgPool3DGrad`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.MaxPool3DGradGrad  is vulnerable to a heap buffer overflow:",
        "Sample Code": "values = [0.01] * 11\norig_input = tf.constant(values, shape=[11, 1, 1, 1, 1], dtype=tf.float32)\norig_output = tf.constant([0.01], shape=[1, 1, 1, 1, 1], dtype=tf.float32)\ngrad = tf.constant([0.01], shape=[1, 1, 1, 1, 1], dtype=tf.float32)\nksize = [1, 1, 1, 1, 1]\nstrides = [1, 1, 1, 1, 1]\npadding = \"SAME\"\n\ntf.raw_ops.MaxPool3DGradGrad(\n    orig_input=orig_input, orig_output=orig_output, grad=grad, ksize=ksize,\n    ,\n    strides=strides, padding=padding)",
        "Bug fix": [
            "@@ -693,6 +693,7 @@ class MaxPooling3dGradGradOp : public OpKernel {\n \n     Pool3dParameters params{context,  ksize_,       stride_,\n                             padding_, data_format_, tensor_in.shape()};\n+    if (!context->status().ok()) return;  // params is invalid\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n@@ -710,6 +711,17 @@ class MaxPooling3dGradGradOp : public OpKernel {\n         context, out_grad_backprop.NumElements() > 0,\n         errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\n                                 out_grad_backprop.DebugString()));\n+    OP_REQUIRES(context,\n+                tensor_in.NumElements() == out_grad_backprop.NumElements(),\n+                errors::InvalidArgument(\"tensor_in and out_grad_backprop must \"\n+                                        \"have same number of elements, got <\",\n+                                        tensor_in.DebugString(), \"> and <\",\n+                                        out_grad_backprop.DebugString(), \">\"));\n+    OP_REQUIRES(\n+        context, tensor_out.NumElements() == output->NumElements(),\n+        errors::InvalidArgument(\n+            \"tensor_out and output must have same number of elements, got <\",\n+            tensor_out.DebugString(), \"> and <\", output->DebugString(), \">\"));\n \n     LaunchMaxPooling3dGradGradOp<Device, T>::launch(\n         context, params, tensor_in, tensor_out, out_grad_backprop, output);\n"
        ],
        "Title": "\n          Heap buffer overflow in `MaxPool3DGradGrad`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.MaxPool3DGradGrad  exhibits undefined behavior by dereferencing null pointers backing attacker-supplied empty tensors:",
        "Sample Code": "orig_input = tf.constant([0.0], shape=[1, 1, 1, 1, 1], dtype=tf.float32)\norig_output = tf.constant([0.0], shape=[1, 1, 1, 1, 1], dtype=tf.float32)\ngrad = tf.constant([], shape=[0, 0, 0, 0, 0], dtype=tf.float32)\nksize = [1, 1, 1, 1, 1]\nstrides = [1, 1, 1, 1, 1]\npadding = \"SAME\"\n\ntf.raw_ops.MaxPool3DGradGrad(\n    orig_input=orig_input, orig_output=orig_output, grad=grad, ksize=ksize,\n    ,\n    strides=strides, padding=padding)",
        "Bug fix": [
            "@@ -698,6 +698,19 @@ class MaxPooling3dGradGradOp : public OpKernel {\n     OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                 {2}, 0, tensor_out.shape(), &output));\n \n+    // Given access patterns in LaunchMaxPooling3dGradGradOp, these tensors must\n+    // have elements.\n+    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n+                errors::InvalidArgument(\"received empty tensor tensor_in: \",\n+                                        tensor_in.DebugString()));\n+    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n+                errors::InvalidArgument(\"received empty tensor tensor_out: \",\n+                                        tensor_out.DebugString()));\n+    OP_REQUIRES(\n+        context, out_grad_backprop.NumElements() > 0,\n+        errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\n+                                out_grad_backprop.DebugString()));\n+\n     LaunchMaxPooling3dGradGradOp<Device, T>::launch(\n         context, params, tensor_in, tensor_out, out_grad_backprop, output);\n   }\n"
        ],
        "Title": "\n          Undefined behavior in `MaxPool3DGradGrad`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.MaxPoolGradWithArgmax  is vulnerable to a division by 0:",
        "Sample Code": "input = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)\ngrad = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)\nargmax = tf.constant([], shape=[0], dtype=tf.int64)\nksize = [1, 1, 1, 1]\nstrides = [1, 1, 1, 1]\n\ntf.raw_ops.MaxPoolGradWithArgmax(\n  input=input, grad=grad, argmax=argmax, ksize=ksize, strides=strides,\n  ,\n  padding='SAME', include_batch_in_index=False)",
        "Bug fix": [
            "@@ -1088,6 +1088,8 @@ class MaxPoolingGradWithArgmaxOp : public OpKernel {\n     OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                 {0}, 0, out_shape, &grad_out));\n \n+    if (out_shape.num_elements() == 0) return;  // nothing to be done\n+\n     LaunchMaxPoolingGradWithArgmax<Device, T>::launch(\n         context, params, grad_in, argmax, grad_out, include_batch_in_index_);\n   }\n"
        ],
        "Title": "\n          Division by 0 in `MaxPoolGradWithArgmax`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.ReverseSequence  allows for stack overflow and/or  CHECK -fail based denial of service.",
        "Sample Code": "input = tf.zeros([1, 1, 1], dtype=tf.int32)\nseq_lengths = tf.constant([0], shape=[1], dtype=tf.int32)\n\ntf.raw_ops.ReverseSequence(\n    (\n    input=input, seq_lengths=seq_lengths, seq_dim=-2, batch_dim=0)",
        "Bug fix": [
            "@@ -115,6 +115,10 @@ class ReverseSequenceOp : public OpKernel {\n       : OpKernel(context) {\n     OP_REQUIRES_OK(context, context->GetAttr(\"batch_dim\", &batch_dim_));\n     OP_REQUIRES_OK(context, context->GetAttr(\"seq_dim\", &seq_dim_));\n+    OP_REQUIRES(context, batch_dim_ >= 0,\n+                errors::InvalidArgument(\"Invalid batch_dim \", batch_dim_));\n+    OP_REQUIRES(context, seq_dim_ >= 0,\n+                errors::InvalidArgument(\"Invalid seq_dim \", seq_dim_));\n   }\n \n   void Compute(OpKernelContext* context) override {\n"
        ],
        "Title": "\n          Overflow/denial of service in `tf.raw_ops.ReverseSequence`\n        "
    }
]
[
    {
        "Bug description": "The implementation of  tf.raw_ops.SdcaOptimizer  triggers undefined behavior due to dereferencing a null pointer:",
        "Sample Code": "sparse_example_indices = [tf.constant((0), dtype=tf.int64), tf.constant((0), dtype=tf.int64)]\nsparse_feature_indices = [tf.constant([], shape=[0, 0, 0, 0], dtype=tf.int64), tf.constant((0), dtype=tf.int64)]\nsparse_feature_values = []\n\ndense_features = []\ndense_weights = []\n\nexample_weights = tf.constant((0.0), dtype=tf.float32)\nexample_labels = tf.constant((0.0), dtype=tf.float32)\n\nsparse_indices = [tf.constant((0), dtype=tf.int64), tf.constant((0), dtype=tf.int64)]\nsparse_weights = [tf.constant((0.0), dtype=tf.float32), tf.constant((0.0), dtype=tf.float32)]\n  \nexample_state_data = tf.constant([0.0, 0.0, 0.0, 0.0], shape=[1, 4], dtype=tf.float32)\n  \ntf.raw_ops.SdcaOptimizer(\n  sparse_example_indices=sparse_example_indices,\n  sparse_feature_indices=sparse_feature_indices,\n  sparse_feature_values=sparse_feature_values, dense_features=dense_features,\n  example_weights=example_weights, example_labels=example_labels, \n  sparse_indices=sparse_indices, sparse_weights=sparse_weights, \n  dense_weights=dense_weights, example_state_data=example_state_data,\n  loss_type=\"logistic_loss\", l1=0.0, l2=0.0, num_loss_partitions=1,\n  ,\n  num_inner_iterations=1, adaptative=False)",
        "Bug fix": [
            "@@ -99,6 +99,10 @@ Status ModelWeights::Initialize(OpKernelContext* const context) {\n   OpInputList sparse_weights_inputs;\n   TF_RETURN_IF_ERROR(\n       context->input_list(\"sparse_weights\", &sparse_weights_inputs));\n+  if (sparse_indices_inputs.size() != sparse_weights_inputs.size())\n+    return errors::InvalidArgument(\n+        \"sparse_indices and sparse_weights must have the same length, got \",\n+        sparse_indices_inputs.size(), \" and \", sparse_weights_inputs.size());\n   OpInputList dense_weights_inputs;\n   TF_RETURN_IF_ERROR(\n       context->input_list(\"dense_weights\", &dense_weights_inputs));\n@@ -106,10 +110,20 @@ Status ModelWeights::Initialize(OpKernelContext* const context) {\n   OpOutputList sparse_weights_outputs;\n   TF_RETURN_IF_ERROR(context->output_list(\"out_delta_sparse_weights\",\n                                           &sparse_weights_outputs));\n+  if (sparse_weights_outputs.size() != sparse_weights_inputs.size())\n+    return errors::InvalidArgument(\n+        \"out_delta_sparse_weights and sparse_weights must have the same \"\n+        \"length, got \",\n+        sparse_weights_outputs.size(), \" and \", sparse_weights_inputs.size());\n \n   OpOutputList dense_weights_outputs;\n   TF_RETURN_IF_ERROR(\n       context->output_list(\"out_delta_dense_weights\", &dense_weights_outputs));\n+  if (dense_weights_outputs.size() != dense_weights_inputs.size())\n+    return errors::InvalidArgument(\n+        \"out_delta_dense_weights and dense_weights must have the same length, \"\n+        \"got \",\n+        dense_weights_outputs.size(), \" and \", dense_weights_inputs.size());\n \n   for (int i = 0; i < sparse_weights_inputs.size(); ++i) {\n     Tensor* delta_t;\n@@ -327,13 +341,28 @@ Status Examples::Initialize(OpKernelContext* const context,\n   OpInputList sparse_example_indices_inputs;\n   TF_RETURN_IF_ERROR(context->input_list(\"sparse_example_indices\",\n                                          &sparse_example_indices_inputs));\n+  if (sparse_example_indices_inputs.size() != num_sparse_features)\n+    return errors::InvalidArgument(\n+        \"Expected \", num_sparse_features,\n+        \" tensors in sparse_example_indices but got \",\n+        sparse_example_indices_inputs.size());\n   OpInputList sparse_feature_indices_inputs;\n   TF_RETURN_IF_ERROR(context->input_list(\"sparse_feature_indices\",\n                                          &sparse_feature_indices_inputs));\n+  if (sparse_feature_indices_inputs.size() != num_sparse_features)\n+    return errors::InvalidArgument(\n+        \"Expected \", num_sparse_features,\n+        \" tensors in sparse_feature_indices but got \",\n+        sparse_feature_indices_inputs.size());\n   OpInputList sparse_feature_values_inputs;\n   if (num_sparse_features_with_values > 0) {\n     TF_RETURN_IF_ERROR(context->input_list(\"sparse_feature_values\",\n                                            &sparse_feature_values_inputs));\n+    if (sparse_feature_values_inputs.size() != num_sparse_features_with_values)\n+      return errors::InvalidArgument(\n+          \"Expected \", num_sparse_features_with_values,\n+          \" tensors in sparse_feature_values but got \",\n+          sparse_feature_values_inputs.size());\n   }\n \n   const Tensor* example_weights_t;\n@@ -400,6 +429,13 @@ Status Examples::CreateSparseFeatureRepresentation(\n           sparse_example_indices_inputs[i].template flat<int64>();\n       auto feature_indices =\n           sparse_feature_indices_inputs[i].template flat<int64>();\n+      if (example_indices.size() != feature_indices.size()) {\n+        mutex_lock l(mu);\n+        result = errors::InvalidArgument(\n+            \"Found mismatched example_indices and feature_indices [\",\n+            example_indices, \"] vs [\", feature_indices, \"]\");\n+        return;\n+      }\n \n       // Parse features for each example. Features for a particular example\n       // are at the offsets (start_id, end_id]\n"
        ],
        "Title": "\n          Reference binding to nullptr in `SdcaOptimizer`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.MaxPoolGradWithArgmax  can cause reads outside of bounds of heap allocated data if attacker supplies specially crafted inputs:",
        "Sample Code": "images = tf.fill([10, 96, 0, 1], 0.)\nboxes = tf.fill([10, 53, 0], 0.)\ncolors = tf.fill([0, 1], 0.)\n\n)\n\ntf.raw_ops.DrawBoundingBoxesV2(images=images, boxes=boxes, colors=colors)",
        "Bug fix": [
            "@@ -73,6 +73,12 @@ class DrawBoundingBoxesOp : public OpKernel {\n         errors::InvalidArgument(\"Channel depth should be either 1 (GRY), \"\n                                 \"3 (RGB), or 4 (RGBA)\"));\n \n+    OP_REQUIRES(\n+        context, boxes.dim_size(2) == 4,\n+        errors::InvalidArgument(\n+            \"The size of the third dimension of the box must be 4. Received: \",\n+            boxes.dim_size(2)));\n+\n     const int64 batch_size = images.dim_size(0);\n     const int64 height = images.dim_size(1);\n     const int64 width = images.dim_size(2);\n"
        ],
        "Title": "\n          Memory corruption in `DrawBoundingBoxesV2`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.MaxPoolGradWithArgmax  can cause reads outside of bounds of heap allocated data if attacker supplies specially crafted inputs:",
        "Sample Code": "input = tf.constant([1], shape=[1], dtype=tf.qint32) \ninput_max = tf.constant([], dtype=tf.float32)\ninput_min = tf.constant([], dtype=tf.float32)\n\n)\n\ntf.raw_ops.RequantizationRange(input=input, input_min=input_min, input_max=input_max)",
        "Bug fix": [
            "@@ -46,6 +46,10 @@ class RequantizationRangeOp : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input = ctx->input(0);\n+    OP_REQUIRES(ctx, ctx->input(1).NumElements() > 0,\n+                errors::InvalidArgument(\"Input min must not be empty.\"));\n+    OP_REQUIRES(ctx, ctx->input(2).NumElements() > 0,\n+                errors::InvalidArgument(\"Input max must not be empty.\"));\n     const float input_min_float = ctx->input(1).flat<float>()(0);\n     const float input_max_float = ctx->input(2).flat<float>()(0);\n     Tensor* output_min = nullptr;\n"
        ],
        "Title": "\n          Heap out of bounds read in `RequantizationRange`\n        "
    },
    {
        "Bug description": "The implementation of  tf.raw_ops.MaxPoolGradWithArgmax  can cause reads outside of bounds of heap allocated data if attacker supplies specially crafted inputs:",
        "Sample Code": "input = tf.constant([10.0, 10.0, 10.0], shape=[1, 1, 3, 1], dtype=tf.float32)\ngrad = tf.constant([10.0, 10.0, 10.0, 10.0], shape=[1, 1, 1, 4], dtype=tf.float32)\nargmax = tf.constant([1], shape=[1], dtype=tf.int64)\nksize = [1, 1, 1, 1]\nstrides = [1, 1, 1, 1]\n  \ntf.raw_ops.MaxPoolGradWithArgmax(\n  input=input, grad=grad, argmax=argmax, ksize=ksize, strides=strides,\n  ,\n  padding='SAME', include_batch_in_index=False)",
        "Bug fix": [
            "@@ -1014,6 +1014,9 @@ struct LaunchMaxPoolingGradWithArgmax<CPUDevice, T> {\n         const int input_start = start * input_size_per_batch;\n         const int input_end = limit * input_size_per_batch;\n         for (int64 index = input_start; index < input_end; index++) {\n+          if (index >= argmax.NumElements()) {\n+            break;\n+          }\n           int64 grad_out_index = argmax_flat(index);\n           if (!include_batch_in_index) {\n             const int64 cur_batch = index / input_size_per_batch;\n"
        ],
        "Title": "\n          Heap out of bounds read in `MaxPoolGradWithArgmax`\n        "
    },
    {
        "Bug description": "Due to lack of validation in  tf.raw_ops.SparseDenseCwiseMul , an attacker can trigger denial of service via  CHECK -fails or accesses to outside the bounds of heap allocated data:",
        "Sample Code": "indices = tf.constant([], shape=[10, 0], dtype=tf.int64)\nvalues = tf.constant([], shape=[0], dtype=tf.int64)\nshape = tf.constant([0, 0], shape=[2], dtype=tf.int64)\ndense = tf.constant([], shape=[0], dtype=tf.int64)\n  \ntf.raw_ops.SparseDenseCwiseMul(\n    (\n    sp_indices=indices, sp_values=values, sp_shape=shape, dense=dense)",
        "Bug fix": [
            "@@ -78,6 +78,11 @@ class SparseDenseBinaryOpShared : public OpKernel {\n                     \"but received shapes: \",\n                     values_t->shape().DebugString(), \" and \",\n                     shape_t->shape().DebugString()));\n+    OP_REQUIRES(\n+        ctx, values_t->dim_size(0) == indices_t->dim_size(0),\n+        errors::InvalidArgument(\n+            \"The first dimension of values and indices should match. (\",\n+            values_t->dim_size(0), \" vs. \", indices_t->dim_size(0), \")\"));\n \n     const auto indices_mat = indices_t->matrix<int64>();\n     const auto shape_vec = shape_t->vec<int64>();\n"
        ],
        "Title": "\n          Lack of validation in `SparseDenseCwiseMul`\n        "
    },
    {
        "Bug description": "An attacker can trigger undefined behavior by binding to null pointer in  tf.raw_ops.ParameterizedTruncatedNormal :",
        "Sample Code": "shape = tf.constant([], shape=[0], dtype=tf.int32)\nmeans = tf.constant((1), dtype=tf.float32)\nstdevs = tf.constant((1), dtype=tf.float32)\nminvals = tf.constant((1), dtype=tf.float32)\nmaxvals = tf.constant((1), dtype=tf.float32)\n  \ntf.raw_ops.ParameterizedTruncatedNormal(\n  (\n  shape=shape, means=means, stdevs=stdevs, minvals=minvals, maxvals=maxvals)",
        "Bug fix": [
            "@@ -627,6 +627,9 @@ class ParameterizedTruncatedNormalOp : public OpKernel {\n         ctx, TensorShapeUtils::IsVector(shape_tensor.shape()),\n         errors::InvalidArgument(\"Input shape should be a vector, got shape: \",\n                                 shape_tensor.shape().DebugString()));\n+    OP_REQUIRES(ctx, shape_tensor.NumElements() > 0,\n+                errors::InvalidArgument(\"Shape tensor must not be empty, got \",\n+                                        shape_tensor.DebugString()));\n     int32 num_batches = shape_tensor.flat<int32>()(0);\n \n     int32 samples_per_batch = 1;\n"
        ],
        "Title": "\n          Reference binding to null in `ParameterizedTruncatedNormal`\n        "
    },
    {
        "Bug description": "An attacker can write outside the bounds of heap allocated arrays by passing invalid arguments to  tf.raw_ops.Dilation2DBackpropInput :",
        "Sample Code": "input_tensor = tf.constant([1.1] * 81, shape=[3, 3, 3, 3], dtype=tf.float32)\nfilter = tf.constant([], shape=[0, 0, 3], dtype=tf.float32)\nout_backprop = tf.constant([1.1] * 1062, shape=[3, 2, 59, 3], dtype=tf.float32)\n\ntf.raw_ops.Dilation2DBackpropInput(\n  input=input_tensor, filter=filter, out_backprop=out_backprop, \n  , \n  strides=[1, 40, 1, 1], rates=[1, 56, 56, 1], padding='VALID')",
        "Bug fix": [
            "@@ -130,6 +130,7 @@ class DilationOp : public OpKernel {\n     ParseSizes(context, strides_, rates_, padding_, &stride_rows, &stride_cols,\n                &rate_rows, &rate_cols, &pad_top, &pad_left, &out_rows,\n                &out_cols);\n+    if (!context->status().ok()) return;\n \n     // Output tensor is of the following dimensions:\n     // [ batch, out_rows, out_cols, depth ]\n@@ -229,6 +230,7 @@ class DilationBackpropInputOp : public OpKernel {\n     ParseSizes(context, strides_, rates_, padding_, &stride_rows, &stride_cols,\n                &rate_rows, &rate_cols, &pad_top, &pad_left, &out_rows,\n                &out_cols);\n+    if (!context->status().ok()) return;\n \n     // Verify that the incoming gradient tensor has the expected size\n     // [ batch, out_rows, out_cols, depth ]\n@@ -318,8 +320,10 @@ struct DilationBackpropInput<CPUDevice, T> {\n                 }\n               }\n             }\n-            in_backprop(b, h_in_max, w_in_max, d) +=\n-                out_backprop(b, h_out, w_out, d);\n+            if (h_in_max < input_rows && w_in_max < input_cols) {\n+              in_backprop(b, h_in_max, w_in_max, d) +=\n+                  out_backprop(b, h_out, w_out, d);\n+            }\n           }\n         }\n       }\n@@ -349,6 +353,7 @@ class DilationBackpropFilterOp : public OpKernel {\n     ParseSizes(context, strides_, rates_, padding_, &stride_rows, &stride_cols,\n                &rate_rows, &rate_cols, &pad_top, &pad_left, &out_rows,\n                &out_cols);\n+    if (!context->status().ok()) return;\n \n     // Verify that the incoming gradient tensor has the expected size\n     // [ batch, out_rows, out_cols, depth ]\n@@ -438,8 +443,10 @@ struct DilationBackpropFilter<CPUDevice, T> {\n                 }\n               }\n             }\n-            filter_backprop(h_max, w_max, d) +=\n-                out_backprop(b, h_out, w_out, d);\n+            if (h_max < filter_rows && w_max < filter_cols) {\n+              filter_backprop(h_max, w_max, d) +=\n+                  out_backprop(b, h_out, w_out, d);\n+            }\n           }\n         }\n       }\n"
        ],
        "Title": "\n          Heap OOB access in `Dilation2DBackpropInput`\n        "
    },
    {
        "Bug description": "An attacker can trigger a null pointer dereference in the implementation of  tf.raw_ops.SparseFillEmptyRows :",
        "Sample Code": "indices = tf.constant([], shape=[0, 0], dtype=tf.int64)\nvalues = tf.constant([], shape=[0], dtype=tf.int64)\ndense_shape = tf.constant([], shape=[0], dtype=tf.int64)\ndefault_value = 0\n    \ntf.raw_ops.SparseFillEmptyRows(\n    indices=indices, values=values, dense_shape=dense_shape,\n    ,\n    default_value=default_value)",
        "Bug fix": [
            "@@ -228,7 +228,10 @@ void SparseFillEmptyRowsOpImpl(OpKernelContext* context,\n                               default_value_t.shape().DebugString()),\n       done);\n   // TODO(ebrevdo): add shape checks between values, indices,\n-  // dense_shape.  Also add check that dense rank > 0.\n+  // Also add check that dense rank > 0.\n+  OP_REQUIRES_ASYNC(context, dense_shape_t.NumElements() != 0,\n+                    errors::InvalidArgument(\"Dense shape cannot be empty.\"),\n+                    done);\n \n   using FunctorType = functor::SparseFillEmptyRows<Device, T, Tindex>;\n   OP_REQUIRES_OK_ASYNC(context,\n"
        ],
        "Title": "\n          Null pointer dereference in `SparseFillEmptyRows`\n        "
    },
    {
        "Bug description": "An attacker can trigger a null pointer dereference in the implementation of  tf.raw_ops.EditDistance :",
        "Sample Code": "hypothesis_indices = tf.constant([247, 247, 247], shape=[1, 3], dtype=tf.int64)\nhypothesis_values = tf.constant([-9.9999], shape=[1], dtype=tf.float32)\nhypothesis_shape = tf.constant([0, 0, 0], shape=[3], dtype=tf.int64)\ntruth_indices = tf.constant([], shape=[0, 3], dtype=tf.int64)\ntruth_values = tf.constant([], shape=[0], dtype=tf.float32)\ntruth_shape = tf.constant([0, 0, 0], shape=[3], dtype=tf.int64)\n\ntf.raw_ops.EditDistance(\n    hypothesis_indices=hypothesis_indices, hypothesis_values=hypothesis_values,\n    hypothesis_shape=hypothesis_shape, truth_indices=truth_indices,\n    ,\n    truth_values=truth_values, truth_shape=truth_shape, normalize=True)",
        "Bug fix": [
            "@@ -64,6 +64,12 @@ Status ValidateShapes(OpKernelContext* ctx, const Tensor& hypothesis_indices,\n     return errors::InvalidArgument(\n         \"truth_shape should be a vector, but got shape: \",\n         truth_shape.shape().DebugString());\n+  if (hypothesis_values.NumElements() != hypothesis_indices.dim_size(0))\n+    return errors::InvalidArgument(\n+        \"Expected hypothesis_values.NumElements == \"\n+        \"#rows(hypothesis_indices), their shapes are: \",\n+        hypothesis_values.shape().DebugString(), \" and \",\n+        hypothesis_indices.shape().DebugString());\n   if (hypothesis_shape.NumElements() != hypothesis_indices.dim_size(1))\n     return errors::InvalidArgument(\n         \"Expected hypothesis_shape.NumElements == \"\n@@ -75,6 +81,12 @@ Status ValidateShapes(OpKernelContext* ctx, const Tensor& hypothesis_indices,\n         \"Input SparseTensors must have rank at least 2, but truth_shape \"\n         \"rank is: \",\n         truth_shape.NumElements());\n+  if (truth_values.NumElements() != truth_indices.dim_size(0))\n+    return errors::InvalidArgument(\n+        \"Expected truth_values.NumElements == \"\n+        \"#rows(truth_indices), their shapes are: \",\n+        truth_values.shape().DebugString(), \" and \",\n+        truth_indices.shape().DebugString());\n   if (truth_shape.NumElements() != truth_indices.dim_size(1))\n     return errors::InvalidArgument(\n         \"Expected truth_shape.NumElements == \"\n@@ -153,6 +165,11 @@ class EditDistanceOp : public OpKernel {\n       output_shape.AddDim(std::max(hypothesis_st_shape.dim_size(d),\n                                    truth_st_shape.dim_size(d)));\n     }\n+    const auto output_elements = output_shape.num_elements();\n+    OP_REQUIRES(\n+        ctx, output_elements > 0,\n+        errors::InvalidArgument(\"Got output shape \", output_shape.DebugString(),\n+                                \" which has 0 elements\"));\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(\"output\", output_shape, &output));\n@@ -185,6 +202,12 @@ class EditDistanceOp : public OpKernel {\n       if (g_truth == g_hypothesis) {\n         auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                       output_strides.begin(), int64{0});\n+        OP_REQUIRES(\n+            ctx, loc < output_elements,\n+            errors::Internal(\"Got an inner product \", loc,\n+                             \" which would require in writing to outside of \"\n+                             \"the buffer for the output tensor (max elements \",\n+                             output_elements, \")\"));\n         output_t(loc) =\n             gtl::LevenshteinDistance<T>(truth_seq, hypothesis_seq, cmp);\n         if (normalize_) output_t(loc) /= truth_seq.size();\n@@ -194,6 +217,12 @@ class EditDistanceOp : public OpKernel {\n       } else if (g_truth > g_hypothesis) {  // zero-length truth\n         auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                       output_strides.begin(), int64{0});\n+        OP_REQUIRES(\n+            ctx, loc < output_elements,\n+            errors::Internal(\"Got an inner product \", loc,\n+                             \" which would require in writing to outside of \"\n+                             \"the buffer for the output tensor (max elements \",\n+                             output_elements, \")\"));\n         output_t(loc) = hypothesis_seq.size();\n         if (normalize_ && output_t(loc) != 0.0f) {\n           output_t(loc) = std::numeric_limits<float>::infinity();\n@@ -202,6 +231,12 @@ class EditDistanceOp : public OpKernel {\n       } else {  // zero-length hypothesis\n         auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                       output_strides.begin(), int64{0});\n+        OP_REQUIRES(\n+            ctx, loc < output_elements,\n+            errors::Internal(\"Got an inner product \", loc,\n+                             \" which would require in writing to outside of \"\n+                             \"the buffer for the output tensor (max elements \",\n+                             output_elements, \")\"));\n         output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n         ++truth_iter;\n       }\n@@ -212,6 +247,12 @@ class EditDistanceOp : public OpKernel {\n       auto hypothesis_seq = hypothesis_j.values<T>();\n       auto loc = std::inner_product(g_hypothesis.begin(), g_hypothesis.end(),\n                                     output_strides.begin(), int64{0});\n+      OP_REQUIRES(\n+          ctx, loc < output_elements,\n+          errors::Internal(\"Got an inner product \", loc,\n+                           \" which would require in writing to outside of the \"\n+                           \"buffer for the output tensor (max elements \",\n+                           output_elements, \")\"));\n       output_t(loc) = hypothesis_seq.size();\n       if (normalize_ && output_t(loc) != 0.0f) {\n         output_t(loc) = std::numeric_limits<float>::infinity();\n@@ -224,6 +265,12 @@ class EditDistanceOp : public OpKernel {\n       auto truth_seq = truth_i.values<T>();\n       auto loc = std::inner_product(g_truth.begin(), g_truth.end(),\n                                     output_strides.begin(), int64{0});\n+      OP_REQUIRES(\n+          ctx, loc < output_elements,\n+          errors::Internal(\"Got an inner product \", loc,\n+                           \" which would require in writing to outside of the \"\n+                           \"buffer for the output tensor (max elements \",\n+                           output_elements, \")\"));\n       output_t(loc) = (normalize_) ? 1.0 : truth_seq.size();\n       ++truth_iter;\n     }\n"
        ],
        "Title": "\n          Null pointer dereference in `EditDistance`\n        "
    },
    {
        "Bug description": "An attacker can cause a denial of service by exploiting a  CHECK -failure coming from the implementation of  tf.raw_ops.RFFT :",
        "Sample Code": "inputs = tf.constant([1], shape=[1], dtype=tf.float32)\nfft_length = tf.constant([0], shape=[1], dtype=tf.int32)\n\n)\n\ntf.raw_ops.RFFT(input=inputs, fft_length=fft_length)",
        "Bug fix": [
            "@@ -222,6 +222,9 @@ class FFTCPU : public FFTBase {\n       input_slice_sizes[i] = fft_shape[i - 1];\n       temp_shape.AddDim(fft_shape[i - 1]);\n     }\n+    OP_REQUIRES(ctx, temp_shape.num_elements() > 0,\n+                errors::InvalidArgument(\"Obtained a FFT shape of 0 elements: \",\n+                                        temp_shape.DebugString()));\n \n     auto output = out->flat_inner_dims<ComplexT, FFTRank + 1>();\n     const Eigen::DSizes<Eigen::DenseIndex, FFTRank + 1> zero_start_indices;\n"
        ],
        "Title": "\n          `CHECK`-fail in `tf.raw_ops.RFFT`\n        "
    }
]
[
    {
        "Bug description": "An attacker can cause a denial of service by exploiting a  CHECK -failure coming from the implementation of  tf.raw_ops.IRFFT :",
        "Sample Code": "values = [-10.0] * 130\nvalues[0] = -9.999999999999995\ninputs = tf.constant(values, shape=[10, 13], dtype=tf.float32)\ninputs = tf.cast(inputs, dtype=tf.complex64)\nfft_length = tf.constant([0], shape=[1], dtype=tf.int32)\n\n)\n\ntf.raw_ops.IRFFT(input=inputs, fft_length=fft_length)",
        "Bug fix": [
            "@@ -13,6 +13,7 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include \"tensorflow/core/platform/errors.h\"\n #define EIGEN_USE_THREADS\n \n // See docs in ../ops/fft_ops.cc.\n@@ -261,6 +262,9 @@ class FFTCPU : public FFTBase {\n           i == FFTRank ? fft_shape[i - 1] / 2 + 1 : fft_shape[i - 1];\n       full_fft_shape.AddDim(fft_shape[i - 1]);\n     }\n+    OP_REQUIRES(ctx, full_fft_shape.num_elements() > 0,\n+                errors::InvalidArgument(\"Obtained a FFT shape of 0 elements: \",\n+                                        full_fft_shape.DebugString()));\n \n     Tensor temp;\n     OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<ComplexT>::v(),\n"
        ],
        "Title": "\n          `CHECK`-fail in `tf.raw_ops.IRFFT`\n        "
    },
    {
        "Bug description": "An attacker can cause a denial of service by exploiting a  CHECK -failure coming from  tf.raw_ops.LoadAndRemapMatrix :",
        "Sample Code": "ckpt_path = tf.constant([], shape=[0], dtype=tf.string)\nold_tensor_name = tf.constant(\"\")\nrow_remapping = tf.constant([], shape=[0], dtype=tf.int64)\ncol_remapping = tf.constant([1], shape=[1], dtype=tf.int64)\ninitializing_values = tf.constant(1.0)\n\ntf.raw_ops.LoadAndRemapMatrix(\n    ckpt_path=ckpt_path, old_tensor_name=old_tensor_name,\n    row_remapping=row_remapping, col_remapping=col_remapping,\n    ,\n    initializing_values=initializing_values, num_rows=0, num_cols=1)",
        "Bug fix": [
            "@@ -123,6 +123,11 @@ class LoadAndRemapMatrixOp : public OpKernel {\n     // Processes the checkpoint source and the provided Tensor name.\n     const Tensor* ckpt_path_t;\n     OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n+    OP_REQUIRES(\n+        context, ckpt_path_t->NumElements() == 1,\n+        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n+                                \"element, got tensor of shape \",\n+                                ckpt_path_t->shape().DebugString()));\n     const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n     const Tensor* old_tensor_name_t;\n     OP_REQUIRES_OK(context,\n"
        ],
        "Title": "\n          `CHECK`-fail in `LoadAndRemapMatrix` \n        "
    },
    {
        "Bug description": "An attacker can cause a heap buffer overflow in  tf.raw_ops.RaggedTensorToTensor :",
        "Sample Code": "shape = tf.constant([10, 10], shape=[2], dtype=tf.int64)\nvalues = tf.constant(0, shape=[1], dtype=tf.int64)\ndefault_value = tf.constant(0, dtype=tf.int64)\nl = [849, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nrow = tf.constant(l, shape=[5, 43], dtype=tf.int64)\nrows = [row]\ntypes = ['ROW_SPLITS']\n\ntf.raw_ops.RaggedTensorToTensor(\n    shape=shape, values=values, default_value=default_value,\n    ,\n    row_partition_tensors=rows, row_partition_types=types) ",
        "Bug fix": [
            "@@ -313,6 +313,12 @@ class RaggedTensorToTensorBaseOp : public OpKernel {\n             output_index_multiplier, output_size, result);\n         return tensorflow::Status::OK();\n       case RowPartitionType::ROW_SPLITS:\n+        if (row_partition_tensor.size() - 1 > parent_output_index.size()) {\n+          return errors::InvalidArgument(\n+              \"Row partition size is greater than output size: \",\n+              row_partition_tensor.size() - 1, \" > \",\n+              parent_output_index.size());\n+        }\n         CalculateOutputIndexRowSplit(\n             context, row_partition_tensor, parent_output_index,\n             output_index_multiplier, output_size, result);\n"
        ],
        "Title": "\n          Heap buffer overflow in `RaggedTensorToTensor`\n        "
    },
    {
        "Bug description": "An attacker can access data outside of bounds of heap allocated array in  tf.raw_ops.UnicodeEncode :",
        "Sample Code": "input_values = tf.constant([58], shape=[1], dtype=tf.int32)\ninput_splits = tf.constant([[81, 101, 0]], shape=[3], dtype=tf.int32)\noutput_encoding = \"UTF-8\"\n\ntf.raw_ops.UnicodeEncode(\n    input_values=input_values, input_splits=input_splits,\n    ,\n    output_encoding=output_encoding)",
        "Bug fix": [
            "@@ -533,6 +533,17 @@ class UnicodeEncodeOp : public OpKernel {\n     const Tensor& input_splits = context->input(1);\n     const auto input_splits_flat = input_splits.flat<SPLITS_TYPE>();\n \n+    // Operation will treat first argument in input_splits as if it were zero\n+    // regardless of its actual value since splits should begin with zero and\n+    // end with the length of the input values vector.\n+    OP_REQUIRES(\n+        context, input_splits_flat(0) == 0,\n+        errors::InvalidArgument(\"First value in input_splits must be zero.\"));\n+    OP_REQUIRES(context,\n+                input_splits_flat(input_splits_flat.size() - 1) ==\n+                    input_tensor_flat.size(),\n+                errors::InvalidArgument(\"Last value in input_splits must be \"\n+                                        \"equal to length of input_tensor.\"));\n     // Since we limit to a 2-D input (flat_values of rank 1 and a single splits\n     // tensor), our output dimension will be 1 with it's size equal to the\n     // number of splits (outer dimension or ragged tensor).\n@@ -548,6 +559,14 @@ class UnicodeEncodeOp : public OpKernel {\n     for (int i = 1; i < input_splits_flat.size(); ++i) {\n       icu::UnicodeString unicode_string;\n       icu::UnicodeStringAppendable appendable_unicode_string(unicode_string);\n+      OP_REQUIRES(\n+          context, input_splits_flat(i - 1) <= input_splits_flat(i),\n+          errors::InvalidArgument(\n+              \"Values in input_splits must be equal or in ascending order.\"));\n+      OP_REQUIRES(\n+          context, input_splits_flat(i) <= input_tensor_flat.size(),\n+          errors::InvalidArgument(\"Values in input_splits must be less than or \"\n+                                  \"equal to input_tensor length.\"));\n       for (; idx < input_splits_flat(i); ++idx) {\n         int32 code_point = input_tensor_flat(idx);\n         // Check for invalid code point\n"
        ],
        "Title": "\n          Heap OOB access in unicode ops\n        "
    },
    {
        "Bug description": "An attacker can cause a heap buffer overflow in  tf.raw_ops.SparseSplit :",
        "Sample Code": "shape_dims = tf.constant(0, dtype=tf.int64)\nindices = tf.ones([1, 1], dtype=tf.int64)\nvalues = tf.ones([1], dtype=tf.int64)\nshape = tf.ones([1], dtype=tf.int64)\n\ntf.raw_ops.SparseSplit(\n    split_dim=shape_dims, indices=indices, values=values,\n    ,\n    shape=shape, num_split=1)",
        "Bug fix": [
            "@@ -527,6 +527,10 @@ inline Status SparseTensor::Split(const SparseTensor& input_tensor,\n   for (int i = 0; i < input_tensor.indices().dim_size(0); ++i) {\n     const int dim = input_tensor.indices().matrix<int64>()(i, split_dim);\n     int slice_index = GetSliceIndex(dim, split_size, residual);\n+    if (slice_index >= num_values.size()) {\n+      return errors::InvalidArgument(\"Slice index \", slice_index,\n+                                     \" is larger than num_split.\");\n+    }\n     num_values[slice_index]++;\n   }\n \n"
        ],
        "Title": "\n          Heap buffer overflow in `SparseSplit`\n        "
    },
    {
        "Bug description": "An attacker can cause a denial of service via a FPE runtime error in  tf.raw_ops.Reverse :",
        "Sample Code": "tensor_input = tf.constant([], shape=[0, 1, 1], dtype=tf.int32)\ndims = tf.constant([False, True, False], shape=[3], dtype=tf.bool)\n\n)\n\ntf.raw_ops.Reverse(tensor=tensor_input, dims=dims)",
        "Bug fix": [
            "@@ -155,6 +155,12 @@ class ReverseOp : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n+    // If input is provided, check to make sure the first dimension is valid.\n+    if (input.dims() > 0) {\n+      OP_REQUIRES(\n+          context, input.dim_size(0) != 0,\n+          errors::InvalidArgument(\"Invalid input first dimension. Found 0.\"));\n+    }\n     const Tensor& dims = context->input(1);\n \n     if (TensorShapeUtils::IsScalar(input.shape())) {\n"
        ],
        "Title": "\n          Division by 0 in `Reverse`\n        "
    },
    {
        "Bug description": "An attacker can cause a denial of service via a FPE runtime error in  tf.raw_ops.SparseMatMul :",
        "Sample Code": "a = tf.constant([100.0, 100.0, 100.0, 100.0], shape=[2, 2], dtype=tf.float32)\nb = tf.constant([], shape=[0, 2], dtype=tf.float32)\n\ntf.raw_ops.SparseMatMul(\n    a=a, b=b, transpose_a=True, transpose_b=True,\n    ,\n    a_is_sparse=True, b_is_sparse=True)",
        "Bug fix": [
            "@@ -1039,6 +1039,10 @@ class SparseMatMulOp : public OpKernel {\n     if (transpose_b) {\n       // TODO(agarwal): avoid transposing the matrix here and directly handle\n       // transpose in CreateDenseSlices.\n+      OP_REQUIRES(ctx, right->dim_size(0) != 0,\n+                  errors::InvalidArgument(\"b has an entry 0 in it's shape.\"));\n+      OP_REQUIRES(ctx, right->dim_size(1) != 0,\n+                  errors::InvalidArgument(\"b has an entry 0 in it's shape.\"));\n       right_tr.reset(\n           new Tensor(right->dtype(),\n                      TensorShape({right->dim_size(1), right->dim_size(0)})));\n"
        ],
        "Title": "\n          Division by 0 in `SparseMatMul`\n        "
    },
    {
        "Bug description": "An attacker can cause a denial of service via a FPE runtime error in  tf.raw_ops.FusedBatchNorm :",
        "Sample Code": "x = tf.constant([], shape=[1, 1, 1, 0], dtype=tf.float32)\nscale = tf.constant([], shape=[0], dtype=tf.float32)\noffset = tf.constant([], shape=[0], dtype=tf.float32)\nmean = tf.constant([], shape=[0], dtype=tf.float32)\nvariance = tf.constant([], shape=[0], dtype=tf.float32)\nepsilon = 0.0\nexponential_avg_factor = 0.0\ndata_format = \"NHWC\"\nis_training = False\n\ntf.raw_ops.FusedBatchNorm(\n    x=x, scale=scale, offset=offset, mean=mean,\n    variance=variance, epsilon=epsilon,\n    exponential_avg_factor=exponential_avg_factor,\n    ,\n    data_format=data_format, is_training=is_training)",
        "Bug fix": [
            "@@ -293,6 +293,9 @@ struct FusedBatchNorm<CPUDevice, T, U, /* is_training= */ false> {\n     const CPUDevice& d = context->eigen_device<CPUDevice>();\n \n     const int depth = x.dimension(3);\n+    OP_REQUIRES(\n+        context, depth != 0,\n+        errors::Internal(\"The 4th element in the input shape cannot be 0.\"));\n     const int size = x.size();\n     const int rest_size = size / depth;\n     Eigen::DSizes<Eigen::Index, 2> rest_by_depth(rest_size, depth);\n"
        ],
        "Title": "\n          Division by 0 in `FusedBatchNorm`\n        "
    },
    {
        "Bug description": "An attacker can cause a denial of service via a FPE runtime error in  tf.raw_ops.DenseCountSparseOutput :",
        "Sample Code": "values = tf.constant([], shape=[0, 0], dtype=tf.int64)\nweights = tf.constant([])\n\ntf.raw_ops.DenseCountSparseOutput(\n  values=values, weights=weights,\n  ,\n  minlength=-1, maxlength=58, binary_output=True)",
        "Bug fix": [
            "@@ -122,6 +122,9 @@ class DenseCount : public OpKernel {\n \n     int num_batch_elements = 1;\n     for (int i = 0; i < num_batch_dimensions; ++i) {\n+      OP_REQUIRES(context, data.shape().dim_size(i) != 0,\n+                  errors::InvalidArgument(\n+                      \"Invalid input: Shapes dimension cannot be 0.\"));\n       num_batch_elements *= data.shape().dim_size(i);\n     }\n     int num_value_elements = data.shape().num_elements() / num_batch_elements;\n"
        ],
        "Title": "\n          Division by 0 in `DenseCountSparseOutput`\n        "
    },
    {
        "Bug description": "An attacker can cause a denial of service by controlling the values of  num_segments  tensor argument for  UnsortedSegmentJoin :",
        "Sample Code": "inputs = tf.constant([], dtype=tf.string)\nsegment_ids = tf.constant([], dtype=tf.int32)\nnum_segments = tf.constant([], dtype=tf.int32)\nseparator = ''\n\ntf.raw_ops.UnsortedSegmentJoin(\n  inputs=inputs, segment_ids=segment_ids,\n  ,\n  num_segments=num_segments, separator=separator)",
        "Bug fix": [
            "@@ -90,6 +90,8 @@ class UnsortedSegmentJoinOp : public OpKernel {\n     const int32 segment_dims = segment_id_shape.dims();\n \n     const Tensor& num_segments_tensor = context->input(2);\n+    OP_REQUIRES(context, num_segments_tensor.NumElements() != 0,\n+                errors::InvalidArgument(\"Number of segments cannot be empty.\"));\n     auto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();\n \n     OP_REQUIRES(context, segment_dims != 0,\n"
        ],
        "Title": "\n          `CHECK`-failure in `UnsortedSegmentJoin`\n        "
    }
]
[
    {
        "Bug description": "An attacker can read data outside of bounds of heap allocated buffer in  tf.raw_ops.QuantizeAndDequantizeV3 :",
        "Sample Code": "tf.raw_ops.QuantizeAndDequantizeV3(\n  input=[2.5,2.5], input_min=[0,0], input_max=[1,1], num_bits=[30],\n  ],\n  signed_input=False, range_given=False, narrow_range=False, axis=3)",
        "Bug fix": [
            "@@ -13,6 +13,7 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include \"tensorflow/core/framework/op_requires.h\"\n #define EIGEN_USE_THREADS\n \n #if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n@@ -234,6 +235,10 @@ class QuantizeAndDequantizeV3Op : public OpKernel {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input = ctx->input(0);\n+    OP_REQUIRES(ctx, axis_ < input.dims(),\n+                errors::InvalidArgument(\n+                    \"Axis requested is larger than input dimensions. Axis: \",\n+                    axis_, \" Input Dimensions: \", input.dims()));\n     const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n"
        ],
        "Title": "\n          Heap OOB in `QuantizeAndDequantizeV3`\n        "
    },
    {
        "Bug description": "The implementation of  MatrixTriangularSolve  fails to terminate kernel execution if one validation condition fails:",
        "Sample Code": "import numpy as np\n\nmatrix_array = np.array([])\nmatrix_tensor = tf.convert_to_tensor(np.reshape(matrix_array,(1,0)),dtype=tf.float32)\nrhs_array = np.array([])\nrhs_tensor = tf.convert_to_tensor(np.reshape(rhs_array,(0,1)),dtype=tf.float32)\n\n)\n\ntf.raw_ops.MatrixTriangularSolve(matrix=matrix_tensor,rhs=rhs_tensor,lower=False,adjoint=False)",
        "Bug fix": [
            "@@ -162,6 +162,9 @@ class BaseMatrixTriangularSolveOp : public OpKernel {\n     const Tensor& in1 = ctx->input(1);\n \n     ValidateInputTensors(ctx, in0, in1);\n+    if (!ctx->status().ok()) {\n+      return;\n+    }\n \n     MatMulBCast bcast(in0.shape().dim_sizes(), in1.shape().dim_sizes());\n     OP_REQUIRES(\n@@ -230,13 +233,22 @@ class MatrixTriangularSolveOp\n  private:\n   void ValidateInputTensors(OpKernelContext* ctx, const Tensor& in0,\n                             const Tensor& in1) override {\n+    const auto in0_num_dims = in0.dims();\n     OP_REQUIRES(\n-        ctx, in0.dims() >= 2,\n-        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0.dims()));\n+        ctx, in0_num_dims >= 2,\n+        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0_num_dims));\n \n+    const auto in1_num_dims = in1.dims();\n     OP_REQUIRES(\n-        ctx, in1.dims() >= 2,\n-        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in1.dims()));\n+        ctx, in1_num_dims >= 2,\n+        errors::InvalidArgument(\"In[1] ndims must be >= 2: \", in1_num_dims));\n+\n+    const auto in0_last_dim = in0.dim_size(in0_num_dims - 1);\n+    const auto in0_prev_dim = in0.dim_size(in0_num_dims - 2);\n+    OP_REQUIRES(ctx, in0_last_dim == in0_prev_dim,\n+                errors::InvalidArgument(\n+                    \"In[0] matrices in the last dimensions must be square (\",\n+                    in0_last_dim, \" =/= \", in0_prev_dim, \")\"));\n   }\n };\n \n"
        ],
        "Title": "\n          OOB read in `MatrixTriangularSolve`\n        "
    },
    {
        "Bug description": "An attacker can cause a runtime division by zero error and denial of service in  tf.raw_ops.FractionalAvgPool :",
        "Sample Code": "value = tf.constant([60], shape=[1, 1, 1, 1], dtype=tf.int32)\npooling_ratio = [1.0, 1.0000014345305555, 1.0, 1.0]\npseudo_random = False\noverlapping = False\ndeterministic = False\nseed = 0\nseed2 = 0\n\ntf.raw_ops.FractionalAvgPool(\n  value=value, pooling_ratio=pooling_ratio, pseudo_random=pseudo_random,\n  ,\n  overlapping=overlapping, deterministic=deterministic, seed=seed, seed2=seed2)",
        "Bug fix": [
            "@@ -80,6 +80,10 @@ class FractionalAvgPoolOp : public OpKernel {\n     std::vector<int> output_size(tensor_in_and_out_dims);\n     for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n       input_size[i] = tensor_in.dim_size(i);\n+      OP_REQUIRES(\n+          context, pooling_ratio_[i] <= input_size[i],\n+          errors::InvalidArgument(\n+              \"Pooling ratio cannot be bigger than input tensor dim size.\"));\n     }\n     // Output size.\n     for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n"
        ],
        "Title": "\n          Division by 0 in `FractionalAvgPool`\n        "
    },
    {
        "Bug description": "An attacker can cause a runtime division by zero error and denial of service in  tf.raw_ops.QuantizedAdd :",
        "Sample Code": "x = tf.constant([68, 228], shape=[2, 1], dtype=tf.quint8)\ny = tf.constant([], shape=[2, 0], dtype=tf.quint8)\n\nmin_x = tf.constant(10.723421015884028)\nmax_x = tf.constant(15.19578006631113)\nmin_y = tf.constant(-5.539003866682977)\nmax_y = tf.constant(42.18819949559947)\n\n)\n\ntf.raw_ops.QuantizedAdd(x=x, y=y, min_x=min_x, max_x=max_x, min_y=min_y, max_y=max_y)",
        "Bug fix": [
            "@@ -538,6 +538,8 @@ class QuantizedAddOp : public OpKernel {\n         tensor_min = min_x;\n         tensor_max = max_x;\n       }\n+      OP_REQUIRES(context, vector_num_elements > 0,\n+                  errors::InvalidArgument(\"Must have some elements to add\"));\n       VectorTensorAddition<T, Toutput>(\n           vector_data, vector_min, vector_max, vector_num_elements, tensor_data,\n           tensor_min, tensor_max, tensor_num_elements, min_z_value, max_z_value,\n"
        ],
        "Title": "\n          Division by 0 in `QuantizedAdd`\n        "
    },
    {
        "Bug description": "An attacker can cause a runtime division by zero error and denial of service in  tf.raw_ops.QuantizedBatchNormWithGlobalNormalization :",
        "Sample Code": "t = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.quint8)\nt_min = tf.constant(-10.0, dtype=tf.float32)\nt_max = tf.constant(-10.0, dtype=tf.float32)\nm = tf.constant([], shape=[0], dtype=tf.quint8)\nm_min = tf.constant(-10.0, dtype=tf.float32)\nm_max = tf.constant(-10.0, dtype=tf.float32)\nv = tf.constant([], shape=[0], dtype=tf.quint8)\nv_min = tf.constant(-10.0, dtype=tf.float32)\nv_max = tf.constant(-10.0, dtype=tf.float32)\nbeta = tf.constant([], shape=[0], dtype=tf.quint8)\nbeta_min = tf.constant(-10.0, dtype=tf.float32)\nbeta_max = tf.constant(-10.0, dtype=tf.float32)\ngamma = tf.constant([], shape=[0], dtype=tf.quint8)\ngamma_min = tf.constant(-10.0, dtype=tf.float32)\ngamma_max = tf.constant(-10.0, dtype=tf.float32)\n\ntf.raw_ops.QuantizedBatchNormWithGlobalNormalization(\n  t=t, t_min=t_min, t_max=t_max, m=m, m_min=m_min, m_max=m_max,\n  v=v, v_min=v_min, v_max=v_max, beta=beta, beta_min=beta_min,\n  beta_max=beta_max, gamma=gamma, gamma_min=gamma_min,\n  gamma_max=gamma_max, out_type=tf.qint32,\n  ,\n  variance_epsilon=0.1, scale_after_normalization=True)",
        "Bug fix": [
            "@@ -173,20 +173,50 @@ class QuantizedBatchNormOp : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n-    const float input_min = context->input(1).flat<float>()(0);\n-    const float input_max = context->input(2).flat<float>()(0);\n+    const auto& input_min_tensor = context->input(1);\n+    OP_REQUIRES(context, input_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"input_min must have 1 element\"));\n+    const float input_min = input_min_tensor.flat<float>()(0);\n+    const auto& input_max_tensor = context->input(2);\n+    OP_REQUIRES(context, input_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"input_max must have 1 element\"));\n+    const float input_max = input_max_tensor.flat<float>()(0);\n     const Tensor& mean = context->input(3);\n-    const float mean_min = context->input(4).flat<float>()(0);\n-    const float mean_max = context->input(5).flat<float>()(0);\n+    const auto& mean_min_tensor = context->input(4);\n+    OP_REQUIRES(context, mean_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"mean_min must have 1 element\"));\n+    const float mean_min = mean_min_tensor.flat<float>()(0);\n+    const auto& mean_max_tensor = context->input(5);\n+    OP_REQUIRES(context, mean_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"mean_max must have 1 element\"));\n+    const float mean_max = mean_max_tensor.flat<float>()(0);\n     const Tensor& var = context->input(6);\n-    const float var_min = context->input(7).flat<float>()(0);\n-    const float var_max = context->input(8).flat<float>()(0);\n+    const auto& var_min_tensor = context->input(7);\n+    OP_REQUIRES(context, var_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"var_min must have 1 element\"));\n+    const float var_min = var_min_tensor.flat<float>()(0);\n+    const auto& var_max_tensor = context->input(8);\n+    OP_REQUIRES(context, var_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"var_max must have 1 element\"));\n+    const float var_max = var_max_tensor.flat<float>()(0);\n     const Tensor& beta = context->input(9);\n-    const float beta_min = context->input(10).flat<float>()(0);\n-    const float beta_max = context->input(11).flat<float>()(0);\n+    const auto& beta_min_tensor = context->input(10);\n+    OP_REQUIRES(context, beta_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"beta_min must have 1 element\"));\n+    const float beta_min = beta_min_tensor.flat<float>()(0);\n+    const auto& beta_max_tensor = context->input(11);\n+    OP_REQUIRES(context, beta_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"beta_max must have 1 element\"));\n+    const float beta_max = beta_max_tensor.flat<float>()(0);\n     const Tensor& gamma = context->input(12);\n-    const float gamma_min = context->input(13).flat<float>()(0);\n-    const float gamma_max = context->input(14).flat<float>()(0);\n+    const auto& gamma_min_tensor = context->input(13);\n+    OP_REQUIRES(context, gamma_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"gamma_min must have 1 element\"));\n+    const float gamma_min = gamma_min_tensor.flat<float>()(0);\n+    const auto& gamma_max_tensor = context->input(14);\n+    OP_REQUIRES(context, gamma_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"gamma_max must have 1 element\"));\n+    const float gamma_max = gamma_max_tensor.flat<float>()(0);\n \n     OP_REQUIRES(context, input.dims() == 4,\n                 errors::InvalidArgument(\"input must be 4-dimensional\",\n@@ -203,6 +233,33 @@ class QuantizedBatchNormOp : public OpKernel {\n     OP_REQUIRES(context, gamma.dims() == 1,\n                 errors::InvalidArgument(\"gamma must be 1-dimensional\",\n                                         gamma.shape().DebugString()));\n+    OP_REQUIRES(context, mean.NumElements() > 1,\n+                errors::InvalidArgument(\"Must have at least a mean value\",\n+                                        gamma.shape().DebugString()));\n+    OP_REQUIRES(context, mean.NumElements() > 1,\n+                errors::InvalidArgument(\"Must have at least a mean value\"));\n+    const auto last_dim = input.shape().dims() - 1;\n+    OP_REQUIRES(context,\n+                mean.shape().dim_size(0) == input.shape().dim_size(last_dim),\n+                errors::InvalidArgument(\"Must provide as many means as the \"\n+                                        \"last dimension of the input tensor: \",\n+                                        mean.shape().DebugString(), \" vs. \",\n+                                        input.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, mean.shape().dim_size(0) == var.shape().dim_size(0),\n+        errors::InvalidArgument(\n+            \"Mean and variance tensors must have the same shape: \",\n+            mean.shape().DebugString(), \" vs. \", var.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, mean.shape().dim_size(0) == beta.shape().dim_size(0),\n+        errors::InvalidArgument(\n+            \"Mean and beta tensors must have the same shape: \",\n+            mean.shape().DebugString(), \" vs. \", beta.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, mean.shape().dim_size(0) == gamma.shape().dim_size(0),\n+        errors::InvalidArgument(\n+            \"Mean and gamma tensors must have the same shape: \",\n+            mean.shape().DebugString(), \" vs. \", gamma.shape().DebugString()));\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(context,\n"
        ],
        "Title": "\n          Division by 0 in `QuantizedBatchNormWithGlobalNormalization`\n        "
    },
    {
        "Bug description": "An attacker can cause a segfault and denial of service via accessing data outside of bounds in  tf.raw_ops.QuantizedBatchNormWithGlobalNormalization :",
        "Sample Code": "t = tf.constant([1], shape=[1, 1, 1, 1], dtype=tf.quint8)\nt_min = tf.constant([], shape=[0], dtype=tf.float32)\nt_max = tf.constant([], shape=[0], dtype=tf.float32)\nm = tf.constant([1], shape=[1], dtype=tf.quint8)\nm_min = tf.constant([], shape=[0], dtype=tf.float32)\nm_max = tf.constant([], shape=[0], dtype=tf.float32)\nv = tf.constant([1], shape=[1], dtype=tf.quint8)\nv_min = tf.constant([], shape=[0], dtype=tf.float32)\nv_max = tf.constant([], shape=[0], dtype=tf.float32)\nbeta = tf.constant([1], shape=[1], dtype=tf.quint8)\nbeta_min = tf.constant([], shape=[0], dtype=tf.float32)\nbeta_max = tf.constant([], shape=[0], dtype=tf.float32)\ngamma = tf.constant([1], shape=[1], dtype=tf.quint8)\ngamma_min = tf.constant([], shape=[0], dtype=tf.float32)\ngamma_max = tf.constant([], shape=[0], dtype=tf.float32) \n\ntf.raw_ops.QuantizedBatchNormWithGlobalNormalization(\n  t=t, t_min=t_min, t_max=t_max, m=m, m_min=m_min, m_max=m_max,\n  v=v, v_min=v_min, v_max=v_max, beta=beta, beta_min=beta_min,\n  beta_max=beta_max, gamma=gamma, gamma_min=gamma_min,\n  gamma_max=gamma_max, out_type=tf.qint32,\n  ,\n  variance_epsilon=0.1, scale_after_normalization=True)",
        "Bug fix": [
            "@@ -173,20 +173,50 @@ class QuantizedBatchNormOp : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n-    const float input_min = context->input(1).flat<float>()(0);\n-    const float input_max = context->input(2).flat<float>()(0);\n+    const auto& input_min_tensor = context->input(1);\n+    OP_REQUIRES(context, input_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"input_min must have 1 element\"));\n+    const float input_min = input_min_tensor.flat<float>()(0);\n+    const auto& input_max_tensor = context->input(2);\n+    OP_REQUIRES(context, input_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"input_max must have 1 element\"));\n+    const float input_max = input_max_tensor.flat<float>()(0);\n     const Tensor& mean = context->input(3);\n-    const float mean_min = context->input(4).flat<float>()(0);\n-    const float mean_max = context->input(5).flat<float>()(0);\n+    const auto& mean_min_tensor = context->input(4);\n+    OP_REQUIRES(context, mean_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"mean_min must have 1 element\"));\n+    const float mean_min = mean_min_tensor.flat<float>()(0);\n+    const auto& mean_max_tensor = context->input(5);\n+    OP_REQUIRES(context, mean_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"mean_max must have 1 element\"));\n+    const float mean_max = mean_max_tensor.flat<float>()(0);\n     const Tensor& var = context->input(6);\n-    const float var_min = context->input(7).flat<float>()(0);\n-    const float var_max = context->input(8).flat<float>()(0);\n+    const auto& var_min_tensor = context->input(7);\n+    OP_REQUIRES(context, var_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"var_min must have 1 element\"));\n+    const float var_min = var_min_tensor.flat<float>()(0);\n+    const auto& var_max_tensor = context->input(8);\n+    OP_REQUIRES(context, var_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"var_max must have 1 element\"));\n+    const float var_max = var_max_tensor.flat<float>()(0);\n     const Tensor& beta = context->input(9);\n-    const float beta_min = context->input(10).flat<float>()(0);\n-    const float beta_max = context->input(11).flat<float>()(0);\n+    const auto& beta_min_tensor = context->input(10);\n+    OP_REQUIRES(context, beta_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"beta_min must have 1 element\"));\n+    const float beta_min = beta_min_tensor.flat<float>()(0);\n+    const auto& beta_max_tensor = context->input(11);\n+    OP_REQUIRES(context, beta_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"beta_max must have 1 element\"));\n+    const float beta_max = beta_max_tensor.flat<float>()(0);\n     const Tensor& gamma = context->input(12);\n-    const float gamma_min = context->input(13).flat<float>()(0);\n-    const float gamma_max = context->input(14).flat<float>()(0);\n+    const auto& gamma_min_tensor = context->input(13);\n+    OP_REQUIRES(context, gamma_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"gamma_min must have 1 element\"));\n+    const float gamma_min = gamma_min_tensor.flat<float>()(0);\n+    const auto& gamma_max_tensor = context->input(14);\n+    OP_REQUIRES(context, gamma_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"gamma_max must have 1 element\"));\n+    const float gamma_max = gamma_max_tensor.flat<float>()(0);\n \n     OP_REQUIRES(context, input.dims() == 4,\n                 errors::InvalidArgument(\"input must be 4-dimensional\",\n@@ -203,6 +233,33 @@ class QuantizedBatchNormOp : public OpKernel {\n     OP_REQUIRES(context, gamma.dims() == 1,\n                 errors::InvalidArgument(\"gamma must be 1-dimensional\",\n                                         gamma.shape().DebugString()));\n+    OP_REQUIRES(context, mean.NumElements() > 1,\n+                errors::InvalidArgument(\"Must have at least a mean value\",\n+                                        gamma.shape().DebugString()));\n+    OP_REQUIRES(context, mean.NumElements() > 1,\n+                errors::InvalidArgument(\"Must have at least a mean value\"));\n+    const auto last_dim = input.shape().dims() - 1;\n+    OP_REQUIRES(context,\n+                mean.shape().dim_size(0) == input.shape().dim_size(last_dim),\n+                errors::InvalidArgument(\"Must provide as many means as the \"\n+                                        \"last dimension of the input tensor: \",\n+                                        mean.shape().DebugString(), \" vs. \",\n+                                        input.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, mean.shape().dim_size(0) == var.shape().dim_size(0),\n+        errors::InvalidArgument(\n+            \"Mean and variance tensors must have the same shape: \",\n+            mean.shape().DebugString(), \" vs. \", var.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, mean.shape().dim_size(0) == beta.shape().dim_size(0),\n+        errors::InvalidArgument(\n+            \"Mean and beta tensors must have the same shape: \",\n+            mean.shape().DebugString(), \" vs. \", beta.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, mean.shape().dim_size(0) == gamma.shape().dim_size(0),\n+        errors::InvalidArgument(\n+            \"Mean and gamma tensors must have the same shape: \",\n+            mean.shape().DebugString(), \" vs. \", gamma.shape().DebugString()));\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(context,\n"
        ],
        "Title": "\n          Heap out of bounds in `QuantizedBatchNormWithGlobalNormalization`\n        "
    },
    {
        "Bug description": "An attacker can trigger an integer division by zero undefined behavior in  tf.raw_ops.QuantizedBiasAdd :",
        "Sample Code": "input_tensor = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.quint8)\nbias = tf.constant([], shape=[0], dtype=tf.quint8)\nmin_input = tf.constant(-10.0, dtype=tf.float32)\nmax_input = tf.constant(-10.0, dtype=tf.float32)\nmin_bias = tf.constant(-10.0, dtype=tf.float32)\nmax_bias = tf.constant(-10.0, dtype=tf.float32)\n\ntf.raw_ops.QuantizedBiasAdd(input=input_tensor, bias=bias, min_input=min_input,\n                            max_input=max_input, min_bias=min_bias,\n                            ,\n                            max_bias=max_bias, out_type=tf.qint32)",
        "Bug fix": [
            "@@ -56,6 +56,8 @@ class QuantizedBiasAddOp : public OpKernel {\n             \"Must provide as many biases as the last dimension \"\n             \"of the input tensor: \",\n             bias.shape().DebugString(), \" vs. \", input.shape().DebugString()));\n+    OP_REQUIRES(context, bias.NumElements() > 0,\n+                errors::InvalidArgument(\"Must provide at least 1 bias\"));\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(context,\n"
        ],
        "Title": "\n          Division by 0 in `QuantizedBiasAdd`\n        "
    },
    {
        "Bug description": "An attacker can trigger a denial of service via a  CHECK -fail in converting sparse tensors to CSR Sparse matrices:",
        "Sample Code": "import numpy as np\nfrom tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops\n\nindices_array = np.array([[0, 0]])\nvalue_array = np.array([0.0], dtype=np.float32)\ndense_shape = [0, 0]\n\nst = tf.SparseTensor(indices_array, value_array, dense_shape)\n\nvalues_tensor = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n       (\n       st.indices, st.values, st.dense_shape)",
        "Bug fix": [
            "@@ -22,6 +22,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/tensor_types.h\"\n #include \"tensorflow/core/lib/core/errors.h\"\n #include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n \n namespace tensorflow {\n namespace functor {\n@@ -63,6 +64,11 @@ Status SparseTensorToCSRSparseMatrixCPUFunctor::operator()(\n \n     for (int64 i = 0; i < total_nnz; ++i) {\n       // For now, the rows pointers store the corresponding row counts.\n+      int64 ix = indices(i, 0) + 1;\n+      if (ix >= csr_row_ptr.size()) {\n+        return errors::InvalidArgument(\"Got an index \", ix,\n+                                       \" that is outside of csr_row_ptr\");\n+      }\n       csr_row_ptr(indices(i, 0) + 1) += 1;\n       csr_col_ind(i) = indices(i, 1);\n     }\n"
        ],
        "Title": "\n          Heap buffer overflow in `SparseTensorToCSRSparseMatrix`\n        "
    },
    {
        "Bug description": "An attacker can trigger a denial of service via a  CHECK -fail in  tf.raw_ops.CTCGreedyDecoder :",
        "Sample Code": "inputs = tf.constant([], shape=[18, 2, 0], dtype=tf.float32)\nsequence_length = tf.constant([-100, 17], shape=[2], dtype=tf.int32)\nmerge_repeated = False\n\n\n\ntf.raw_ops.CTCGreedyDecoder(inputs=inputs, sequence_length=sequence_length, merge_repeated=merge_repeated)",
        "Bug fix": [
            "@@ -232,6 +232,8 @@ class CTCGreedyDecoderOp : public OpKernel {\n         int prev_indices = -1;\n         for (int t = 0; t < seq_len_t(b); ++t) {\n           int max_class_indices;\n+          OP_REQUIRES(ctx, input_list_t[t].dimension(1) > 0,\n+                      errors::InvalidArgument(\"Invalid input dimensions.\"));\n           log_prob_t(b, 0) +=\n               -RowMax<T>(input_list_t[t], b, &max_class_indices);\n           if (max_class_indices != blank_index &&\n"
        ],
        "Title": "\n          `CHECK`-fail in `CTCGreedyDecoder`\n        "
    },
    {
        "Bug description": "An attacker can trigger a denial of service via a  CHECK -fail in  tf.raw_ops.QuantizeAndDequantizeV4Grad :",
        "Sample Code": "gradient_tensor = tf.constant([0.0], shape=[1])\ninput_tensor = tf.constant([0.0], shape=[1])\ninput_min = tf.constant([[0.0]], shape=[1, 1])\ninput_max = tf.constant([[0.0]], shape=[1, 1])\n\ntf.raw_ops.QuantizeAndDequantizeV4Grad(\n  gradients=gradient_tensor, input=input_tensor,\n  ,\n  input_min=input_min, input_max=input_max, axis=0)",
        "Bug fix": [
            "@@ -160,7 +160,17 @@ class QuantizeAndDequantizeV4GradientOp : public OpKernel {\n         errors::InvalidArgument(\"gradient and input must be the same size\"));\n     const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n     const Tensor& input_min_tensor = ctx->input(2);\n+    OP_REQUIRES(ctx,\n+                input_min_tensor.dims() == 0 || input_min_tensor.dims() == 1,\n+                errors::InvalidArgument(\n+                    \"Input min tensor must have dimension 1. Recieved \",\n+                    input_min_tensor.dims(), \".\"));\n     const Tensor& input_max_tensor = ctx->input(3);\n+    OP_REQUIRES(ctx,\n+                input_max_tensor.dims() == 0 || input_max_tensor.dims() == 1,\n+                errors::InvalidArgument(\n+                    \"Input max tensor must have dimension 1. Recieved \",\n+                    input_max_tensor.dims(), \".\"));\n     if (axis_ != -1) {\n       OP_REQUIRES(\n           ctx, input_min_tensor.dim_size(0) == depth,\n"
        ],
        "Title": "\n          `CHECK`-fail in `QuantizeAndDequantizeV4Grad`\n        "
    }
]
[
    {
        "Bug description": "An attacker can trigger a dereference of a null pointer in  tf.raw_ops.StringNGrams :",
        "Sample Code": "data=tf.constant([''] * 11, shape=[11], dtype=tf.string)\n\nsplits = [0]*115\nsplits.append(3)\ndata_splits=tf.constant(splits, shape=[116], dtype=tf.int64)\n\ntf.raw_ops.StringNGrams(data=data, data_splits=data_splits, separator=b'Ss',\n                        ngram_widths=[7,6,11],\n                        left_pad='ABCDE', right_pad=b'ZYXWVU',\n                        ,\n                        pad_width=50, preserve_short_sequences=True)",
        "Bug fix": [
            "@@ -61,16 +61,28 @@ class StringNGramsOp : public tensorflow::OpKernel {\n     OP_REQUIRES_OK(context, context->input(\"data_splits\", &splits));\n     const auto& splits_vec = splits->flat<SPLITS_TYPE>();\n \n-    // Validate that the splits are valid indices into data\n+    // Validate that the splits are valid indices into data, only if there are\n+    // splits specified.\n     const int input_data_size = data->flat<tstring>().size();\n     const int splits_vec_size = splits_vec.size();\n-    for (int i = 0; i < splits_vec_size; ++i) {\n-      bool valid_splits = splits_vec(i) >= 0;\n-      valid_splits = valid_splits && (splits_vec(i) <= input_data_size);\n-      OP_REQUIRES(\n-          context, valid_splits,\n-          errors::InvalidArgument(\"Invalid split value \", splits_vec(i),\n-                                  \", must be in [0,\", input_data_size, \"]\"));\n+    if (splits_vec_size > 0) {\n+      int prev_split = splits_vec(0);\n+      OP_REQUIRES(context, prev_split == 0,\n+                  errors::InvalidArgument(\"First split value must be 0, got \",\n+                                          prev_split));\n+      for (int i = 1; i < splits_vec_size; ++i) {\n+        bool valid_splits = splits_vec(i) >= prev_split;\n+        valid_splits = valid_splits && (splits_vec(i) <= input_data_size);\n+        OP_REQUIRES(context, valid_splits,\n+                    errors::InvalidArgument(\n+                        \"Invalid split value \", splits_vec(i), \", must be in [\",\n+                        prev_split, \", \", input_data_size, \"]\"));\n+        prev_split = splits_vec(i);\n+      }\n+      OP_REQUIRES(context, prev_split == input_data_size,\n+                  errors::InvalidArgument(\n+                      \"Last split value must be data size. Expected \",\n+                      input_data_size, \", got \", prev_split));\n     }\n \n     int num_batch_items = splits_vec.size() - 1;\n@@ -174,13 +186,31 @@ class StringNGramsOp : public tensorflow::OpKernel {\n         ngram->append(left_pad_);\n         ngram->append(separator_);\n       }\n+      // Only output first num_tokens - 1 pairs of data and separator\n       for (int n = 0; n < num_tokens - 1; ++n) {\n         ngram->append(data[data_start_index + n]);\n         ngram->append(separator_);\n       }\n-      ngram->append(data[data_start_index + num_tokens - 1]);\n-      for (int n = 0; n < right_padding; ++n) {\n-        ngram->append(separator_);\n+      // Handle case when there are no tokens or no right padding as these can\n+      // result in consecutive separators.\n+      if (num_tokens > 0) {\n+        // If we have tokens, then output last and then pair each separator with\n+        // the right padding that follows, to ensure ngram ends either with the\n+        // token or with the right pad.\n+        ngram->append(data[data_start_index + num_tokens - 1]);\n+        for (int n = 0; n < right_padding; ++n) {\n+          ngram->append(separator_);\n+          ngram->append(right_pad_);\n+        }\n+      } else {\n+        // If we don't have tokens, then the last item inserted into the ngram\n+        // has been the separator from the left padding loop above. Hence,\n+        // output right pad and separator and make sure to finish with a\n+        // padding, not a separator.\n+        for (int n = 0; n < right_padding - 1; ++n) {\n+          ngram->append(right_pad_);\n+          ngram->append(separator_);\n+        }\n         ngram->append(right_pad_);\n       }\n \n",
            "@@ -542,6 +542,40 @@ TEST_F(NgramKernelTest, TestEmptyInput) {\n   assert_int64_equal(expected_splits, *GetOutput(1));\n }\n \n+TEST_F(NgramKernelTest, TestNoTokens) {\n+  MakeOp(\"|\", {3}, \"L\", \"R\", -1, false);\n+  // Batch items are:\n+  // 0:\n+  // 1: \"a\"\n+  AddInputFromArray<tstring>(TensorShape({1}), {\"a\"});\n+  AddInputFromArray<int64>(TensorShape({3}), {0, 0, 1});\n+  TF_ASSERT_OK(RunOpKernel());\n+\n+  std::vector<tstring> expected_values(\n+      {\"L|L|R\", \"L|R|R\",             // no input in first split\n+       \"L|L|a\", \"L|a|R\", \"a|R|R\"});  // second split\n+  std::vector<int64> expected_splits({0, 2, 5});\n+\n+  assert_string_equal(expected_values, *GetOutput(0));\n+  assert_int64_equal(expected_splits, *GetOutput(1));\n+}\n+\n+TEST_F(NgramKernelTest, TestNoTokensNoPad) {\n+  MakeOp(\"|\", {3}, \"\", \"\", 0, false);\n+  // Batch items are:\n+  // 0:\n+  // 1: \"a\"\n+  AddInputFromArray<tstring>(TensorShape({1}), {\"a\"});\n+  AddInputFromArray<int64>(TensorShape({3}), {0, 0, 1});\n+  TF_ASSERT_OK(RunOpKernel());\n+\n+  std::vector<tstring> expected_values({});\n+  std::vector<int64> expected_splits({0, 0, 0});\n+\n+  assert_string_equal(expected_values, *GetOutput(0));\n+  assert_int64_equal(expected_splits, *GetOutput(1));\n+}\n+\n TEST_F(NgramKernelTest, ShapeFn) {\n   ShapeInferenceTestOp op(\"StringNGrams\");\n   INFER_OK(op, \"?;?\", \"[?];[?]\");\n"
        ],
        "Title": "\n          Null pointer dereference in `StringNGrams`\n        "
    },
    {
        "Bug description": "An attacker can cause a heap buffer overflow by passing crafted inputs to  tf.raw_ops.StringNGrams :",
        "Sample Code": "separator = b'    \nngram_widths = [7, 6, 11]\nleft_pad = b'\nright_pad = b'\npad_width = 50\npreserve_short_sequences = True\n  \nl = ['', '', '', '', '', '', '', '', '', '', '']\n  \ndata = tf.constant(l, shape=[11], dtype=tf.string)\n  \nl2 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n     0, 0, 3]\ndata_splits = tf.constant(l2, shape=[116], dtype=tf.int64)\n\nout = tf.raw_ops.StringNGrams(data=data,\n    data_splits=data_splits, separator=separator,\n    ngram_widths=ngram_widths, left_pad=left_pad,\n    right_pad=right_pad, pad_width=pad_width,\n    ,\n    preserve_short_sequences=preserve_short_sequences)",
        "Bug fix": [
            "@@ -61,16 +61,28 @@ class StringNGramsOp : public tensorflow::OpKernel {\n     OP_REQUIRES_OK(context, context->input(\"data_splits\", &splits));\n     const auto& splits_vec = splits->flat<SPLITS_TYPE>();\n \n-    // Validate that the splits are valid indices into data\n+    // Validate that the splits are valid indices into data, only if there are\n+    // splits specified.\n     const int input_data_size = data->flat<tstring>().size();\n     const int splits_vec_size = splits_vec.size();\n-    for (int i = 0; i < splits_vec_size; ++i) {\n-      bool valid_splits = splits_vec(i) >= 0;\n-      valid_splits = valid_splits && (splits_vec(i) <= input_data_size);\n-      OP_REQUIRES(\n-          context, valid_splits,\n-          errors::InvalidArgument(\"Invalid split value \", splits_vec(i),\n-                                  \", must be in [0,\", input_data_size, \"]\"));\n+    if (splits_vec_size > 0) {\n+      int prev_split = splits_vec(0);\n+      OP_REQUIRES(context, prev_split == 0,\n+                  errors::InvalidArgument(\"First split value must be 0, got \",\n+                                          prev_split));\n+      for (int i = 1; i < splits_vec_size; ++i) {\n+        bool valid_splits = splits_vec(i) >= prev_split;\n+        valid_splits = valid_splits && (splits_vec(i) <= input_data_size);\n+        OP_REQUIRES(context, valid_splits,\n+                    errors::InvalidArgument(\n+                        \"Invalid split value \", splits_vec(i), \", must be in [\",\n+                        prev_split, \", \", input_data_size, \"]\"));\n+        prev_split = splits_vec(i);\n+      }\n+      OP_REQUIRES(context, prev_split == input_data_size,\n+                  errors::InvalidArgument(\n+                      \"Last split value must be data size. Expected \",\n+                      input_data_size, \", got \", prev_split));\n     }\n \n     int num_batch_items = splits_vec.size() - 1;\n@@ -174,13 +186,31 @@ class StringNGramsOp : public tensorflow::OpKernel {\n         ngram->append(left_pad_);\n         ngram->append(separator_);\n       }\n+      // Only output first num_tokens - 1 pairs of data and separator\n       for (int n = 0; n < num_tokens - 1; ++n) {\n         ngram->append(data[data_start_index + n]);\n         ngram->append(separator_);\n       }\n-      ngram->append(data[data_start_index + num_tokens - 1]);\n-      for (int n = 0; n < right_padding; ++n) {\n-        ngram->append(separator_);\n+      // Handle case when there are no tokens or no right padding as these can\n+      // result in consecutive separators.\n+      if (num_tokens > 0) {\n+        // If we have tokens, then output last and then pair each separator with\n+        // the right padding that follows, to ensure ngram ends either with the\n+        // token or with the right pad.\n+        ngram->append(data[data_start_index + num_tokens - 1]);\n+        for (int n = 0; n < right_padding; ++n) {\n+          ngram->append(separator_);\n+          ngram->append(right_pad_);\n+        }\n+      } else {\n+        // If we don't have tokens, then the last item inserted into the ngram\n+        // has been the separator from the left padding loop above. Hence,\n+        // output right pad and separator and make sure to finish with a\n+        // padding, not a separator.\n+        for (int n = 0; n < right_padding - 1; ++n) {\n+          ngram->append(right_pad_);\n+          ngram->append(separator_);\n+        }\n         ngram->append(right_pad_);\n       }\n \n",
            "@@ -542,6 +542,40 @@ TEST_F(NgramKernelTest, TestEmptyInput) {\n   assert_int64_equal(expected_splits, *GetOutput(1));\n }\n \n+TEST_F(NgramKernelTest, TestNoTokens) {\n+  MakeOp(\"|\", {3}, \"L\", \"R\", -1, false);\n+  // Batch items are:\n+  // 0:\n+  // 1: \"a\"\n+  AddInputFromArray<tstring>(TensorShape({1}), {\"a\"});\n+  AddInputFromArray<int64>(TensorShape({3}), {0, 0, 1});\n+  TF_ASSERT_OK(RunOpKernel());\n+\n+  std::vector<tstring> expected_values(\n+      {\"L|L|R\", \"L|R|R\",             // no input in first split\n+       \"L|L|a\", \"L|a|R\", \"a|R|R\"});  // second split\n+  std::vector<int64> expected_splits({0, 2, 5});\n+\n+  assert_string_equal(expected_values, *GetOutput(0));\n+  assert_int64_equal(expected_splits, *GetOutput(1));\n+}\n+\n+TEST_F(NgramKernelTest, TestNoTokensNoPad) {\n+  MakeOp(\"|\", {3}, \"\", \"\", 0, false);\n+  // Batch items are:\n+  // 0:\n+  // 1: \"a\"\n+  AddInputFromArray<tstring>(TensorShape({1}), {\"a\"});\n+  AddInputFromArray<int64>(TensorShape({3}), {0, 0, 1});\n+  TF_ASSERT_OK(RunOpKernel());\n+\n+  std::vector<tstring> expected_values({});\n+  std::vector<int64> expected_splits({0, 0, 0});\n+\n+  assert_string_equal(expected_values, *GetOutput(0));\n+  assert_int64_equal(expected_splits, *GetOutput(1));\n+}\n+\n TEST_F(NgramKernelTest, ShapeFn) {\n   ShapeInferenceTestOp op(\"StringNGrams\");\n   INFER_OK(op, \"?;?\", \"[?];[?]\");\n"
        ],
        "Title": "\n          Heap buffer overflow in `StringNGrams`\n        "
    },
    {
        "Bug description": "An attacker can cause a heap buffer overflow to occur in  Conv2DBackpropFilter :",
        "Sample Code": "input_tensor = tf.constant([], shape=[0, 1, 1, 5], dtype=tf.float32)\nfilter_sizes = tf.constant([3, 8, 1, 1], shape=[4], dtype=tf.int32)\nout_backprop = tf.constant([], shape=[0, 1, 1, 1], dtype=tf.float32)\n\ntf.raw_ops.Conv2DBackpropFilter(\n  input=input_tensor,\n  filter_sizes=filter_sizes, \n  out_backprop=out_backprop,\n  strides=[1, 66, 49, 1], \n  use_cudnn_on_gpu=True,\n  padding='VALID',\n  explicit_paddings=[],\n  data_format='NHWC',\n  dilations=[1, 1, 1, 1]\n)]\n)",
        "Bug fix": [
            "@@ -495,6 +495,14 @@ class Conv2DCustomBackpropFilterOp : public OpKernel {\n     const int filter_total_size = dims.spatial_dims[0].filter_size *\n                                   dims.spatial_dims[1].filter_size *\n                                   dims.in_depth;\n+    OP_REQUIRES(\n+        context,\n+        filter_total_size * dims.out_depth == filter_backprop->NumElements(),\n+        errors::InvalidArgument(\n+            \"filter_size does not have enough elements, requested \",\n+            filter_total_size * dims.out_depth, \", got \",\n+            filter_backprop->NumElements()));\n+\n     // The output image size is the spatial size of the output.\n     const int output_image_size =\n         dims.spatial_dims[0].output_size * dims.spatial_dims[1].output_size;\n@@ -518,6 +526,11 @@ class Conv2DCustomBackpropFilterOp : public OpKernel {\n \n     const size_t work_unit_size = size_A + size_B + size_C;\n \n+    OP_REQUIRES(\n+        context, work_unit_size != 0,\n+        errors::InvalidArgument(\n+            \"Work size for convolution would be 0, which is not acceptable\"));\n+\n     const size_t shard_size =\n         (target_working_set_size + work_unit_size - 1) / work_unit_size;\n \n"
        ],
        "Title": "\n          Heap buffer overflow in `Conv2DBackpropFilter`\n        "
    },
    {
        "Bug description": "An attacker can cause a division by zero to occur in  Conv2DBackpropFilter :",
        "Sample Code": "input_tensor = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)\nfilter_sizes = tf.constant([0, 0, 0, 0], shape=[4], dtype=tf.int32)\nout_backprop = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)\n\ntf.raw_ops.Conv2DBackpropFilter(\n  input=input_tensor,\n  filter_sizes=filter_sizes,\n  out_backprop=out_backprop,\n  strides=[1, 1, 1, 1],\n  use_cudnn_on_gpu=False,\n  padding='SAME',\n  explicit_paddings=[],\n  data_format='NHWC',\n  dilations=[1, 1, 1, 1]\n)]\n)",
        "Bug fix": [
            "@@ -495,6 +495,14 @@ class Conv2DCustomBackpropFilterOp : public OpKernel {\n     const int filter_total_size = dims.spatial_dims[0].filter_size *\n                                   dims.spatial_dims[1].filter_size *\n                                   dims.in_depth;\n+    OP_REQUIRES(\n+        context,\n+        filter_total_size * dims.out_depth == filter_backprop->NumElements(),\n+        errors::InvalidArgument(\n+            \"filter_size does not have enough elements, requested \",\n+            filter_total_size * dims.out_depth, \", got \",\n+            filter_backprop->NumElements()));\n+\n     // The output image size is the spatial size of the output.\n     const int output_image_size =\n         dims.spatial_dims[0].output_size * dims.spatial_dims[1].output_size;\n@@ -518,6 +526,11 @@ class Conv2DCustomBackpropFilterOp : public OpKernel {\n \n     const size_t work_unit_size = size_A + size_B + size_C;\n \n+    OP_REQUIRES(\n+        context, work_unit_size != 0,\n+        errors::InvalidArgument(\n+            \"Work size for convolution would be 0, which is not acceptable\"));\n+\n     const size_t shard_size =\n         (target_working_set_size + work_unit_size - 1) / work_unit_size;\n \n"
        ],
        "Title": "\n          Division by zero in `Conv2DBackpropFilter`\n        "
    },
    {
        "Bug description": "An attacker can cause a heap buffer overflow in  QuantizedReshape  by passing in invalid thresholds for the quantization:",
        "Sample Code": "tensor = tf.constant([], dtype=tf.qint32)\nshape = tf.constant([], dtype=tf.int32)\ninput_min = tf.constant([], dtype=tf.float32)\ninput_max = tf.constant([], dtype=tf.float32)\n\n)\n\ntf.raw_ops.QuantizedReshape(tensor=tensor, shape=shape, input_min=input_min, input_max=input_max)",
        "Bug fix": [
            "@@ -17,6 +17,7 @@ limitations under the License.\n \n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/tensor_types.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/kernels/reshape_op.h\"\n@@ -30,9 +31,29 @@ class QuantizedReshapeOp : public ReshapeOp {\n   void Compute(OpKernelContext* ctx) override {\n     // This call processes inputs 1 and 2 to write output 0.\n     ReshapeOp::Compute(ctx);\n+    if (!ctx->status().ok()) {\n+      return;\n+    }\n+\n+    const auto& input_min_float_tensor = ctx->input(2);\n+    const auto& input_min_float_shape = input_min_float_tensor.shape();\n+    OP_REQUIRES(ctx,\n+                TensorShapeUtils::IsScalar(input_min_float_shape) ||\n+                    (TensorShapeUtils::IsVector(input_min_float_shape) &&\n+                     (input_min_float_shape.dim_size(0) == 1)),\n+                errors::InvalidArgument(\n+                    \"input_min must be a scalar or a vector of 1 element\"));\n+    const float input_min_float = input_min_float_tensor.flat<float>()(0);\n+    const auto& input_max_float_tensor = ctx->input(3);\n+    const auto& input_max_float_shape = input_max_float_tensor.shape();\n+    OP_REQUIRES(ctx,\n+                TensorShapeUtils::IsScalar(input_max_float_shape) ||\n+                    (TensorShapeUtils::IsVector(input_max_float_shape) &&\n+                     (input_max_float_shape.dim_size(0) == 1)),\n+                errors::InvalidArgument(\n+                    \"input_max must be a scalar or a vector of 1 element\"));\n+    const float input_max_float = input_max_float_tensor.flat<float>()(0);\n \n-    const float input_min_float = ctx->input(2).flat<float>()(0);\n-    const float input_max_float = ctx->input(3).flat<float>()(0);\n     Tensor* output_min = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\n     output_min->flat<float>()(0) = input_min_float;\n"
        ],
        "Title": "\n          Heap buffer overflow in `QuantizedReshape`\n        "
    },
    {
        "Bug description": "An attacker can cause a heap buffer overflow in  QuantizedResizeBilinear  by passing in invalid thresholds for the quantization:",
        "Sample Code": "images = tf.constant([], shape=[0], dtype=tf.qint32)\nsize = tf.constant([], shape=[0], dtype=tf.int32) \nmin = tf.constant([], dtype=tf.float32)\nmax = tf.constant([], dtype=tf.float32)\n\n)\n\ntf.raw_ops.QuantizedResizeBilinear(images=images, size=size, min=min, max=max, align_corners=False, half_pixel_centers=False)",
        "Bug fix": [
            "@@ -702,8 +702,14 @@ class QuantizedResizeBilinearOp : public OpKernel {\n   }\n \n   void Compute(OpKernelContext* context) override {\n-    const float in_min = context->input(2).flat<float>()(0);\n-    const float in_max = context->input(3).flat<float>()(0);\n+    const auto& in_min_tensor = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(in_min_tensor.shape()),\n+                errors::InvalidArgument(\"min must be a scalar\"));\n+    const float in_min = in_min_tensor.flat<float>()(0);\n+    const auto& in_max_tensor = context->input(3);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(in_max_tensor.shape()),\n+                errors::InvalidArgument(\"max must be a scalar\"));\n+    const float in_max = in_max_tensor.flat<float>()(0);\n \n     ImageResizerState st(align_corners_, false);\n     st.ValidateAndCreateOutput(context);\n"
        ],
        "Title": "\n          Heap buffer overflow in `QuantizedResizeBilinear`\n        "
    },
    {
        "Bug description": "An attacker can trigger a denial of service via a  CHECK -fail in  tf.raw_ops.SparseConcat :",
        "Sample Code": "import numpy as np\n\nindices_1 = tf.constant([[514, 514], [514, 514]], dtype=tf.int64)\nindices_2 = tf.constant([[514, 530], [599, 877]], dtype=tf.int64)\nindices = [indices_1, indices_2]\n\nvalues_1 = tf.zeros([0], dtype=tf.int64)\nvalues_2 = tf.zeros([0], dtype=tf.int64)\nvalues = [values_1, values_2]\n\nshape_1 = tf.constant([442, 514, 514, 515, 606, 347, 943, 61, 2], dtype=tf.int64)\nshape_2 = tf.zeros([9], dtype=tf.int64)\nshapes = [shape_1, shape_2]\n\n]\n\ntf.raw_ops.SparseConcat(indices=indices, values=values, shapes=shapes, concat_dim=2)",
        "Bug fix": [
            "@@ -21,9 +21,6 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n-#include \"tensorflow/core/framework/op_kernel.h\"\n-#include \"tensorflow/core/framework/register_types.h\"\n-\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/resource_mgr.h\"\n@@ -31,6 +28,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/tensor_util.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/lib/gtl/inlined_vector.h\"\n+#include \"tensorflow/core/util/overflow.h\"\n #include \"tensorflow/core/util/sparse/sparse_tensor.h\"\n \n namespace tensorflow {\n@@ -254,7 +252,22 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\n         errors::InvalidArgument(\n             \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\n \n-    TensorShape tensor_input_shape(input_shape->vec<int64>());\n+    auto input_shape_vec = input_shape->vec<int64>();\n+    int new_num_elements = 1;\n+    bool overflow_ocurred = false;\n+    for (int i = 0; i < input_shape_vec.size(); i++) {\n+      new_num_elements =\n+          MultiplyWithoutOverflow(new_num_elements, input_shape_vec(i));\n+      if (new_num_elements < 0) {\n+        overflow_ocurred = true;\n+      }\n+    }\n+\n+    OP_REQUIRES(\n+        context, !overflow_ocurred,\n+        errors::Internal(\"Encountered overflow from large input shape.\"));\n+\n+    TensorShape tensor_input_shape(input_shape_vec);\n     gtl::InlinedVector<int64, 8> std_order(rank);\n     std::iota(std_order.begin(), std_order.end(), 0);\n     SparseTensor input_st;\n@@ -262,8 +275,7 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\n                                                  tensor_input_shape, std_order,\n                                                  &input_st));\n \n-    auto input_shape_t = input_shape->vec<int64>();\n-    const int64 N = input_shape_t(0);\n+    const int64 N = input_shape_vec(0);\n \n     Tensor sparse_handles(DT_INT64, TensorShape({N}));\n     auto sparse_handles_t = sparse_handles.vec<int64>();\n@@ -274,7 +286,7 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\n     // minibatch entries.\n     TensorShape output_shape;\n     OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\n-                                input_shape_t.data() + 1,\n+                                input_shape_vec.data() + 1,\n                                 input_shape->NumElements() - 1, &output_shape));\n \n     // Get groups by minibatch dimension\n"
        ],
        "Title": "\n          `CHECK`-fail in `SparseConcat`\n        "
    },
    {
        "Bug description": "An attacker can cause a heap buffer overflow in  QuantizedMul  by passing in invalid thresholds for the quantization:",
        "Sample Code": "x = tf.constant([256, 328], shape=[1, 2], dtype=tf.quint8)\ny = tf.constant([256, 328], shape=[1, 2], dtype=tf.quint8)\nmin_x = tf.constant([], dtype=tf.float32)\nmax_x = tf.constant([], dtype=tf.float32)\nmin_y = tf.constant([], dtype=tf.float32)\nmax_y = tf.constant([], dtype=tf.float32)\n\n)\n\ntf.raw_ops.QuantizedMul(x=x, y=y, min_x=min_x, max_x=max_x, min_y=min_y, max_y=max_y)",
        "Bug fix": [
            "@@ -284,10 +284,22 @@ class QuantizedMulOp : public OpKernel {\n   void Compute(OpKernelContext* context) override {\n     const Tensor& x = context->input(0);\n     const Tensor& y = context->input(1);\n-    const float min_x = context->input(2).flat<float>()(0);\n-    const float max_x = context->input(3).flat<float>()(0);\n-    const float min_y = context->input(4).flat<float>()(0);\n-    const float max_y = context->input(5).flat<float>()(0);\n+    auto& min_x_tensor = context->input(2);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_x_tensor.shape()),\n+                errors::InvalidArgument(\"min_x must be a scalar\"));\n+    const float min_x = min_x_tensor.flat<float>()(0);\n+    auto& max_x_tensor = context->input(3);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_x_tensor.shape()),\n+                errors::InvalidArgument(\"max_x must be a scalar\"));\n+    const float max_x = max_x_tensor.flat<float>()(0);\n+    auto& min_y_tensor = context->input(4);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_y_tensor.shape()),\n+                errors::InvalidArgument(\"min_y must be a scalar\"));\n+    const float min_y = min_y_tensor.flat<float>()(0);\n+    auto& max_y_tensor = context->input(5);\n+    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_y_tensor.shape()),\n+                errors::InvalidArgument(\"max_y must be a scalar\"));\n+    const float max_y = max_y_tensor.flat<float>()(0);\n \n     BCast bcast(BCast::FromShape(x.shape()), BCast::FromShape(y.shape()));\n     if (!bcast.IsValid()) {\n"
        ],
        "Title": "\n          Heap buffer overflow in `QuantizedMul`\n        "
    },
    {
        "Bug description": "An attacker can trigger a denial of service via a  CHECK  failure by passing an empty image to  tf.raw_ops.DrawBoundingBoxes :",
        "Sample Code": "images = tf.fill([53, 0, 48, 1], 0.)\nboxes = tf.fill([53, 31, 4], 0.)\nboxes = tf.Variable(boxes)\nboxes[0, 0, 0].assign(3.90621)\n)\ntf.raw_ops.DrawBoundingBoxes(images=images, boxes=boxes)",
        "Bug fix": [
            "@@ -147,22 +147,46 @@ class DrawBoundingBoxesOp : public OpKernel {\n \n         // At this point, {min,max}_box_{row,col}_clamp are inside the\n         // image.\n-        CHECK_GE(min_box_row_clamp, 0);\n-        CHECK_GE(max_box_row_clamp, 0);\n-        CHECK_LT(min_box_row_clamp, height);\n-        CHECK_LT(max_box_row_clamp, height);\n-        CHECK_GE(min_box_col_clamp, 0);\n-        CHECK_GE(max_box_col_clamp, 0);\n-        CHECK_LT(min_box_col_clamp, width);\n-        CHECK_LT(max_box_col_clamp, width);\n+        OP_REQUIRES(\n+            context, min_box_row_clamp >= 0,\n+            errors::InvalidArgument(\"Min box row clamp is less than 0.\"));\n+        OP_REQUIRES(\n+            context, max_box_row_clamp >= 0,\n+            errors::InvalidArgument(\"Max box row clamp is less than 0.\"));\n+        OP_REQUIRES(context, min_box_row_clamp <= height,\n+                    errors::InvalidArgument(\n+                        \"Min box row clamp is greater than height.\"));\n+        OP_REQUIRES(context, max_box_row_clamp <= height,\n+                    errors::InvalidArgument(\n+                        \"Max box row clamp is greater than height.\"));\n+\n+        OP_REQUIRES(\n+            context, min_box_col_clamp >= 0,\n+            errors::InvalidArgument(\"Min box col clamp is less than 0.\"));\n+        OP_REQUIRES(\n+            context, max_box_col_clamp >= 0,\n+            errors::InvalidArgument(\"Max box col clamp is less than 0.\"));\n+        OP_REQUIRES(context, min_box_col_clamp <= width,\n+                    errors::InvalidArgument(\n+                        \"Min box col clamp is greater than width.\"));\n+        OP_REQUIRES(context, max_box_col_clamp <= width,\n+                    errors::InvalidArgument(\n+                        \"Max box col clamp is greater than width.\"));\n \n         // At this point, the min_box_row and min_box_col are either\n         // in the image or above/left of it, and max_box_row and\n         // max_box_col are either in the image or below/right or it.\n-        CHECK_LT(min_box_row, height);\n-        CHECK_GE(max_box_row, 0);\n-        CHECK_LT(min_box_col, width);\n-        CHECK_GE(max_box_col, 0);\n+\n+        OP_REQUIRES(\n+            context, min_box_row <= height,\n+            errors::InvalidArgument(\"Min box row is greater than height.\"));\n+        OP_REQUIRES(context, max_box_row >= 0,\n+                    errors::InvalidArgument(\"Max box row is less than 0.\"));\n+        OP_REQUIRES(\n+            context, min_box_col <= width,\n+            errors::InvalidArgument(\"Min box col is greater than width.\"));\n+        OP_REQUIRES(context, max_box_col >= 0,\n+                    errors::InvalidArgument(\"Max box col is less than 0.\"));\n \n         // Draw top line.\n         if (min_box_row >= 0) {\n"
        ],
        "Title": "\n          `CHECK`-fail in `DrawBoundingBoxes`\n        "
    },
    {
        "Bug description": "An attacker can force accesses outside the bounds of heap allocated arrays by passing in invalid tensor values to  tf.raw_ops.RaggedCross :",
        "Sample Code": "ragged_values = []\nragged_row_splits = [] \nsparse_indices = []\nsparse_values = []\nsparse_shape = []\n\ndense_inputs_elem = tf.constant([], shape=[92, 0], dtype=tf.int64)\ndense_inputs = [dense_inputs_elem]\n\ninput_order = \"R\"\nhashed_output = False\nnum_buckets = 0\nhash_key = 0 \n\ntf.raw_ops.RaggedCross(ragged_values=ragged_values,\n    ragged_row_splits=ragged_row_splits,\n    sparse_indices=sparse_indices,\n    sparse_values=sparse_values,\n    sparse_shape=sparse_shape,\n    dense_inputs=dense_inputs,\n    input_order=input_order,\n    hashed_output=hashed_output,\n    num_buckets=num_buckets,\n    hash_key=hash_key,\n    out_values_type=tf.int64,\n    ,\n    out_row_splits_type=tf.int64)",
        "Bug fix": [
            "@@ -21,6 +21,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/platform/fingerprint.h\"\n #include \"tensorflow/core/util/util.h\"\n #include \"tensorflow/core/util/work_sharder.h\"\n@@ -466,16 +467,45 @@ class RaggedCrossOp : public OpKernel {\n     int next_dense = 0;\n     for (char c : input_order_) {\n       if (c == 'R') {\n+        if (next_ragged >= ragged_values_list.size())\n+          return errors::InvalidArgument(\n+              \"input_order \\\"\", input_order_,\n+              \"\\\" specifies reading a ragged tensor value at index \",\n+              next_ragged, \" from a list of \", ragged_values_list.size(),\n+              \" values.\");\n+        if (next_ragged >= ragged_splits_list.size())\n+          return errors::InvalidArgument(\n+              \"input_order \\\"\", input_order_,\n+              \"\\\" specifies reading a ragged tensor split at index \",\n+              next_ragged, \" from a list of \", ragged_splits_list.size(),\n+              \" splits.\");\n         TF_RETURN_IF_ERROR(BuildRaggedFeatureReader(\n             ragged_values_list[next_ragged], ragged_splits_list[next_ragged],\n             features));\n         next_ragged++;\n       } else if (c == 'S') {\n+        if (next_sparse >= sparse_values_list.size())\n+          return errors::InvalidArgument(\n+              \"input_order \\\"\", input_order_,\n+              \"\\\" specifies reading a sparse tensor value at index \",\n+              next_sparse, \" from a list of \", sparse_values_list.size(),\n+              \" values.\");\n+        if (next_sparse >= sparse_indices_list.size())\n+          return errors::InvalidArgument(\n+              \"input_order \\\"\", input_order_,\n+              \"\\\" specifies reading a sparse tensor index at index \",\n+              next_sparse, \" from a list of \", sparse_indices_list.size(),\n+              \" indices.\");\n         TF_RETURN_IF_ERROR(BuildSparseFeatureReader(\n             sparse_indices_list[next_sparse], sparse_values_list[next_sparse],\n             batch_size, features));\n         next_sparse++;\n       } else if (c == 'D') {\n+        if (next_dense >= dense_list.size())\n+          return errors::InvalidArgument(\n+              \"input_order \\\"\", input_order_,\n+              \"\\\" specifies reading a dense tensor at index \", next_dense,\n+              \" from a list of \", dense_list.size(), \" tensors.\");\n         TF_RETURN_IF_ERROR(\n             BuildDenseFeatureReader(dense_list[next_dense++], features));\n       } else {\n"
        ],
        "Title": "\n          Heap out of bounds read in `RaggedCross`\n        "
    }
]
[
    {
        "Bug description": "An attacker can trigger a  CHECK  fail in PNG encoding by providing an empty input tensor as the pixel data:",
        "Sample Code": "image = tf.zeros([0, 0, 3])\nimage = tf.cast(image, dtype=tf.uint8) \n) \ntf.raw_ops.EncodePng(image=image) ",
        "Bug fix": [
            "@@ -54,6 +54,8 @@ class EncodePngOp : public OpKernel {\n     OP_REQUIRES(context, image.dims() == 3,\n                 errors::InvalidArgument(\"image must be 3-dimensional\",\n                                         image.shape().DebugString()));\n+    OP_REQUIRES(context, image.NumElements() > 0,\n+                errors::Internal(\"Invalid image provided.\"));\n     OP_REQUIRES(\n         context,\n         FastBoundsCheck(image.NumElements(), std::numeric_limits<int32>::max()),\n"
        ],
        "Title": "\n          `CHECK`-fail in `tf.raw_ops.EncodePng`\n        "
    },
    {
        "Bug description": "An attacker can trigger a heap buffer overflow in  tf.raw_ops.QuantizedResizeBilinear  by manipulating input values so that float rounding results in off-by-one error in accessing image elements:",
        "Sample Code": "l = [256, 328, 361, 17, 361, 361, 361, 361, 361, 361, 361, 361, 361, 361, 384]\nimages = tf.constant(l, shape=[1, 1, 15, 1], dtype=tf.qint32)\nsize = tf.constant([12, 6], shape=[2], dtype=tf.int32)\nmin = tf.constant(80.22522735595703)\nmax = tf.constant(80.39215850830078)\n\ntf.raw_ops.QuantizedResizeBilinear(images=images, size=size, min=min, max=max,\n                                   ,\n                                   align_corners=True, half_pixel_centers=True)",
        "Bug fix": [
            "@@ -64,6 +64,8 @@ inline void ComputeInterpolationWeights(\n         std::max(static_cast<int64>(in_f), static_cast<int64>(0));\n     interpolation->upper[i] =\n         std::min(static_cast<int64>(std::ceil(in)), in_size - 1);\n+    interpolation->lower[i] =\n+        std::min(interpolation->lower[i], interpolation->upper[i]);\n     interpolation->lerp[i] = in - in_f;\n     interpolation->ilerp[i] =\n         static_cast<T_SCALE>((in - in_f) * (1 << resolution));\n"
        ],
        "Title": "\n          Heap buffer overflow caused by rounding\n        "
    },
    {
        "Bug description": "An attacker can trigger a null pointer dereference by providing an invalid  permutation  to  tf.raw_ops.SparseMatrixSparseCholesky :",
        "Sample Code": "import numpy as np\nfrom tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops\n\nindices_array = np.array([[0, 0]])\nvalue_array = np.array([-10.0], dtype=np.float32)\ndense_shape = [1, 1]\nst = tf.SparseTensor(indices_array, value_array, dense_shape)\n\ninput = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\n       st.indices, st.values, st.dense_shape)\n\npermutation = tf.constant([], shape=[1, 0], dtype=tf.int32)\n \n)\n \ntf.raw_ops.SparseMatrixSparseCholesky(input=input, permutation=permutation, type=tf.float32)",
        "Bug fix": [
            "@@ -17,6 +17,8 @@ limitations under the License.\n #include <numeric>\n #include <vector>\n \n+#include \"tensorflow/core/framework/op_requires.h\"\n+\n #define EIGEN_USE_THREADS\n \n #include \"third_party/eigen3/Eigen/Core\"\n@@ -82,8 +84,8 @@ class CSRSparseCholeskyCPUOp : public OpKernel {\n \n     int64 num_rows;\n     int batch_size;\n-    ValidateInputs(ctx, *input_matrix, input_permutation_indices, &batch_size,\n-                   &num_rows);\n+    OP_REQUIRES_OK(ctx, ValidateInputs(*input_matrix, input_permutation_indices,\n+                                       &batch_size, &num_rows));\n \n     // Allocate batch pointers.\n     Tensor batch_ptr(cpu_allocator(), DT_INT32, TensorShape({batch_size + 1}));\n@@ -226,49 +228,48 @@ class CSRSparseCholeskyCPUOp : public OpKernel {\n   }\n \n  private:\n-  void ValidateInputs(OpKernelContext* ctx,\n-                      const CSRSparseMatrix& sparse_matrix,\n-                      const Tensor& permutation_indices, int* batch_size,\n-                      int64* num_rows) {\n-    OP_REQUIRES(ctx, sparse_matrix.dtype() == DataTypeToEnum<T>::value,\n-                errors::InvalidArgument(\n-                    \"Asked for a CSRSparseMatrix of type \",\n-                    DataTypeString(DataTypeToEnum<T>::value),\n-                    \" but saw dtype: \", DataTypeString(sparse_matrix.dtype())));\n+  Status ValidateInputs(const CSRSparseMatrix& sparse_matrix,\n+                        const Tensor& permutation_indices, int* batch_size,\n+                        int64* num_rows) {\n+    if (sparse_matrix.dtype() != DataTypeToEnum<T>::value)\n+      return errors::InvalidArgument(\n+          \"Asked for a CSRSparseMatrix of type \",\n+          DataTypeString(DataTypeToEnum<T>::value),\n+          \" but saw dtype: \", DataTypeString(sparse_matrix.dtype()));\n \n     const Tensor& dense_shape = sparse_matrix.dense_shape();\n     const int rank = dense_shape.dim_size(0);\n-    OP_REQUIRES(ctx, rank == 2 || rank == 3,\n-                errors::InvalidArgument(\"sparse matrix must have rank 2 or 3; \",\n-                                        \"but dense_shape has size \", rank));\n+    if (rank < 2 || rank > 3)\n+      return errors::InvalidArgument(\"sparse matrix must have rank 2 or 3; \",\n+                                     \"but dense_shape has size \", rank);\n     const int row_dim = (rank == 2) ? 0 : 1;\n     auto dense_shape_vec = dense_shape.vec<int64>();\n     *num_rows = dense_shape_vec(row_dim);\n     const int64 num_cols = dense_shape_vec(row_dim + 1);\n-    OP_REQUIRES(ctx, *num_rows == num_cols,\n-                errors::InvalidArgument(\"sparse matrix must be square; got: \",\n-                                        *num_rows, \" != \", num_cols));\n+    if (*num_rows != num_cols)\n+      return errors::InvalidArgument(\n+          \"sparse matrix must be square; got: \", *num_rows, \" != \", num_cols);\n     const TensorShape& perm_shape = permutation_indices.shape();\n-    OP_REQUIRES(\n-        ctx, perm_shape.dims() + 1 == rank,\n-        errors::InvalidArgument(\n-            \"sparse matrix must have the same rank as permutation; got: \", rank,\n-            \" != \", perm_shape.dims(), \" + 1.\"));\n-    OP_REQUIRES(\n-        ctx, perm_shape.dim_size(rank - 2) == *num_rows,\n-        errors::InvalidArgument(\n-            \"permutation must have the same number of elements in each batch \"\n-            \"as the number of rows in sparse matrix; got: \",\n-            perm_shape.dim_size(rank - 2), \" != \", *num_rows));\n+    if (perm_shape.dims() + 1 != rank)\n+      return errors::InvalidArgument(\n+          \"sparse matrix must have the same rank as permutation; got: \", rank,\n+          \" != \", perm_shape.dims(), \" + 1.\");\n+    if (perm_shape.dim_size(rank - 2) != *num_rows)\n+      return errors::InvalidArgument(\n+          \"permutation must have the same number of elements in each batch \"\n+          \"as the number of rows in sparse matrix; got: \",\n+          perm_shape.dim_size(rank - 2), \" != \", *num_rows);\n \n     *batch_size = sparse_matrix.batch_size();\n     if (*batch_size > 1) {\n-      OP_REQUIRES(\n-          ctx, perm_shape.dim_size(0) == *batch_size,\n-          errors::InvalidArgument(\"permutation must have the same batch size \"\n-                                  \"as sparse matrix; got: \",\n-                                  perm_shape.dim_size(0), \" != \", *batch_size));\n+      if (perm_shape.dim_size(0) != *batch_size)\n+        return errors::InvalidArgument(\n+            \"permutation must have the same batch size \"\n+            \"as sparse matrix; got: \",\n+            perm_shape.dim_size(0), \" != \", *batch_size);\n     }\n+\n+    return Status::OK();\n   }\n };\n \n"
        ],
        "Title": "\n          Invalid validation in `SparseMatrixSparseCholesky`\n        "
    },
    {
        "Bug description": "An attacker can trigger a division by 0 in  tf.raw_ops.QuantizedMul :",
        "Sample Code": "x = tf.zeros([4, 1], dtype=tf.quint8)\ny = tf.constant([], dtype=tf.quint8)\nmin_x = tf.constant(0.0)\nmax_x = tf.constant(0.0010000000474974513)\nmin_y = tf.constant(0.0)\nmax_y = tf.constant(0.0010000000474974513)\n\n)\n\ntf.raw_ops.QuantizedMul(x=x, y=y, min_x=min_x, max_x=max_x, min_y=min_y, max_y=max_y)",
        "Bug fix": [
            "@@ -347,6 +347,11 @@ class QuantizedMulOp : public OpKernel {\n         tensor_num_elements = x.NumElements();\n         tensor_offset = offset_x;\n       }\n+      if (vector_num_elements == 0) {\n+        context->SetStatus(\n+            errors::InvalidArgument(\"vector must have at least 1 element\"));\n+        return;\n+      }\n       VectorTensorMultiply<T, Toutput>(\n           vector_data, vector_offset, vector_num_elements, tensor_data,\n           tensor_offset, tensor_num_elements, z_data);\n"
        ],
        "Title": "\n          Division by 0 in `QuantizedMul`\n        "
    },
    {
        "Bug description": "An attacker can trigger a division by 0 in  tf.raw_ops.QuantizedConv2D :",
        "Sample Code": "input = tf.zeros([1, 1, 1, 1], dtype=tf.quint8)\nfilter = tf.constant([], shape=[1, 0, 1, 1], dtype=tf.quint8)\nmin_input = tf.constant(0.0)\nmax_input = tf.constant(0.0001)\nmin_filter = tf.constant(0.0)\nmax_filter = tf.constant(0.0001)\nstrides = [1, 1, 1, 1]\npadding = \"SAME\"               \n                               \n\n               \n                               \n\ntf.raw_ops.QuantizedConv2D(input=input, filter=filter, min_input=min_input, max_input=max_input, min_filter=min_filter, max_filter=max_filter, strides=strides, padding=padding)",
        "Bug fix": [
            "@@ -18,6 +18,8 @@ limitations under the License.\n #include <algorithm>\n #include <vector>\n \n+#include \"tensorflow/core/platform/errors.h\"\n+\n #define EIGEN_USE_THREADS\n \n #define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\n@@ -227,8 +229,12 @@ class Im2ColConvFunctor {\n       return;\n     }\n \n-    CHECK_GT(output_width, 0);\n-    CHECK_GT(output_height, 0);\n+    OP_REQUIRES(\n+        context, output_width > 0,\n+        errors::InvalidArgument(\"output_width must be strictly positive\"));\n+    OP_REQUIRES(\n+        context, output_height > 0,\n+        errors::InvalidArgument(\"output_height must be strictly positive\"));\n     int filter_left_offset;\n     int filter_top_offset;\n     if (padding == VALID) {\n@@ -255,6 +261,9 @@ class Im2ColConvFunctor {\n     // by the width, then the height. This is the standard memory order in the\n     // image world if it helps to visualize it.\n     const int filter_value_count = filter_width * filter_height * input_depth;\n+    OP_REQUIRES(context, filter_value_count > 0,\n+                errors::InvalidArgument(\n+                    \"filter patch must contain at least one element\"));\n     const int64 patches_per_chunk =\n         kMaxChunkSize / (filter_value_count * sizeof(T1));\n     const int64 chunk_value_count =\n"
        ],
        "Title": "\n          Division by 0 in `QuantizedConv2D`\n        "
    },
    {
        "Bug description": "An attacker can trigger a division by 0 in  tf.raw_ops.Conv2D :",
        "Sample Code": "input = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)\nfilter = tf.constant([], shape=[0, 0, 0, 0], dtype=tf.float32)\n\nstrides = [1, 1, 1, 1]\npadding = \"SAME\"\n                               \n\n                               \ntf.raw_ops.Conv2D(input=input, filter=filter, strides=strides, padding=padding)",
        "Bug fix": [
            "@@ -260,6 +260,11 @@ struct LaunchConv2DOp<CPUDevice, T> {\n     const int64 out_depth = output->dim_size(3);\n     const int64 patch_depth = filter.dim_size(2);\n \n+    if (patch_depth <= 0) {\n+      ctx->SetStatus(errors::InvalidArgument(\n+          \"filter depth must be stricly positive, got \", patch_depth));\n+      return;\n+    }\n     if (in_depth % patch_depth != 0) {\n       ctx->SetStatus(errors::InvalidArgument(\n           \"input depth must be evenly divisible by filter depth: \", in_depth,\n@@ -268,6 +273,11 @@ struct LaunchConv2DOp<CPUDevice, T> {\n     }\n \n     const int64 num_groups = in_depth / patch_depth;\n+    if (num_groups <= 0) {\n+      ctx->SetStatus(errors::InvalidArgument(\n+          \"number of groups must be stricly positive, got \", num_groups));\n+      return;\n+    }\n     if (out_depth % num_groups != 0 || out_depth < num_groups) {\n       ctx->SetStatus(errors::InvalidArgument(\n           \"output depth must be evenly divisible by number of groups: \",\n@@ -536,6 +546,9 @@ Status ComputeConv2DDimension(const Conv2DParameters& params,\n               errors::InvalidArgument(\"Patch depth too large\"));\n   const int in_depth = static_cast<int>(in_depth_raw);\n   const int patch_depth = static_cast<int>(patch_depth_raw);\n+  TF_REQUIRES(patch_depth > 0,\n+              errors::InvalidArgument(\n+                  \"filter depth must be stricly positive, got \", patch_depth));\n   TF_REQUIRES(in_depth % patch_depth == 0,\n               errors::InvalidArgument(\n                   \"input depth must be evenly divisible by filter depth: \",\n"
        ],
        "Title": "\n          Division by 0 in `Conv2D`\n        "
    },
    {
        "Bug description": "An attacker can trigger a division by 0 in  tf.raw_ops.Conv2DBackpropInput :",
        "Sample Code": "input_tensor = tf.constant([52, 1, 1, 5], shape=[4], dtype=tf.int32)\nfilter_tensor = tf.constant([], shape=[0, 1, 5, 0], dtype=tf.float32)\nout_backprop = tf.constant([], shape=[52, 1, 1, 0], dtype=tf.float32)\n\ntf.raw_ops.Conv2DBackpropInput(input_sizes=input_tensor, filter=filter_tensor,\n                               out_backprop=out_backprop, strides=[1, 1, 1, 1],\n                               use_cudnn_on_gpu=True, padding='SAME',\n                               explicit_paddings=[], data_format='NHWC',\n                               ,\n                               dilations=[1, 1, 1, 1])",
        "Bug fix": [
            "@@ -649,6 +649,11 @@ class Conv2DCustomBackpropInputOp : public OpKernel {\n         dims.batch_size == 1 ||\n         thread_work_unit_size >= min_thread_work_unit_size;\n \n+    OP_REQUIRES(\n+        context, work_unit_size > 0,\n+        errors::InvalidArgument(\"input, filter_sizes and out_backprop tensors \"\n+                                \"must all have at least 1 element\"));\n+\n     const size_t shard_size =\n         use_parallel_contraction\n             ? 1\n"
        ],
        "Title": "\n          Division by 0 in `Conv2DBackpropInput`\n        "
    },
    {
        "Bug description": "An attacker can trigger a division by 0 in  tf.raw_ops.Conv2DBackpropFilter :",
        "Sample Code": "input_tensor = tf.constant([], shape=[0, 0, 1, 0], dtype=tf.float32)\nfilter_sizes = tf.constant([1, 1, 1, 1], shape=[4], dtype=tf.int32)\nout_backprop = tf.constant([], shape=[0, 0, 1, 1], dtype=tf.float32)\n\ntf.raw_ops.Conv2DBackpropFilter(input=input_tensor, filter_sizes=filter_sizes,\n                                out_backprop=out_backprop,\n                                strides=[1, 66, 18, 1], use_cudnn_on_gpu=True,\n                                padding='SAME', explicit_paddings=[],\n                                [],\n                                data_format='NHWC', dilations=[1, 1, 1, 1])",
        "Bug fix": [
            "@@ -127,6 +127,10 @@ Status ConvBackpropComputeDimensionsV2(\n   // dimensions of the filter Tensor.\n   VLOG(2) << \"input vs filter_in depth \" << dims->in_depth << \" \"\n           << filter_shape.dim_size(num_dims - 2);\n+  if (filter_shape.dim_size(num_dims - 2) <= 0) {\n+    return errors ::InvalidArgument(\n+        label, \": filter depth must be strictly greated than zero\");\n+  }\n   if (dims->in_depth % filter_shape.dim_size(num_dims - 2)) {\n     return errors::InvalidArgument(\n         label, \": input depth must be evenly divisible by filter depth\");\n"
        ],
        "Title": "\n          Division by 0 in `Conv2DBackpropFilter`\n        "
    },
    {
        "Bug description": "An attacker can trigger a denial of service via a  CHECK -fail in   tf.raw_ops.AddManySparseToTensorsMap :",
        "Sample Code": "import numpy as np\n\nsparse_indices = tf.constant(530, shape=[1, 1], dtype=tf.int64)\nsparse_values = tf.ones([1], dtype=tf.int64)\n\nshape = tf.Variable(tf.ones([55], dtype=tf.int64))\nshape[:8].assign(np.array([855, 901, 429, 892, 892, 852, 93, 96], dtype=np.int64))\n\ntf.raw_ops.AddManySparseToTensorsMap(sparse_indices=sparse_indices,\n                    sparse_values=sparse_values,\n                    ,\n                    sparse_shape=shape)",
        "Bug fix": [
            "@@ -21,9 +21,6 @@ limitations under the License.\n #include <utility>\n #include <vector>\n \n-#include \"tensorflow/core/framework/op_kernel.h\"\n-#include \"tensorflow/core/framework/register_types.h\"\n-\n #include \"tensorflow/core/framework/op_kernel.h\"\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/resource_mgr.h\"\n@@ -31,6 +28,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/tensor_util.h\"\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/lib/gtl/inlined_vector.h\"\n+#include \"tensorflow/core/util/overflow.h\"\n #include \"tensorflow/core/util/sparse/sparse_tensor.h\"\n \n namespace tensorflow {\n@@ -254,7 +252,22 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\n         errors::InvalidArgument(\n             \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\n \n-    TensorShape tensor_input_shape(input_shape->vec<int64>());\n+    auto input_shape_vec = input_shape->vec<int64>();\n+    int new_num_elements = 1;\n+    bool overflow_ocurred = false;\n+    for (int i = 0; i < input_shape_vec.size(); i++) {\n+      new_num_elements =\n+          MultiplyWithoutOverflow(new_num_elements, input_shape_vec(i));\n+      if (new_num_elements < 0) {\n+        overflow_ocurred = true;\n+      }\n+    }\n+\n+    OP_REQUIRES(\n+        context, !overflow_ocurred,\n+        errors::Internal(\"Encountered overflow from large input shape.\"));\n+\n+    TensorShape tensor_input_shape(input_shape_vec);\n     gtl::InlinedVector<int64, 8> std_order(rank);\n     std::iota(std_order.begin(), std_order.end(), 0);\n     SparseTensor input_st;\n@@ -262,8 +275,7 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\n                                                  tensor_input_shape, std_order,\n                                                  &input_st));\n \n-    auto input_shape_t = input_shape->vec<int64>();\n-    const int64 N = input_shape_t(0);\n+    const int64 N = input_shape_vec(0);\n \n     Tensor sparse_handles(DT_INT64, TensorShape({N}));\n     auto sparse_handles_t = sparse_handles.vec<int64>();\n@@ -274,7 +286,7 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\n     // minibatch entries.\n     TensorShape output_shape;\n     OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\n-                                input_shape_t.data() + 1,\n+                                input_shape_vec.data() + 1,\n                                 input_shape->NumElements() - 1, &output_shape));\n \n     // Get groups by minibatch dimension\n"
        ],
        "Title": "\n          `CHECK`-fail in `AddManySparseToTensorsMap`\n        "
    },
    {
        "Bug description": "The  tf.raw_ops.Conv3DBackprop*  operations fail to validate that the input tensors are not empty. In turn, this would result in a division by 0:",
        "Sample Code": "input_sizes = tf.constant([1], shape=[1, 1, 1, 1, 1], dtype=tf.float32)\nfilter_tensor = tf.constant([0, 0, 0, 1, 0], shape=[5], dtype=tf.int32)\nout_backprop = tf.constant([], shape=[1, 1, 1, 1, 0], dtype=tf.float32)\n\n)\n\ntf.raw_ops.Conv3DBackpropFilterV2(input=input_sizes, filter_sizes=filter_tensor, out_backprop=out_backprop, strides=[1, 1, 1, 1, 1], padding='SAME', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])",
        "Bug fix": [
            "@@ -239,6 +239,14 @@ class Conv3DBackpropInputOp : public OpKernel {\n       input_shape = context->input(0).shape();\n     }\n \n+    OP_REQUIRES(context, input_shape.dims() == 5,\n+                errors::InvalidArgument(\"input tensor must have 5 dimensions\"));\n+    OP_REQUIRES(\n+        context, filter_shape.dims() == 5,\n+        errors::InvalidArgument(\"filter_sizes tensor must have 5 dimensions\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dims() == 5,\n+        errors::InvalidArgument(\"out_backprop tensor must have 5 dimensions\"));\n     OP_REQUIRES(\n         context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n         errors::InvalidArgument(\"input and filter_sizes must have the same \"\n@@ -360,6 +368,14 @@ class Conv3DCustomBackpropInputOp : public OpKernel {\n       input_shape = context->input(0).shape();\n     }\n \n+    OP_REQUIRES(context, input_shape.dims() == 5,\n+                errors::InvalidArgument(\"input tensor must have 5 dimensions\"));\n+    OP_REQUIRES(\n+        context, filter_shape.dims() == 5,\n+        errors::InvalidArgument(\"filter_sizes tensor must have 5 dimensions\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dims() == 5,\n+        errors::InvalidArgument(\"out_backprop tensor must have 5 dimensions\"));\n     OP_REQUIRES(\n         context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n         errors::InvalidArgument(\"input and filter_sizes must have the same \"\n@@ -444,6 +460,11 @@ class Conv3DCustomBackpropInputOp : public OpKernel {\n     // contraction compared to sharding and matmuls.\n     const bool use_parallel_contraction = dims.batch_size == 1;\n \n+    OP_REQUIRES(\n+        context, work_unit_size > 0,\n+        errors::InvalidArgument(\"input, filter_sizes and out_backprop tensors \"\n+                                \"must all have at least 1 element\"));\n+\n     const size_t shard_size =\n         use_parallel_contraction\n             ? 1\n@@ -724,6 +745,14 @@ class Conv3DBackpropFilterOp : public OpKernel {\n       filter_shape = context->input(1).shape();\n     }\n \n+    OP_REQUIRES(context, input_shape.dims() == 5,\n+                errors::InvalidArgument(\"input tensor must have 5 dimensions\"));\n+    OP_REQUIRES(\n+        context, filter_shape.dims() == 5,\n+        errors::InvalidArgument(\"filter_sizes tensor must have 5 dimensions\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dims() == 5,\n+        errors::InvalidArgument(\"out_backprop tensor must have 5 dimensions\"));\n     OP_REQUIRES(\n         context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n         errors::InvalidArgument(\"input and filter_sizes must have the same \"\n@@ -850,6 +879,14 @@ class Conv3DCustomBackpropFilterOp : public OpKernel {\n       filter_shape = context->input(1).shape();\n     }\n \n+    OP_REQUIRES(context, input_shape.dims() == 5,\n+                errors::InvalidArgument(\"input tensor must have 5 dimensions\"));\n+    OP_REQUIRES(\n+        context, filter_shape.dims() == 5,\n+        errors::InvalidArgument(\"filter_sizes tensor must have 5 dimensions\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dims() == 5,\n+        errors::InvalidArgument(\"out_backprop tensor must have 5 dimensions\"));\n     OP_REQUIRES(\n         context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n         errors::InvalidArgument(\"input and filter_sizes must have the same \"\n@@ -936,6 +973,11 @@ class Conv3DCustomBackpropFilterOp : public OpKernel {\n \n     const int64 work_unit_size = size_A + size_B + size_C;\n \n+    OP_REQUIRES(\n+        context, work_unit_size > 0,\n+        errors::InvalidArgument(\"input, filter_sizes and out_backprop tensors \"\n+                                \"must all have at least 1 element\"));\n+\n     const size_t shard_size =\n         (target_working_set_size + work_unit_size - 1) / work_unit_size;\n \n"
        ],
        "Title": "\n          Division by 0 in `Conv3DBackprop*`\n        "
    }
]
[
    {
        "Bug description": "Missing validation between arguments to  tf.raw_ops.Conv3DBackprop*  operations can result in heap buffer overflows:",
        "Sample Code": "input_values = [-10.0] * (7 * 7 * 7 * 7 * 7)\ninput_values[0] = 429.6491056791816\ninput_sizes = tf.constant(input_values, shape=[7, 7, 7, 7, 7], dtype=tf.float32)\nfilter_tensor = tf.constant([7, 7, 7, 1, 1], shape=[5], dtype=tf.int32)\nout_backprop = tf.constant([-10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0], shape=[7, 1, 1, 1, 1], dtype=tf.float32)\n  \n)\n  \ntf.raw_ops.Conv3DBackpropFilterV2(input=input_sizes, filter_sizes=filter_tensor, out_backprop=out_backprop, strides=[1, 37, 65, 93, 1], padding='VALID', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])",
        "Bug fix": [
            "@@ -239,6 +239,20 @@ class Conv3DBackpropInputOp : public OpKernel {\n       input_shape = context->input(0).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context, ConvBackpropComputeDimensions(\n                                 \"Conv3DBackpropInputOp\", /*num_spatial_dims=*/3,\n@@ -346,6 +360,20 @@ class Conv3DCustomBackpropInputOp : public OpKernel {\n       input_shape = context->input(0).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context, ConvBackpropComputeDimensions(\n                                 \"Conv3DBackpropInputOp\", /*num_spatial_dims=*/3,\n@@ -696,6 +724,20 @@ class Conv3DBackpropFilterOp : public OpKernel {\n       filter_shape = context->input(1).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context,\n                    ConvBackpropComputeDimensions(\n@@ -808,6 +850,20 @@ class Conv3DCustomBackpropFilterOp : public OpKernel {\n       filter_shape = context->input(1).shape();\n     }\n \n+    OP_REQUIRES(\n+        context, input_shape.dim_size(4) == filter_shape.dim_size(3),\n+        errors::InvalidArgument(\"input and filter_sizes must have the same \"\n+                                \"number of channels. Got \",\n+                                input_shape.dim_size(4), \" for input and \",\n+                                filter_shape.dim_size(3), \" for filter_sizes\"));\n+    OP_REQUIRES(\n+        context, out_backprop_shape.dim_size(4) == filter_shape.dim_size(4),\n+        errors::InvalidArgument(\"out_backprop and filter_sizes must have the \"\n+                                \"same number of channels. Got \",\n+                                out_backprop_shape.dim_size(4),\n+                                \" for out_backprop and \",\n+                                filter_shape.dim_size(4), \" for filter_sizes\"));\n+\n     ConvBackpropDimensions dims;\n     OP_REQUIRES_OK(context,\n                    ConvBackpropComputeDimensions(\n"
        ],
        "Title": "\n          Heap buffer overflow in `Conv3DBackprop*`\n        "
    },
    {
        "Bug description": "Specifying a negative dense shape in  tf.raw_ops.SparseCountSparseOutput  results in a segmentation fault being thrown out from the standard library as  std::vector  invariants are broken.",
        "Sample Code": "indices = tf.constant([], shape=[0, 0], dtype=tf.int64)\nvalues = tf.constant([], shape=[0, 0], dtype=tf.int64)\ndense_shape = tf.constant([-100, -100, -100], shape=[3], dtype=tf.int64)\nweights = tf.constant([], shape=[0, 0], dtype=tf.int64)\n\n)\n\ntf.raw_ops.SparseCountSparseOutput(indices=indices, values=values, dense_shape=dense_shape, weights=weights, minlength=79, maxlength=96, binary_output=False)",
        "Bug fix": [
            "@@ -197,9 +197,17 @@ class SparseCount : public OpKernel {\n                     \"The shape argument requires at least one element.\"));\n \n     bool is_1d = shape.NumElements() == 1;\n-    int num_batches = is_1d ? 1 : shape.flat<int64>()(0);\n+    auto shape_vector = shape.flat<int64>();\n+    int num_batches = is_1d ? 1 : shape_vector(0);\n     int num_values = values.NumElements();\n \n+    for (int b = 0; b < shape_vector.size(); b++) {\n+      OP_REQUIRES(context, shape_vector(b) >= 0,\n+                  errors::InvalidArgument(\n+                      \"Elements in dense_shape must be >= 0. Instead got:\",\n+                      shape.DebugString()));\n+    }\n+\n     OP_REQUIRES(context, num_values == indices.shape().dim_size(0),\n                 errors::InvalidArgument(\n                     \"Number of values must match first dimension of indices.\",\n"
        ],
        "Title": "\n          Segfault in `SparseCountSparseOutput`\n        "
    },
    {
        "Bug description": "The API of  tf.raw_ops.SparseCross  allows combinations which would result in a  CHECK -failure and denial of service:",
        "Sample Code": "hashed_output = False\nnum_buckets = 1949315406\nhash_key = 1869835877\nout_type = tf.string \ninternal_type = tf.string\n\nindices_1 = tf.constant([0, 6], shape=[1, 2], dtype=tf.int64)\nindices_2 = tf.constant([0, 0], shape=[1, 2], dtype=tf.int64)\nindices = [indices_1, indices_2]\n\nvalues_1 = tf.constant([0], dtype=tf.int64)\nvalues_2 = tf.constant([72], dtype=tf.int64)\nvalues = [values_1, values_2]\n\nbatch_size = 4\nshape_1 = tf.constant([4, 122], dtype=tf.int64)\nshape_2 = tf.constant([4, 188], dtype=tf.int64)\nshapes = [shape_1, shape_2]\n\ndense_1 = tf.constant([188, 127, 336, 0], shape=[4, 1], dtype=tf.int64)\ndense_2 = tf.constant([341, 470, 470, 470], shape=[4, 1], dtype=tf.int64)\ndense_3 = tf.constant([188, 188, 341, 922], shape=[4, 1], dtype=tf.int64)\ndenses = [dense_1, dense_2, dense_3]\n\ntf.raw_ops.SparseCross(indices=indices, values=values, shapes=shapes, dense_inputs=denses, hashed_output=hashed_output,\n                       ,\n                       num_buckets=num_buckets, hash_key=hash_key, out_type=out_type, internal_type=internal_type)",
        "Bug fix": [
            "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n #include \"tensorflow/core/lib/core/stringpiece.h\"\n #include \"tensorflow/core/lib/strings/str_util.h\"\n #include \"tensorflow/core/platform/fingerprint.h\"\n@@ -460,10 +461,19 @@ int64 CalculateBatchSize(const OpInputList& shapes_list_in,\n Status ValidateInput(const OpInputList& indices_list_in,\n                      const OpInputList& values_list_in,\n                      const OpInputList& shapes_list_in,\n-                     const OpInputList& dense_list_in) {\n+                     const OpInputList& dense_list_in,\n+                     const DataType& internal_type) {\n   const auto size = indices_list_in.size();\n+  // Only perform internal_type check for SparseCrossOp.\n+  // Check if the internal_type is not invalid before doing so.\n+  bool check_type = internal_type != DT_INVALID;\n   // Validates indices_list_in OpInputList.\n   for (int i = 0; i < size; i++) {\n+    if (check_type && indices_list_in[i].dtype() != DT_INT64) {\n+      return errors::InvalidArgument(\"Input indices should be of type \",\n+                                     DT_INT64, \" but received \",\n+                                     indices_list_in[i].dtype());\n+    }\n     if (!TensorShapeUtils::IsMatrix(indices_list_in[i].shape())) {\n       return errors::InvalidArgument(\n           \"Input indices should be a matrix but received shape \",\n@@ -482,6 +492,14 @@ Status ValidateInput(const OpInputList& indices_list_in,\n                                    values_list_in.size());\n   }\n   for (int i = 0; i < size; i++) {\n+    // Make sure to avoid the expected type to be string, but input values to be\n+    // int64.\n+    if (check_type && internal_type == DT_STRING &&\n+        values_list_in[i].dtype() == DT_INT64) {\n+      return errors::InvalidArgument(\"Input values should be of internal type \",\n+                                     internal_type, \" but received \",\n+                                     values_list_in[i].dtype());\n+    }\n     if (!TensorShapeUtils::IsVector(values_list_in[i].shape())) {\n       return errors::InvalidArgument(\n           \"Input values should be a vector but received shape \",\n@@ -502,6 +520,11 @@ Status ValidateInput(const OpInputList& indices_list_in,\n                                    shapes_list_in.size());\n   }\n   for (int i = 0; i < size; i++) {\n+    if (check_type && shapes_list_in[i].dtype() != DT_INT64) {\n+      return errors::InvalidArgument(\"Input shape should be of type \", DT_INT64,\n+                                     \" but received \",\n+                                     shapes_list_in[i].dtype());\n+    }\n     if (!TensorShapeUtils::IsVector(shapes_list_in[i].shape())) {\n       return errors::InvalidArgument(\n           \"Input shapes should be a vector but received shape \",\n@@ -517,6 +540,14 @@ Status ValidateInput(const OpInputList& indices_list_in,\n \n   // Validates dense_list_in OpInputList\n   for (int i = 0; i < dense_list_in.size(); ++i) {\n+    // Make sure to avoid the expected type to be string, but input values to be\n+    // int64.\n+    if (check_type && internal_type == DT_STRING &&\n+        dense_list_in[i].dtype() == DT_INT64) {\n+      return errors::InvalidArgument(\"Dense inputs should be of internal type \",\n+                                     internal_type, \" but received \",\n+                                     dense_list_in[i].dtype());\n+    }\n     if (!TensorShapeUtils::IsMatrix(dense_list_in[i].shape())) {\n       return errors::InvalidArgument(\n           \"Dense inputs should be a matrix but received shape \",\n@@ -698,6 +729,7 @@ class SparseCrossOp : public OpKernel {\n     int64 signed_hash_key_;\n     OP_REQUIRES_OK(context, context->GetAttr(\"hash_key\", &signed_hash_key_));\n     hash_key_ = static_cast<uint64>(signed_hash_key_);\n+    OP_REQUIRES_OK(context, context->GetAttr(\"internal_type\", &internal_type_));\n   }\n \n   void Compute(OpKernelContext* context) override {\n@@ -711,8 +743,10 @@ class SparseCrossOp : public OpKernel {\n     OP_REQUIRES_OK(context,\n                    context->input_list(\"dense_inputs\", &dense_list_in));\n \n-    OP_REQUIRES_OK(context, ValidateInput(indices_list_in, values_list_in,\n-                                          shapes_list_in, dense_list_in));\n+    DataType internal_type = internal_type_;\n+    OP_REQUIRES_OK(\n+        context, ValidateInput(indices_list_in, values_list_in, shapes_list_in,\n+                               dense_list_in, internal_type));\n \n     std::vector<std::unique_ptr<ColumnInterface<InternalType>>> columns =\n         GenerateColumnsFromInput<InternalType>(indices_list_in, values_list_in,\n@@ -756,6 +790,7 @@ class SparseCrossOp : public OpKernel {\n  private:\n   int64 num_buckets_;\n   uint64 hash_key_;\n+  DataType internal_type_;\n };\n \n class SparseCrossV2Op : public OpKernel {\n@@ -773,8 +808,11 @@ class SparseCrossV2Op : public OpKernel {\n     OP_REQUIRES_OK(context,\n                    context->input_list(\"dense_inputs\", &dense_list_in));\n \n-    OP_REQUIRES_OK(context, ValidateInput(indices_list_in, values_list_in,\n-                                          shapes_list_in, dense_list_in));\n+    // Set internal_type to invalid_type so that the check will be ignored.\n+    DataType internal_type = DT_INVALID;\n+    OP_REQUIRES_OK(\n+        context, ValidateInput(indices_list_in, values_list_in, shapes_list_in,\n+                               dense_list_in, internal_type));\n \n     const Tensor* sep_t;\n     OP_REQUIRES_OK(context, context->input(\"sep\", &sep_t));\n@@ -832,8 +870,11 @@ class SparseCrossHashedOp : public OpKernel {\n     OP_REQUIRES_OK(context,\n                    context->input_list(\"dense_inputs\", &dense_list_in));\n \n-    OP_REQUIRES_OK(context, ValidateInput(indices_list_in, values_list_in,\n-                                          shapes_list_in, dense_list_in));\n+    // Set internal_type to invalid_type so that the check will be ignored.\n+    DataType internal_type = DT_INVALID;\n+    OP_REQUIRES_OK(\n+        context, ValidateInput(indices_list_in, values_list_in, shapes_list_in,\n+                               dense_list_in, internal_type));\n \n     const Tensor* num_buckets_t;\n     OP_REQUIRES_OK(context, context->input(\"num_buckets\", &num_buckets_t));\n"
        ],
        "Title": "\n          `CHECK`-fail in `SparseCross` due to type confusion\n        "
    },
    {
        "Bug description": "In eager mode (default in TF 2.0 and later), session operations are invalid. However, users could still call the raw ops associated with them and trigger a null pointer dereference:",
        "Sample Code": " tensorflow as tf\ntf.raw_ops.DeleteSessionTensor(handle=['])",
        "Bug fix": [
            "@@ -91,7 +91,6 @@ TF_CALL_NUMBER_TYPES(REGISTER_GPU_KERNEL);\n REGISTER_GPU_KERNEL(bool);\n #undef REGISTER_GPU_KERNEL\n \n-\n class GetSessionTensorOp : public OpKernel {\n  public:\n   explicit GetSessionTensorOp(OpKernelConstruction* context)\n@@ -101,7 +100,11 @@ class GetSessionTensorOp : public OpKernel {\n     const Tensor& handle = ctx->input(0);\n     const string& name = handle.scalar<tstring>()();\n     Tensor val;\n-    OP_REQUIRES_OK(ctx, ctx->session_state()->GetTensor(name, &val));\n+    auto session_state = ctx->session_state();\n+    OP_REQUIRES(ctx, session_state != nullptr,\n+                errors::FailedPrecondition(\n+                    \"GetSessionTensor called on null session state\"));\n+    OP_REQUIRES_OK(ctx, session_state->GetTensor(name, &val));\n     ctx->set_output(0, val);\n   }\n \n@@ -122,7 +125,6 @@ TF_CALL_NUMBER_TYPES(REGISTER_GPU_KERNEL);\n REGISTER_GPU_KERNEL(bool);\n #undef REGISTER_GPU_KERNEL\n \n-\n class DeleteSessionTensorOp : public OpKernel {\n  public:\n   explicit DeleteSessionTensorOp(OpKernelConstruction* context)\n@@ -131,7 +133,11 @@ class DeleteSessionTensorOp : public OpKernel {\n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& handle = ctx->input(0);\n     const string& name = handle.scalar<tstring>()();\n-    OP_REQUIRES_OK(ctx, ctx->session_state()->DeleteTensor(name));\n+    auto session_state = ctx->session_state();\n+    OP_REQUIRES(ctx, session_state != nullptr,\n+                errors::FailedPrecondition(\n+                    \"DeleteSessionTensor called on null session state\"));\n+    OP_REQUIRES_OK(ctx, session_state->DeleteTensor(name));\n   }\n \n   TF_DISALLOW_COPY_AND_ASSIGN(DeleteSessionTensorOp);\n"
        ],
        "Title": "\n          Session operations in eager mode lead to null pointer dereferences\n        "
    },
    {
        "Bug description": "A malicious user could trigger a division by 0 in  Conv3D  implementation:",
        "Sample Code": "input_tensor = tf.constant([], shape=[2, 2, 2, 2, 0], dtype=tf.float32)\nfilter_tensor = tf.constant([], shape=[0, 0, 2, 6, 2], dtype=tf.float32)\n\n)\n\ntf.raw_ops.Conv3D(input=input_tensor, filter=filter_tensor, strides=[1, 56, 39, 34, 1], padding='VALID', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])",
        "Bug fix": [
            "@@ -69,6 +69,11 @@ struct LaunchConvOp<CPUDevice, T> {\n                 errors::InvalidArgument(\"CPU implementation of Conv3D \"\n                                         \"currently only supports dilated rates \"\n                                         \"of 1.\"));\n+    OP_REQUIRES(context, filter.dim_size(3) == input.dim_size(input.dims() - 1),\n+                errors::InvalidArgument(\n+                    \"Number of channels in filter (\", filter.dim_size(3),\n+                    \") must match last dimension of input (\",\n+                    input.dim_size(input.dims() - 1), \")\"));\n     functor::CuboidConvolution<CPUDevice, T>()(\n         context->eigen_device<CPUDevice>(), output->tensor<T, 5>(),\n         input.tensor<T, 5>(), filter.tensor<T, 5>(), strides[2], strides[1],\n@@ -142,6 +147,8 @@ class Conv3DOp : public BinaryOp<T> {\n     const int64 filter_depth = filter.dim_size(3);\n     const int64 out_depth = filter.dim_size(4);\n \n+    OP_REQUIRES(context, filter_depth != 0,\n+                errors::InvalidArgument(\"filter_depth must be non-zero\"));\n     OP_REQUIRES(context, in_depth % filter_depth == 0,\n                 errors::InvalidArgument(\n                     \"Input depth must be evenly divisible by filter depth: \",\n"
        ],
        "Title": "\n          Division by zero in `Conv3D`\n        "
    },
    {
        "Bug description": "Calling  tf.raw_ops.RaggedTensorToVariant  with arguments specifying an invalid ragged tensor results in a null pointer dereference:",
        "Sample Code": "input_tensor = tf.constant([], shape=[2, 2, 2, 2, 0], dtype=tf.float32)\nfilter_tensor = tf.constant([], shape=[0, 0, 2, 6, 2], dtype=tf.float32)\n\n)\n\ntf.raw_ops.Conv3D(input=input_tensor, filter=filter_tensor, strides=[1, 56, 39, 34, 1], padding='VALID', data_format='NDHWC', dilations=[1, 1, 1, 1, 1])",
        "Bug fix": [
            "@@ -159,6 +159,11 @@ class RaggedTensorToVariantOp : public OpKernel {\n \n     // Unbatch the Ragged Tensor and encode the components.\n     std::vector<RaggedTensorVariant> unbatched_ragged_input;\n+    auto batched_splits_top_vec =\n+        batched_ragged_input.splits(0).vec<SPLIT_TYPE>();\n+    int num_components = batched_splits_top_vec.size() - 1;\n+    OP_REQUIRES(context, num_components >= 0,\n+                errors::Internal(\"Invalid split argument.\"));\n     OP_REQUIRES_OK(context, UnbatchRaggedZerothDim<VALUE_TYPE, SPLIT_TYPE>(\n                                 batched_ragged_input, &unbatched_ragged_input));\n \n"
        ],
        "Title": "\n          Null pointer dereference via invalid Ragged Tensors\n        "
    },
    {
        "Bug description": "The implementation of  MatrixDiag*  does not validate that the tensor arguments are non-empty:",
        "Sample Code": "d = tf.convert_to_tensor([],dtype=tf.float32)\np = tf.convert_to_tensor([],dtype=tf.float32)\n)\ntf.raw_ops.MatrixDiagV2(diagonal=d, k=0, num_rows=0, num_cols=0, padding_value=p)",
        "Bug fix": [
            "@@ -192,9 +192,22 @@ class MatrixDiagOp : public OpKernel {\n           upper_diag_index = diag_index.flat<int32>()(1);\n         }\n       }\n-      num_rows = context->input(2).flat<int32>()(0);\n-      num_cols = context->input(3).flat<int32>()(0);\n-      padding_value = context->input(4).flat<T>()(0);\n+\n+      auto& num_rows_tensor = context->input(2);\n+      OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_rows_tensor.shape()),\n+                  errors::InvalidArgument(\"num_rows must be a scalar\"));\n+      num_rows = num_rows_tensor.flat<int32>()(0);\n+\n+      auto& num_cols_tensor = context->input(3);\n+      OP_REQUIRES(context, TensorShapeUtils::IsScalar(num_cols_tensor.shape()),\n+                  errors::InvalidArgument(\"num_cols must be a scalar\"));\n+      num_cols = num_cols_tensor.flat<int32>()(0);\n+\n+      auto& padding_value_tensor = context->input(4);\n+      OP_REQUIRES(context,\n+                  TensorShapeUtils::IsScalar(padding_value_tensor.shape()),\n+                  errors::InvalidArgument(\"padding_value must be a scalar\"));\n+      padding_value = padding_value_tensor.flat<T>()(0);\n     }\n \n     // Size validations.\n"
        ],
        "Title": "\n          Reference binding to null pointer in `MatrixDiag*` ops\n        "
    },
    {
        "Bug description": "If the  splits  argument of  RaggedBincount  does not specify a valid  SparseTensor , then an attacker can trigger a heap buffer overflow:",
        "Sample Code": "tf.raw_ops.RaggedBincount(splits=[7,8], values= [5, 16, 51, 76, 29, 27, 54, 95],\\\n                          size= 59, weights= [0, 0, 0, 0, 0, 0, 0, 0],\\\n                          ],\\\n                          binary_output=False)",
        "Bug fix": [
            "@@ -420,6 +420,15 @@ class RaggedBincountOp : public OpKernel {\n     int num_values = values.size();\n     int batch_idx = 0;\n \n+    OP_REQUIRES(ctx, splits(0) == 0,\n+                errors::InvalidArgument(\"Splits must start with 0, not with \",\n+                                        splits(0)));\n+\n+    OP_REQUIRES(ctx, splits(num_rows) == num_values,\n+                errors::InvalidArgument(\n+                    \"Splits must end with the number of values, got \",\n+                    splits(num_rows), \" instead of \", num_values));\n+\n     Tensor* out_t;\n     OP_REQUIRES_OK(\n         ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n"
        ],
        "Title": "\n          Heap out of bounds write in `RaggedBinCount`\n        "
    },
    {
        "Bug description": "Calling TF operations with tensors of non-numeric types when the operations expect numeric tensors result in null pointer dereferences.",
        "Sample Code": "import numpy as np\n\nwriter_array = np.array([1,2],dtype=np.int32)\n)\nwriter_tensor = tf.convert_to_tensor(writer_array,dtype=tf.resource)",
        "Bug fix": [
            "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"tensorflow/python/lib/core/ndarray_tensor.h\"\n \n #include <cstring>\n+#include <optional>\n \n #include \"tensorflow/c/eager/tfe_context_internal.h\"\n #include \"tensorflow/c/tf_tensor_internal.h\"\n@@ -74,6 +75,13 @@ Status PyArrayDescr_to_TF_DataType(PyArray_Descr* descr,\n   PyObject* key;\n   PyObject* value;\n   Py_ssize_t pos = 0;\n+\n+  // Return an error if the fields attribute is null.\n+  // Occurs with an improper conversion attempt to resource.\n+  if (descr->fields == nullptr) {\n+    return errors::Internal(\"Unexpected numpy data type\");\n+  }\n+\n   if (PyDict_Next(descr->fields, &pos, &key, &value)) {\n     // In Python 3, the keys of numpy custom struct types are unicode, unlike\n     // Python 2, where the keys are bytes.\n"
        ],
        "Title": "\n          Type confusion during tensor casts lead to dereferencing null pointers\n        "
    },
    {
        "Bug description": "If the  splits  argument of  RaggedBincount  does not specify a valid  SparseTensor , then an attacker can trigger a heap buffer overflow:",
        "Sample Code": " tensorflow as tf\ntf.raw_ops.RaggedBincount(splits=[0], values=[1,1,1,1,1], size=5, weights=[1,2,3,4], binary_output=False)",
        "Bug fix": [
            "@@ -420,6 +420,15 @@ class RaggedBincountOp : public OpKernel {\n     int num_values = values.size();\n     int batch_idx = 0;\n \n+    OP_REQUIRES(ctx, splits(0) == 0,\n+                errors::InvalidArgument(\"Splits must start with 0, not with \",\n+                                        splits(0)));\n+\n+    OP_REQUIRES(ctx, splits(num_rows) == num_values,\n+                errors::InvalidArgument(\n+                    \"Splits must end with the number of values, got \",\n+                    splits(num_rows), \" instead of \", num_values));\n+\n     Tensor* out_t;\n     OP_REQUIRES_OK(\n         ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n"
        ],
        "Title": "\n          Heap buffer overflow in `RaggedBinCount`\n        "
    }
]
[
    {
        "Bug description": "Calling  tf.raw_ops.ImmutableConst  with a  dtype  of  tf.resource  or  tf.variant  results in a segfault in the implementation as code assumes that the tensor contents are pure scalars.",
        "Sample Code": ">>> tf.raw_ops.ImmutableConst(dtype=tf.resource, shape=[], memory_region_name=\"/tmp/test.txt\")\n...\n)\n...\nSegmentation fault",
        "Bug fix": "",
        "Title": "\n          Segfault in tf.raw_ops.ImmutableConst\n        "
    },
    {
        "Bug description": "Under certain cases, loading a saved model can result in accessing uninitialized memory while building the computation graph. The  MakeEdge  creates an edge between one output tensor of the  src  node (given by  output_index ) and the input slot of the  dst  node (given by  input_index ). This is only possible if the types of the tensors on both sides coincide, so the function begins by obtaining the corresponding  DataType  values and comparing these for equality:",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Heap out of bounds access in MakeEdge\n        "
    },
    {
        "Bug description": "Running an LSTM/GRU model where the LSTM/GRU layer receives an input with zero-length results in a  CHECK  failure when using the CUDA backend.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          CHECK-fail in LSTM with zero-length input\n        "
    },
    {
        "Bug description": "The general implementation for matching filesystem paths to globbing pattern is vulnerable to an access out of bounds of  the array holding the directories :",
        "Sample Code": "> tf.io.gfile.glob('/tmp/x/')\nSegmentation fault",
        "Bug fix": "",
        "Title": "\n          Heap out of bounds read in filesystem glob matching\n        "
    },
    {
        "Bug description": "The  tf.raw_ops.ImmutableConst  operation returns a constant tensor created from a memory mapped file which is assumed immutable. However, if the type of the tensor is not an integral type, the operation crashes the Python interpreter as it tries to write to the memory area:",
        "Sample Code": ">>> with open('/tmp/test.txt','w') as f: f.write('a'*128)\n>>> tf.raw_ops.ImmutableConst(dtype=tf.string,shape=2,\n                              ,\n                              memory_region_name='/tmp/test.txt')",
        "Bug fix": "",
        "Title": "\n          Write to immutable memory region\n        "
    },
    {
        "Bug description": "The  tf.raw_ops.DataFormatVecPermute  API does not validate the  src_format  and  dst_format  attributes.  The code  assumes that these two arguments define a permutation of  NHWC .",
        "Sample Code": ">>> dst_format='8765')\n<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[1954047348, 1954047348],\n       [1852793646, 1852793646],\n       [1954047348, 1954047348],\n       [],\n       [1852793632, 1852793632]], dtype=int32)>",
        "Bug fix": "",
        "Title": "\n          Lack of validation in data format attributes\n        "
    },
    {
        "Bug description": "Under certain cases, a saved model can trigger use of uninitialized values during code execution. This is caused by having tensor buffers be filled with the default value of the type but forgetting to  default initialize the quantized floating point types in Eigen :",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Uninitialized memory access in Eigen types\n        "
    },
    {
        "Bug description": "When the  boxes  argument of  tf.image.crop_and_resize  has a very large value, the CPU kernel implementation receives it as a C++  nan  floating point value. Attempting to operate on this is undefined behavior which later produces a segmentation fault.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Float cast overflow undefined behavior\n        "
    },
    {
        "Bug description": "An attacker can pass an invalid  axis  value to  tf.quantization.quantize_and_dequantize :",
        "Sample Code": ".quantization.quantize_and_dequantize(\n    input=[2.5, 2.5], input_min=[0,0], input_max=[1,1], axis=10)",
        "Bug fix": "",
        "Title": "\n          Segfault in `tf.quantization.quantize_and_dequantize`\n        "
    },
    {
        "Bug description": "The  tf.raw_ops.Switch  operation takes as input a tensor and a boolean and outputs two tensors. Depending on the boolean value, one of the tensors is exactly the input tensor whereas the other one should be an empty tensor.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Segfault in tf.raw_ops.Switch in eager mode\n        "
    }
]
[
    {
        "Bug description": "If a user passes an invalid argument to  dlpack.to_dlpack  the expected validations will cause variables to bind to  nullptr  while setting a  status  variable to the error condition.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Undefined behavior in dlpack\n        "
    },
    {
        "Bug description": "If a user passes a list of strings to  dlpack.to_dlpack  there is a memory leak following an expected validation failure: \n",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Memory leak in dlpack\n        "
    },
    {
        "Bug description": "The implementation of  dlpack.to_dlpack  can be made to use uninitialized memory resulting in further memory corruption. This is because the pybind11 glue code assumes that the argument is a tensor: \n",
        "Sample Code": "==1720623==WARNING: MemorySanitizer: use-of-uninitialized-value                                                                                                                            \n    #0 0x55b0ba5c410a in tensorflow::(anonymous namespace)::GetTensorFromHandle(TFE_TensorHandle*, TF_Status*) third_party/tensorflow/c/eager/dlpack.cc:46:7\n    #1 0x55b0ba5c38f4 in tensorflow::TFE_HandleToDLPack(TFE_TensorHandle*, TF_Status*) third_party/tensorflow/c/eager/dlpack.cc:252:26\n... \n... ",
        "Bug fix": "",
        "Title": "\n          Memory corruption in dlpack.to_dlpack\n        "
    },
    {
        "Bug description": "The  SparseFillEmptyRowsGrad  implementation has incomplete validation of the shapes of its arguments: \n",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Crash due to invalid shape of grad_values in SparseFillEmptyRowsGrad\n        "
    },
    {
        "Bug description": "The implementation of  SparseFillEmptyRowsGrad  uses a double indexing pattern: \n",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Heap buffer overflow in SparseFillEmptyRowsGrad\n        "
    },
    {
        "Bug description": "The  SparseCountSparseOutput  and  RaggedCountSparseOutput  implementations don't validate that the  weights  tensor has the same shape as the data. The check exists for  DenseCountSparseOutput , where both tensors are fully specified: \n",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Heap buffer overflow in weighted sparse count ops\n        "
    },
    {
        "Bug description": "The  SparseCountSparseOutput  implementation does not validate that the input arguments form a valid sparse tensor. In particular, there is no validation that the  indices  tensor has rank 2. This tensor must be a matrix because code assumes its elements are accessed as elements of a matrix: \n",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Crash due to invalid splits in SparseCountSparseOutput\n        "
    },
    {
        "Bug description": "The  SparseCountSparseOutput  implementation does not validate that the input arguments form a valid sparse tensor. In particular, there is no validation that the  indices  tensor has the same shape as the  values  one. The values in these tensors are always accessed in parallel: \n",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Heap buffer overflow due to invalid indices in SparseCountSparseOutput\n        "
    },
    {
        "Bug description": "The  RaggedCountSparseOutput  does not validate that the input arguments form a valid ragged tensor. In particular, there is no validation that the  splits  tensor has the minimum required number of elements. Code uses this quantity to initialize a different data structure: \n",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Abort due to invalid splits in RaggedCountSparseOutput\n        "
    },
    {
        "Bug description": "The  RaggedCountSparseOutput  implementation does not validate that the input arguments form a valid ragged tensor. In particular, there is no validation that the values in the  splits  tensor generate a valid partitioning of the  values  tensor. Thus, the  following code  sets up conditions to cause a heap buffer overflow:",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Segfault due to invalid splits in RaggedCountSparseOutput\n        "
    }
]
[
    {
        "Bug description": "The  RaggedCountSparseOutput  implementation does not validate that the input arguments form a valid ragged tensor. In particular, there is no validation that the values in the  splits  tensor generate a valid partitioning of the  values  tensor. Hence, this code is prone to heap buffer overflow: \n",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Heap buffer overflow due to invalid splits in RaggedCountSparseOutput\n        "
    },
    {
        "Bug description": "The  Shard  API in TensorFlow expects the last argument to be a function taking two  int64  (i.e.,  long long ) arguments: \n",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Integer truncation in Shard API usage\n        "
    },
    {
        "Bug description": "By controlling the  fill  argument of  tf.strings.as_string , a malicious attacker is able to trigger a format string vulnerability due to the way the internal format use in a  printf  call is constructed: ",
        "Sample Code": "Out[1]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['1234  '], dtype=object)>                                              \nIn [2]: tf.strings.as_string(input=[1234], width=6, fill='+')                                                                     \nOut[2]: <tf.Tensor: shape=(1,), dtype=string, numpy=array([' +1234'], dtype=object)> \nIn [3]: tf.strings.as_string(input=[1234], width=6, fill=\"h\")                                                                     \nOut[3]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['%6d'], dtype=object)> \nIn [4]: tf.strings.as_string(input=[1234], width=6, fill=\"d\")                                                                     \nOut[4]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['12346d'], dtype=object)> \nIn [5]: tf.strings.as_string(input=[1234], width=6, fill=\"o\")\nOut[5]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['23226d'], dtype=object)>\nIn [6]: tf.strings.as_string(input=[1234], width=6, fill=\"x\")\nOut[6]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['4d26d'], dtype=object)>\nIn [7]: tf.strings.as_string(input=[1234], width=6, fill=\"g\")\nOut[7]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['8.67458e-3116d'], dtype=object)>\nIn [8]: tf.strings.as_string(input=[1234], width=6, fill=\"a\")\nOut[8]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['0x0.00ff7eebb4d4p-10226d'], dtype=object)>\nIn [9]: tf.strings.as_string(input=[1234], width=6, fill=\"c\")\nOut[9]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['], dtype=object)>\nIn [10]: tf.strings.as_string(input=[1234], width=6, fill=\"p\")\nOut[10]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['0x4d26d'], dtype=object)>\nIn [11]: tf.strings.as_string(input=[1234], width=6, fill='m') \n) \nOut[11]: <tf.Tensor: shape=(1,), dtype=string, numpy=array(['Success6d'], dtype=object)>",
        "Bug fix": "",
        "Title": "\n          Format-string vulnerability in TensorFlow's `as_string`\n        "
    },
    {
        "Bug description": "In eager mode, TensorFlow does not set the session state. Hence, calling  tf.raw_ops.GetSessionHandle  or  tf.raw_ops.GetSessionHandleV2  results in a null pointer dereference: \n",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Segfault by calling session-only ops in eager mode\n        "
    },
    {
        "Bug description": "The  data_splits  argument of  tf.raw_ops.StringNGrams  lacks validation. This allows a user to pass values that can cause heap overflow errors and even leak contents of memory",
        "Sample Code": "StringNGrams(ngrams=<tf.Tensor: shape=(6,), dtype=string, numpy=\narray([b'aa bb cc', b'bb cc dd', b'cc dd ee', b'dd ee ff',\n       b'ee ff ,\n       ,\n       b'ff ],...",
        "Bug fix": "",
        "Title": "\n          Data leak in `tf.raw_ops.StringNGrams`\n        "
    },
    {
        "Bug description": "Changing the TensorFlow's  SavedModel  protocol buffer and altering the name of required keys results in segfaults and data corruption while loading the model. This can cause a denial of service in products using  tensorflow-serving  or other inference-as-a-service installments.",
        "Sample Code": "",
        "Bug fix": [
            "@@ -21,6 +21,7 @@ limitations under the License.\n #include \"tensorflow/cc/saved_model/loader_util.h\"\n #include \"tensorflow/cc/saved_model/reader.h\"\n #include \"tensorflow/core/framework/attr_value.pb.h\"\n+#include \"tensorflow/core/framework/function.pb.h\"\n #include \"tensorflow/core/framework/node_def.pb.h\"\n #include \"tensorflow/core/framework/tensor.pb.h\"\n #include \"tensorflow/core/lib/io/path.h\"\n@@ -73,26 +74,41 @@ uint64 GetLatencyMicroseconds(const uint64 start_microseconds) {\n // Ensure that constant tensors loaded from the saved model have valid shape.\n // Also ensure that constant nodes have a value assigned to them.\n // TODO(b/154763635): this is temporary and will be replaced with a better audit\n+static Status ValidateNode(const NodeDef& node) {\n+  const auto node_iterator = node.attr().find(\"value\");\n+  if (node_iterator != node.attr().end()) {\n+    AttrValue node_value = node_iterator->second;\n+    if (node_value.has_tensor()) {\n+      const PartialTensorShape node_shape(node_value.tensor().tensor_shape());\n+      if (node_shape.num_elements() < 0) {\n+        return errors::FailedPrecondition(\n+            \"Saved model contains node \\\"\", node.name(), \"\\\" (op \\\"\", node.op(),\n+            \"\\\") which initializes from a tensor with \",\n+            node_shape.num_elements(), \" elements\");\n+      }\n+    }\n+  } else if (node.op() == \"Const\") {\n+    return errors::FailedPrecondition(\n+        \"Saved model contains node \\\"\", node.name(),\n+        \"\\\" which is a constant tensor but no value has been provided\");\n+  }\n+  return Status::OK();\n+}\n+\n static Status ValidateSavedTensors(const GraphDef& graph_def) {\n   for (const auto& node : graph_def.node()) {\n-    const auto node_iterator = node.attr().find(\"value\");\n-    if (node_iterator != node.attr().end()) {\n-      AttrValue node_value = node_iterator->second;\n-      if (node_value.has_tensor()) {\n-        const PartialTensorShape node_shape(node_value.tensor().tensor_shape());\n-        if (node_shape.num_elements() < 0) {\n-          return errors::FailedPrecondition(\n-              \"Saved model contains node \\\"\", node.name(), \"\\\" (op \\\"\",\n-              node.op(), \"\\\") which initializes from a tensor with \",\n-              node_shape.num_elements(), \" elements\");\n-        }\n+    TF_RETURN_IF_ERROR(ValidateNode(node));\n+  }\n+\n+  if (graph_def.has_library()) {\n+    const FunctionDefLibrary& library = graph_def.library();\n+    for (const auto& function : library.function()) {\n+      for (const auto& node : function.node_def()) {\n+        TF_RETURN_IF_ERROR(ValidateNode(node));\n       }\n-    } else if (node.op() == \"Const\") {\n-      return errors::FailedPrecondition(\n-          \"Saved model contains node \\\"\", node.name(),\n-          \"\\\" which is a constant tensor but no value has been provided\");\n     }\n   }\n+\n   return Status::OK();\n }\n \n",
            "@@ -45,6 +45,8 @@ constexpr char kTestFuzzGeneratedNegativeShape[] =\n     \"cc/saved_model/testdata/fuzz_generated/negative_shape\";\n constexpr char kTestFuzzGeneratedConstWithNoValue[] =\n     \"cc/saved_model/testdata/fuzz_generated/const_with_no_value\";\n+constexpr char kTestFuzzGeneratedBadNodeAttr[] =\n+    \"cc/saved_model/testdata/fuzz_generated/bad_node_attr\";\n \n class LoaderTest : public ::testing::Test {\n  protected:\n@@ -328,5 +330,20 @@ TEST_F(LoaderTest, ConstNoValue) {\n       std::string::npos);\n }\n \n+TEST_F(LoaderTest, BadNodeAttr) {\n+  SavedModelBundle bundle;\n+  RunOptions run_options;\n+  SessionOptions session_options;\n+\n+  const string export_dir =\n+      io::JoinPath(testing::TensorFlowSrcRoot(), kTestFuzzGeneratedBadNodeAttr);\n+  Status st = LoadSavedModel(session_options, run_options, export_dir,\n+                             {kSavedModelTagServe}, &bundle);\n+  EXPECT_FALSE(st.ok());\n+  EXPECT_NE(\n+      st.error_message().find(\"constant tensor but no value has been provided\"),\n+      std::string::npos);\n+}\n+\n }  // namespace\n }  // namespace tensorflow\n",
            "",
            "Binary files /dev/null and b/tensorflow/cc/saved_model/testdata/fuzz_generated/bad_node_attr/saved_model.pb differ\n",
            "Binary files /dev/null and b/tensorflow/cc/saved_model/testdata/fuzz_generated/bad_node_attr/variables/variables.data-00000-of-00001 differ\n",
            "Binary files /dev/null and b/tensorflow/cc/saved_model/testdata/fuzz_generated/bad_node_attr/variables/variables.index differ\n"
        ],
        "Title": "\n          Incomplete validation in TensorFlow's SavedModel's constant nodes causes segfaults\n        "
    },
    {
        "Bug description": "To mimic Python's indexing with negative values, TFLite uses  ResolveAxis  to convert negative values to positive indices. However, the only check that the converted index is now valid is only present in debug builds: \n",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Segfault and data corruption caused by negative indexing in TFLite\n        "
    },
    {
        "Bug description": "When determining the common dimension size of two tensors, TFLite uses a  DCHECK  which is no-op outside of debug compilation modes: \n",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Data corruption due to dimension mismatch in TFLite\n        "
    },
    {
        "Bug description": "A crafted TFLite model can force a node to have as input a tensor backed by a  nullptr  buffer. This can be achieved by changing a buffer index in the flatbuffer serialization to convert a read-only tensor to a read-write one. The runtime assumes that these buffers are written to before a possible read, hence they are initialized with  nullptr : \n",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Null pointer dereference in TFLite\n        "
    },
    {
        "Bug description": "If a TFLite saved model uses the same tensor as both input and output of an operator, then, depending on the operator, we can observe a segmentation fault or just memory corruption.",
        "Sample Code": "",
        "Bug fix": "",
        "Title": "\n          Segmentation fault and/or data corruption due to invalid TFLite model\n        "
    }
]
